{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4fyFskoK4Ub"
   },
   "source": [
    "# Homework 9 Backpropagation and introduction to Pytorch\n",
    "Due on April 21th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9xF7q68KE7j"
   },
   "source": [
    "## Problem 1: Backprop in a simple MLP\n",
    "Here, we ask you to derive all the steps of the backpropagation algorithm for a simple classification network. Consider a fully-connected neural network, also known as a multi-layer perceptron (MLP), with a single hidden layer and a one-node output layer. The hidden and output nodes use an elementwise sigmoid activation function and the loss layer uses cross-entropy loss:\n",
    "<p>\n",
    "$f(z)=\\frac{1}{1+exp(-z))}$\n",
    "<br>\n",
    "$L(\\hat{y},y)=-yln(\\hat{y}) - (1-y)ln(1-\\hat{y})$\n",
    "</p>\n",
    "<p>\n",
    "The computation graph for an example network is shown below. Note that it has an equal number of nodes in the input and hidden layer (3 each), but, in general, they need not be equal. Also, to make the application of backprop easier, we show the <i>computation graph</i> which shows the dot product and activation functions as their own nodes, rather than the usual graph showing a single node for both.\n",
    "\n",
    "</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ruizhaoz/EC414/master/mlpgraph.png\" style=\"height:200px;\">\n",
    "\n",
    "The backpropagation algorithm for an MLP is displayed below. For simplicity, we will assume no regularization on the weights, so you can ignore the terms involving $\\Omega$. The forward step is: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ruizhaoz/EC414/master/forward.png\" style=\"width:200px;\">\n",
    "\n",
    "and the backward step is:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ruizhaoz/EC414/master/backward.png\" style=\"width:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VENM1O51Lm3u"
   },
   "source": [
    "Write down each step of the backward pass explicitly for all layers, i.e. for the loss and $k=2,1$, compute all gradients above, expressing them as a function of variables $x, y, h, W, b$. <i>Hint: you should substitute the updated values for the gradient $g$ in each step and simplify as much as possible.</i>  Specifically, compute the following (we have replaced the superscript notation $u^{(i)}$ with $u^i$):\n",
    "\n",
    "**Q1.1**: $\\nabla_{\\hat{y}}L(\\hat{y},y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbpAW-DlLoPp"
   },
   "source": [
    "\n",
    "\n",
    "**Solution:**\n",
    "- $L = (−y)ln(\\hat{y})$\n",
    "- $\\nabla L_\\hat{y} = \\frac{(−y)}{\\hat{y}}$\n",
    "-(dL/da)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1fJZBj3dcYG"
   },
   "source": [
    "**Q1.2**: $\\nabla_{a^{(2)}}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXfklfb7diX1"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "-$\\nabla_{a^{(2)}}J =\\nabla_{a^{(2)}}\\sigma(z^{(2)})$\n",
    "\n",
    "-$\\nabla_{a^{(2)}}J = \\sigma(z^{(2)})*(1-\\sigma(z^{(2)}))$\n",
    "\n",
    "-$(da/dz)J = \\sigma(WXB)*(1-\\sigma(WXB)$\n",
    "\n",
    "-$(dL/dz)J/g = \\sigma(WXB)*(1-\\sigma(WXB)*-y/\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYh1E8CTiN2C"
   },
   "source": [
    "**Q1.3**: $\\nabla_{b^{(2)}}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7pIlEcqiSjP"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "-$\\nabla_{b_2} = g+\\nabla_{b_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp6SkxGwio3n"
   },
   "source": [
    "** Q1.4**: $\\nabla_{W^{(2)}}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYEhB8SYix5-"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "-$\\nabla_{W_2} = g*h_{1}+\\nabla_{W_2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGdFV93qkAni"
   },
   "source": [
    "**Q1.5**: \n",
    "\n",
    "-$\\nabla_{h^{(1)}}J$ \n",
    "\n",
    "-$\\nabla_{h^{(1)}}J = W_2*g$ \n",
    "\n",
    "-$\\nabla_{h^{(1)}}J = W_2*\\sigma(W_2*B_2)*(1-\\sigma(W_2*B_211))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWCCJapRkTZk"
   },
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUcN5zqukgps"
   },
   "source": [
    "**Q1.6**: $\\nabla_{b^{(1)}}J$, $\\nabla_{W^{(1)}}J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0q0REkiokwHg"
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "-∇W1 = g∗h0+∇W1\n",
    "- ∇b1= g+∇b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQppboJql-s4"
   },
   "source": [
    "**Q1.7** Briefly, explain how would the computational speed of backpropagation be affected if it did not save results in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD_17D3QmAqK"
   },
   "source": [
    "**Solution:**\n",
    "It would be slowed by a factor of a half because you would need to recompute the forward propigation step todo the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXTrAN89nkJd"
   },
   "source": [
    "# Problem 2: Pytorch Intro\n",
    "## **Q2.0**: Pytorch tutorials\n",
    "This homework will introduce you to [PyTorch](https://pytorch.org), currently the fastest growing deep learning library, and the one we will use in this course.\n",
    "\n",
    "Before starting the homework, please go over these introductory tutorials on the PyTorch webpage:\n",
    "\n",
    "*   [60-minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NrFPTKJYdaeY"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Z5zMSRqPEG"
   },
   "source": [
    "The `torch.Tensor` class is the basic building block in PyTorch and is used to hold data and parameters. The `autograd` package provides automatic differentiation for all operations on Tensors. After reading about Autograd in the tutorials above,  we will implement a few simple examples of what Autograd can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTYYQoSlqbb0"
   },
   "source": [
    "## **Q2.1**. Simple function\n",
    " Use `autograd` to do backpropagation on the simple function, $f=(x+y)*z$. \n",
    "\n",
    "**Q2.1.1** Create the three input tensors with values $x=-2$, $y=5$ and $z=-4$ as tensors and set `requires_grad=True` to track computation on them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhABwVlxqNJp",
    "outputId": "c18cfc47-1db6-4135-be1e-6102d51f90ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2., requires_grad=True) tensor(5., requires_grad=True) tensor(-4., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "x = torch.tensor(-2.0,requires_grad=True)\n",
    "y = torch.tensor(5.0,requires_grad=True)\n",
    "z = torch.tensor(-4.0,requires_grad=True)\n",
    "print(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVIMV4oUqoDD"
   },
   "source": [
    "**Q2.1.2** Compute the $q=x+y$ and $f=q \\times z$ functions, creating tensors for them in the process. Print out $q,f$, then run `f.backward(retain_graph=True)`, to compute the gradients w.r.t. $x,y,z$. The `retain_graph` attribute tells autograd to keep the computation graph around after backward pass as opposed deleting it (freeing some memory). Print the gradients. Note that the gradient for $q$ will be `None` since it is an intermediate node, even though `requires_grad` for it is automatically set to `True`. To access gradients for intermediate nodes in PyTorch you can use hooks as mentioned in [this answer](https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94/2). Compute the values by hand (or check the slides) to verify your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTON_Ch6q0zX",
    "outputId": "7e026092-bee6-431d-9ead-1fc6c4bf7001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>) tensor(-12., grad_fn=<MulBackward0>) done qf\n",
      "tensor(-4.)\n",
      "starting grads tensor(-4.) tensor(-4.) tensor(3.) -4\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "starting grads tensor(-16.) tensor(-16.) tensor(12.) -4\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "q = x+y\n",
    "f = q*z\n",
    "print(q, f,\"done qf\")\n",
    "q.register_hook(print)\n",
    "f.backward(retain_graph=True)\n",
    "\n",
    "# compute the gradient\n",
    "print(\"starting grads\",x.grad, y.grad, z.grad, -4)\n",
    "\n",
    "q.register_hook(print)\n",
    "f.backward(retain_graph=True)\n",
    "f.backward(retain_graph=True)\n",
    "f.backward(retain_graph=True)\n",
    "# compute the gradient\n",
    "print(\"starting grads\",x.grad, y.grad, z.grad, -4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pfwkA3yq3Mz"
   },
   "source": [
    "**Q2.1.3** If we now run `backward()` again, it will add the gradients to their previous values. Try it by running the above cell multiple times. This is useful in some cases, but if we just wanted to re-compute the gradients again, we need to zero them first, then run `backward()`. Add this step, then try running the  backward function multiple times to make sure the answer is the same each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FKH2O14q8uL",
    "outputId": "b8e9e78f-38ee-44fa-d64a-37077564e825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.) tensor(-4.) tensor(3.) None\n",
      "tensor(-4.)\n",
      "tensor(-4.)\n",
      "tensor(-4.) tensor(-4.) tensor(3.) None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "# zero the gradient\n",
    "x.grad = None  \n",
    "y.grad =None \n",
    "z.grad = None\n",
    "q.grad = None\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)\n",
    "# compute the gradient again\n",
    "x.grad = None  \n",
    "y.grad =None \n",
    "z.grad = None\n",
    "q.grad = None\n",
    "f.backward(retain_graph=True)\n",
    "print(x.grad, y.grad, z.grad, q.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX01mssrrJnk"
   },
   "source": [
    "## **Q2.2** Neuron \n",
    "### 2.2.1 \n",
    "Implement the function corresponding to one neuron (logistic regression unit) that we saw in the lecture and compute the gradient w.r.t. $x$ and $w$. The function is $f=\\sigma(w^Tx)$ where $\\sigma()$ is the sigmoid function. Initialize $x=[-1, -2, 1]$ and the weights to $w=[2, -3, -3]$ where $w_3$ is the bias. Print out the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qOeEIubrAn1",
    "outputId": "1bf9e5c7-12b2-43fa-bdb5-fdc513a3a083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<DotBackward>)\n",
      "\n",
      "x= tensor([-1., -2.,  1.], requires_grad=True) \n",
      "w= tensor([ 2., -3., -3.], requires_grad=True) \n",
      "f(x,w)= tensor(0.7311, grad_fn=<SigmoidBackward>)\n",
      "The gradient of f() w.r.t. x is tensor([ 0.3932, -0.5898, -0.5898])\n",
      "The gradient of f() w.r.t. w is tensor([-0.1966, -0.3932,  0.1966])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import numpy as np\n",
    "\n",
    "X = [-1.0,-2.0,1.0]\n",
    "x = torch.tensor(X,requires_grad=True)\n",
    "W = [2.0,-3.0,-3.0]\n",
    "w =  torch.tensor(W,requires_grad=True)\n",
    "\n",
    "wtx = torch.matmul(w, x)\n",
    "print (wtx)\n",
    "f = torch.sigmoid(wtx)\n",
    "x.grad = None  \n",
    "w.grad =None \n",
    "f.grad = None\n",
    "print(\"\\nx=\", x, \"\\nw=\", w, \"\\nf(x,w)=\", f)\n",
    "# compute the gradient calling backward()\n",
    "f.backward(retain_graph=True)\n",
    "print(\"The gradient of f() w.r.t. x is\", x.grad)\n",
    "print(\"The gradient of f() w.r.t. w is\", w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Zdn-SmS-dW"
   },
   "source": [
    "### 2.2.2 \n",
    "Derive the gradient $\\nabla_x f$ and  $\\nabla_\\omega f$ by hand to verify your results in 2.2.1. (Write out necessary steps i.e. chain rule, final computation results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mRG8qCDTtZA"
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "-$W.T @ X -> h1 -> \\sigma -> f $ \n",
    "\n",
    "- $\\left( \\begin{array}{cc}\n",
    "2.0\\\\-3.0\\\\-3.0\n",
    "\\end{array} \\right)\n",
    "%\n",
    "\\left( \\begin{array}{cc}\n",
    "-1.0&-2.0&1.0\n",
    "\\end{array}\\right)\n",
    "\\left |= 1\\right| ->\\sigma(1) -> 0.731\n",
    "$\n",
    "\n",
    "- $\\frac{ds}{dh1} = (\\sigma(z)*(1 - \\sigma(z)) = 0.731 * 0.279 = 0.204$  \n",
    "\n",
    "- $\\frac{dh1}{w.t@x} = W_1*X_1 + W_2*X_2 + W_3*X_3 $\n",
    "\n",
    "- $\\frac{dh1}{w} = X_1 , X_2 , X_3 $\n",
    "\n",
    "- $\\frac{dh1}{x} = W_1 , W_2 , W_3 $\n",
    "\n",
    "- $\\frac{w.t@x}{x} =\\frac{dh1}{x}*\\frac{ds}{dh1} = <W_1 , W_2 , W_3 >*0.204$\n",
    "\n",
    "- $\\frac{w.t@x}{w} =\\frac{dh1}{w}*\\frac{ds}{dh1} = <X_1 , X_2 , X_3 >*0.204$\n",
    "\n",
    "-$w.t.r_x = 0.204*\\left( \\begin{array}{cc}\n",
    "2.0\\\\-3.0\\\\-3.0\n",
    "\\end{array} \\right) = %\n",
    "\\left( \\begin{array}{cc}\n",
    "0.406\\\\-0.609\\\\-0.609\n",
    "\\end{array}\\right) $ \n",
    "\n",
    "-$w.t.r_w = 0.204*\\left( \\begin{array}{cc}\n",
    "-1.0\\\\-2.0\\\\1.0\n",
    "\\end{array} \\right) = %\n",
    "\\left( \\begin{array}{cc}\n",
    "-0.204\\\\-0.408\\\\0.204\n",
    "\\end{array}\\right) $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f6vmhMQr2EK"
   },
   "source": [
    "## **Q2.3**. torch.nn\n",
    "\n",
    " Implement the function corresponding to one neuron (logistic regression unit) that we saw in the lecture and compute the gradient w.r.t. $x$ and $w$. The function is $f=\\sigma(w^Tx+b)$ where $\\sigma()$ is the sigmoid function. Initialize $x=[-1, -2]$ and the weights to $w=[2, -3]$ bias $b=-3$. Now we are using but using the `Linear` class from `torch.nn`, followed by the [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid) class. \n",
    "\n",
    "In general, many useful functions are already implemented for us in this package. Compute the gradients $\\partial f/\\partial w$ by running `backward()` and print them out (they will be stored in the Linear variable, e.g. in `.weight.grad`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTBMhenyr4uj",
    "outputId": "fd2f1b9c-4b2b-42fe-c42c-127608340d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "weights: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "bias: Parameter containing:\n",
      "tensor([[2.]], requires_grad=True)\n",
      "\n",
      "x: torch.Size([1, 2])\n",
      "\n",
      "lin aprox =  tensor([[6.]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "f: tensor([[0.9975]], grad_fn=<SigmoidBackward>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.0025, -0.0049]]) |b is: tensor([[0.0025]])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "x.grad = None  \n",
    "w.grad =None \n",
    "\n",
    "\n",
    "\n",
    "w = torch.tensor([[2.], [-3.]])\n",
    "b =  torch.tensor( [[2.0]],requires_grad=True)\n",
    "# initilize a linear layer\n",
    "linear_f = nn.Linear(2, 1)\n",
    "# initilize weight and bias\n",
    "linear_f.weight.data = torch.transpose(w, 0, 1)\n",
    "linear_f.bias.data = b\n",
    "\n",
    "print(\"\\nweights:\", linear_f.weight)\n",
    "print(\"\\nbias:\",linear_f.bias)\n",
    "\n",
    "# initilize x and compute f\n",
    "X = [[-1.0,-2.0]]\n",
    "x =  torch.tensor([[-1.0,-2.0]],requires_grad=True)\n",
    "print(\"\\nx:\",x.shape)\n",
    "\n",
    "forSig = linear_f(x)\n",
    "\n",
    "print(\"\\nlin aprox = \",forSig)\n",
    "\n",
    "m =nn.Sigmoid()\n",
    "f = m(forSig)\n",
    "\n",
    "\n",
    "print(\"\\nf:\", f)\n",
    "\n",
    "# do backprop\n",
    "f.backward(retain_graph=True)\n",
    "\n",
    "print(\"The gradient of f() w.r.t. w is\", linear_f.weight.grad,\"|b is:\" ,linear_f.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmTyL4tAtWZC"
   },
   "source": [
    "## **Q2.4** Module\n",
    " Now lets put these two functions (Linear and Sigmoid) together into a \"module\". Read the [Neural Networks tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) if you have not already.\n",
    "\n",
    "**Q2.4.1** Make a subclass of the `Module` class, called `Neuron`. Set variables to the same values as above. You will need to fill out the `__init__` and the `forward` is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4Oy9NG-gr81q"
   },
   "outputs": [],
   "source": [
    "# solution here\n",
    "import torch.nn as nn\n",
    "\n",
    "class Neuron(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Neuron, self).__init__()\n",
    "        # an affine operation: y = weight*x + bias, with fixed parameters\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.linear.weight.data = torch.transpose(torch.tensor([[2.], [-3.]]),0,1);\n",
    "        self.linear.bias.data = torch.tensor([[2.0]])\n",
    "        # a sigmoid function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM13evB9tj0j"
   },
   "source": [
    "**Q2.4.2** Now create a  variable of your `Neuron` class called `my_neuron` and run backpropagation on it. Print out the gradients again. Make sure you zero out the gradients first, by calling `.zero_grad()` function of the parent class. Even if you will not re-compute the backprop, it is good practice to do this every time to avoid accumulating gradient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-VKjnu7tfgP",
    "outputId": "b576db84-167d-4bbf-824b-3448d989b714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "The weights are: Parameter containing:\n",
      "tensor([[ 2., -3.]], requires_grad=True)\n",
      "\n",
      "f(x,w)= tensor([[0.9975]], grad_fn=<SigmoidBackward>)\n",
      "The gradient of f() w.r.t. w is tensor([[-0.0025, -0.0049]]) tensor([[0.0025]])\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "my_neuron = Neuron()\n",
    "print(my_neuron)\n",
    "params = list(my_neuron.parameters())\n",
    "\n",
    "print(\"The weights are:\", params[0])  # linear layer's .weight\n",
    "# initialize the input x same as in Q2.3 and compute the output of my_neuron\n",
    "x =  torch.tensor([[-1.0,-2.0]],requires_grad=True)\n",
    "out = my_neuron.forward(x)\n",
    "\n",
    "print(\"\\nf(x,w)=\", out)\n",
    "# zero the gradient of my_neuron and compute the gradient by calling backward.\n",
    "my_neuron.zero_grad()\n",
    "out.backward(retain_graph=True)\n",
    "print(\"The gradient of f() w.r.t. w is\", params[0].grad, params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ywjySyatv0l"
   },
   "source": [
    "## **Q2.5**. Loss and SGD\n",
    " Now, lets train our neuron on some data. The code below creates a toy dataset containing a few inputs $x$ and outputs $y$ (a binary 0/1 label), as well as a function that plots the data and current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "s7fQKy56txsR",
    "outputId": "d338a7ab-b78d-492f-e6cb-6aac083edd17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = 2.0 w1 = -3.0 bias = 2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create some toy 2-D datapoints with binary (0/1) labels\n",
    "x = torch.tensor([[1.2, 1], [0.2, 1.4], [0.5, 0.5], \n",
    "                  [-1.5, -1.3], [0.2, -1.4], [-0.7, -0.5]])\n",
    "y = torch.tensor([0, 0, 0, 1, 1, 1 ])\n",
    "\n",
    "def plot_soln(x, y, params):\n",
    "  plt.plot(x[y==1,0], x[y==1,1], 'r+')\n",
    "  plt.plot(x[y==0,0], x[y==0,1], 'b.')\n",
    "  plt.grid(True)\n",
    "  plt.axis([-2, 2, -2, 2])\n",
    "  \n",
    "  # NOTE : This may depend on how you implement Neuron.\n",
    "  #   Change accordingly\n",
    "  w0 = params[0][0][0].item()\n",
    "  w1 = params[0][0][1].item()\n",
    "  bias = params[1][0].item()\n",
    "  \n",
    "  print(\"w0 =\", w0, \"w1 =\", w1, \"bias =\", bias)\n",
    "  dbx = torch.tensor([-2, 2])\n",
    "  dby = -(1/w1)*(w0*dbx + bias)  # plot the line corresponding to the weights and bias\n",
    "  plt.plot(dbx, dby)\n",
    "\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpxhBtl8t6Th"
   },
   "source": [
    "**Q2.5.1** Declare an object `criterion` of type `nn.CrossEntropyLoss`. Note that this can be called as a function on two tensors, one representing the network outputs and the other, the targets that the network is being trained to predict, to return the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEZiH8lLtnrT",
    "outputId": "2522a110-5783-417b-f3f6-cbbeda57ea8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.7994996905326843\n"
     ]
    }
   ],
   "source": [
    "# solution here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# forward + backward + optimize\n",
    "\n",
    "outputs =my_neuron.forward((x))\n",
    "input = torch.transpose(torch.tensor([[1.0,1.0,1.0,1.0,1.0,1.0]]),0,1)\n",
    "\n",
    "outputs = torch.cat((outputs,input), 1)\n",
    "loss = criterion(outputs,y)\n",
    "print(\"loss =\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYv8hztvuLg1"
   },
   "source": [
    "\n",
    "**Q2.5.2** Print out the chain of `grad_fn` functions backwards starting from `loss.grad_fn`  to demonstrate what backpropagation will be run on. This part is already given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM6X4OIYuG8O",
    "outputId": "69b2d6ec-6d94-4330-92c6-76bc50d74ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NllLossBackward object at 0x7f45e0bf6650>\n",
      "<LogSoftmaxBackward object at 0x7f45e0c0ecd0>\n",
      "<CatBackward object at 0x7f45e0c0e7d0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  \n",
    "print(loss.grad_fn.next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  \n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jQb5oh5uWG-"
   },
   "source": [
    "**Q2.5.3** Run the Stochastic Gradient Descent (SGD) optimizer from the `torch.optim` package to train your classifier on the toy dataset. Use the entire dataset in each batch. Use a learning rate of $0.01$ (no other hyperparameters). You will need to write a training loop that uses the `.step()` function of the optimizer. Plot the solution and print the loss after 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "vkm4xUHZuVQf",
    "outputId": "e427ba15-7fac-4b7c-932e-0de13a923e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.512342631816864\n",
      "w0 = 4.21055793762207 w1 = 2.4425106048583984 bias = 0.5773947238922119\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5d3/8fc3CckgSdgSwr7JJotsSRC1Foqt6NOKWrUulYSglFJrW7uotW61rd371GrdA9HyqLTWShFrFaHWiiYB2VE2UUH2JQuQhCT374+MbX40ISFzMmdm8nld17nmnJk75/54E/Ods5tzDhERkTi/A4iISGRQQRAREUAFQUREglQQREQEUEEQEZEgFQQREQE8KAhm1sfMlprZBjNbb2bfaKCNmdn9ZrbFzNaY2bhQ+xUREW8leLCOauDbzrmVZpYCrDCzV5xzG+q1uRAYHJwmAA8FX0VEJEKEvIXgnNvlnFsZnC8DNgK9Tmg2DXjS1XkL6GRmPULtW0REvOPFFsK/mVl/YCzw9gkf9QI+qre8I/jergbWMQuYBRAIBMb37dvXy4ieq62tJS6u5XX1cKXjcKWjY6LROWAeJvv/hZozXJTTW8rprWjIuWnTpv3OufQW/bBzzpMJSAZWAJc18Nki4Nx6y0uAzKbWOWTIEBfpli5dGtLP19bWulufW+363bLIPfnm+55kakioOcNFOb2lnN6KhpxAsWvh33FPthDMrB3wHDDfOffnBprsBPrUW+4dfK/NMzPunTaSfWWV3LlwPekpAaaO7O53LBFpg7w4y8iAJ4CNzrlfN9JsITA9eLbRWUCJc+6/dhe1VQnxcfzu6nGM6dOJm555h6LtB/2OJCJtkBc7w84BrgM+Y2argtNFZjbbzGYH2ywGtgFbgMeAOR70G1PaJ8bzRE4WvTu1Z+a8IjbvKfM7koi0MSHvMnLOvQGc9GhocL/W10LtK9Z16ZBIQV42lz30Jjn5hfx5zjl07xjwO5aItBGRfbi8DerT5TTm5mZRWlFN7txCSo4d9zuSiLQRKggRaGSvjjz85fFs3VfOV54qprK6xu9IItIGqCBEqHMHp/GLy0fz1raD3LxgNbW1erKdiLQuTy9ME29dMrYXe0oruO+ld8lICXDH58+g7qQuERHvqSBEuFnnDWR3aQX5/3qf7h2TmHXe6X5HEpEYpYIQ4cyMO/5nOHvLKvnJ4nfJSA0wbcyJt4oSEQmdCkIUiIszfnXFaPaXVfKdP64mLTmJcwal+R1LRGKMDipHiUC7eB6dnsnp6cl85akVrP+4xO9IIhJjVBCiSMf27Zg3I5vUQAK5c4v46OBRvyOJSAxRQYgy3TsGmJeXTeXxGnLmFnLoSJXfkUQkRqggRKEhGSk8kZvFjkPHyCso4liVLlwTkdCpIESprP5duP+qMaz66DBff/odqmtq/Y4kIlFOBSGKTR3Zg3suHsGrG/dwxwvrP3n4kIhIi+i00yg3fWJ/dpdU8PtlW+meGuAb5w/2O5KIRCkVhBjw3QuGsru0gt+8uonuHZP4UlZkP4daRCKTCkIMMDN+9sUz2V9exfefX0d6ShKfGZbhdywRiTI6hhAj2sXH8dC14xjeI5U581fyzoeH/I4kIlFGBSGGdEhKID83i24pAWYWFLNtX7nfkUQkinhSEMws38z2mtm6Rj6fZGYl9Z65fKcX/cp/S09JoiAvG4CcuYXsLavwOZGIRAuvthDmAVObaPNP59yY4PRDj/qVBgxI60B+bhb7y6rIm1dEeWW135FEJAp4UhCcc68DB71Yl3hjTJ9O/P7acWzcVcZX/7CCaj1xTUSaEM5jCBPNbLWZvWRmI8LYb5s1eVg37rtsFP/cvJ/8dVW6cE1ETsq8+iNhZv2BRc65kQ18lgrUOufKzewi4LfOuQavoDKzWcAsgPT09PELFizwJF9rKS8vJzk52e8YJ7VwaxV/3nyciwa048qhiX7HOaloGE9QTq8pp3cmT568wjmX2aIfds55MgH9gXXNbLsdSGuq3ZAhQ1ykW7p0qd8RmlRbW+tyH/ib63fLIjf3jW1+xzmpaBhP55TTa8rpHaDYtfDveFguTDOz7sAe55wzs2zqdlUdCEffUnfh2nXDE0lI6co9izbQLTXARaN6+B1LRCKMV6edPg0sB4aa2Q4zm2lms81sdrDJ5cA6M1sN3A9cFaxkEiZxZvzu6rGM69uZbz67ire3qR77aflyuO++uleRSOHJFoJz7uomPn8AeMCLvqTlAu3ieXx6Jpc//CY3PFnMn756NkMyUvyO1eYsXw5TpkBVFSQmwpIlMHGi36lEdKVym9O5QyIFedkE2sWTk1/Ix4eP+R2pzVm2rK4Y1NTUvS5b5ncikToqCG1Q786nMW9GNmUV1eTOLaTk2HG/I7UpkybVbRnEx9e9TprkdyKROioIbdTwnqk8et143t9/hBueLKbiuB7DGS4TJ9btJrr3Xu0uksiigtCGnT0ojV9dOYbC9w9y84JV1Ohq5rCZOBFuu03FQCKLnofQxl08uid7Syv40Ysb6Zaygbu+MBwz8zuWiPhABUG4/lMD2VVSwRNvvE/3jgFmf/p0vyOJiA9UEASA2y86g71llfz0pXfplpLEZeN6+x1JRMJMBUEAiIszfnnFmewvq+R7f1pDekoSnxqc7ncsEQkjHVSWf0tKiOeR6eMZ1C2Z2U+tYN3OEr8jiUgYqSDI/yc10I6CvGw6nZZI7twiPjxw1O9IIhImKgjyXzJSAxTkZXG8ppacuYUcPFLldyQRCQMVBGnQoG4pPJGTyceHj5E3r4ijVXoMp0isU0GQRmX278L9V49lzY7DfP3/3qG6ptbvSCLSilQQ5KQuGNGdH04byZJ39/KDv6zTYzhFYphOO5UmffmsfuwuqeCBpVvISA3wrc8O8TuSiLQCFQRplm9/bgh7Siv47ZLNZKQGuGZCX78jiYjHVBCkWcyMn1w2in3llfzgL2vplpLE+cMz/I4lIh7SMQRptnbxcTx4zThG9urIjU+vZOWHh/yOJCIeUkGQU9IhKYH83Cy6pwaYOa+IrfvK/Y4kIh7xpCCYWb6Z7TWzdY18bmZ2v5ltMbM1ZjbOi37FH2nJSRTkZRNnRk5+IXtLK/yOJCIe8GoLYR4w9SSfXwgMDk6zgIc86ld80q9rB+bOyOLgkSpy5xZRVqHHcIo/li+H++6re5XQeFIQnHOvAwdP0mQa8KSr8xbQycx6eNG3+OfM3p34/bXj2LSnjNl/WEFVtS5ck/BavhymTIE77qh7VVEIjXl1oZGZ9QcWOedGNvDZIuCnzrk3gstLgFucc8UNtJ1F3VYE6enp4xcsWOBJvtZSXl5OcnKy3zGa1Jo539h5nMfXVjGxRzw3nJlEXAhPXNN4eivWc86f35f8/AHU1hpxcbXk5W3n2ms/bIWEdaJhPCdPnrzCOZfZoh92znkyAf2BdY18tgg4t97yEiCzqXUOGTLERbqlS5f6HaFZWjvnA69tdv1uWeR+8uKGkNaj8fRWrOd8803n2rd3Lj6+7vXNN73NdaJoGE+g2LXw73i4rkPYCfSpt9w7+J7EiDmTTmd3SQWPvL6NjNQAeecO8DuStAETJ8KSJbBsGUyaVLcsLReugrAQuNHMngEmACXOuV1h6lvCwMy4++IR7C2r4N4XN9AtNYnPn9nT71jSBkycqELgFa9OO30aWA4MNbMdZjbTzGab2exgk8XANmAL8Bgwx4t+JbLExxm/vWos4/t25uZnV7N86wG/I4nIKfBkC8E5d3UTnzvga170JZEt0C6ex3Myufzh5cx6qpg/zp7IsO6pfscSkWbQlcriuU6nJVKQl81pifHk5Bey8/AxvyOJSDOoIEir6NWpPQV52RytrCE3v5DDR/UYTpFIp4IgrWZY91QemT6eDw4c5YYni6k4XuN3JBE5CRUEaVVnn57Gr64cTdH2Q3zzmVXU1OqJayKRSgVBWt0XRvfkjs8P52/rd3PPX9frMZwiEUoPyJGwmHnuAPaUVvDo69vo3jHAnEmD/I4kIidQQZCwuXXqMPaUVvDzv71HRkqAL47v7XckEalHBUHCJi7O+MXlo9lfXsktz62ha3Iik4Z28zuWiATpGIKEVWJCHA9/eTyDM1KYM38la3eU+B1JRIJUECTsUgLtKJiRRefTEpkxr5APDhzxO5KIoIIgPumWGuDJmdlU1zpy8gvZX17pdySRNk8FQXxzenoyT+RksaukgpnzijhaVe13JJE2TQVBfDW+X2ceuGYca3eW8LX5K6nWhWsivlFBEN99dngGP7pkFEvf20fB+ipduCbiExUEiQjXTOjLTVMG88+d1fzmlU1+xxFpk1QQJGJ86/zBnNc7gftf28L8tz/wO45Im6ML0yRimBk5wxNJSO7CHX9ZR1pyEheM6O53LJE2Q1sIElHi44wHrhnLqN6duOnpd1jxwUG/I4m0GV49U3mqmb1nZlvM7NYGPs81s31mtio4Xe9FvxKbTktMID8nk56d2jOzoJgte8v9jiTSJoRcEMwsHngQuBAYDlxtZsMbaPqsc25McHo81H4ltnVNTqJgRjYJcXHk5Beyp7TC70hRZ/lymD+/L8uX+51EooUXWwjZwBbn3DbnXBXwDDDNg/VKG9e362nMm5HF4aNV5OQXUlpx3O9IUWP5cpgyBfLzBzBlCioK0ixeHFTuBXxUb3kHMKGBdl80s/OATcC3nHMfNdAGM5sFzAJIT09n2bJlHkRsPeXl5RGfEaI751fPTOA3K8r40v2vcnNmgHZx5k+4eiJ9POfP70tl5QBqa43Kylry87dTWfmh37EaFenj+YloydlizrmQJuBy4PF6y9cBD5zQpiuQFJz/CvBac9Y9ZMgQF+mWLl3qd4Rmifacz634yPW7ZZG78f9Wupqa2vCGakCkj+ebbzrXvr1zcXE1rn37uuVIFunj+YloyAkUuxb+Pfdil9FOoE+95d7B9+oXnQPOuU/uXvY4MN6DfqUNuWxcb26ZOoy/rv6Y+17a6HeciDdxIixZAnl521mypG5ZpCle7DIqAgab2QDqCsFVwDX1G5hZD+fcruDixYD+j5ZTNvvTA9ldcozH/vk+GakBrv/UQL8jRbSJE6Gy8kMmTtQ4SfOEXBCcc9VmdiPwMhAP5Dvn1pvZD6nbdFkI3GRmFwPVwEEgN9R+pe0xM+78wgj2lVfyoxc30i01wMWje/odSyRmeHKlsnNuMbD4hPfurDd/G3CbF31J2xYfZ/z6yjHsLyvk2wtWkdYhkbMHpfkdSyQm6EpliTqBdvE8Nj2TAWkd+MpTK9jwcanfkURiggqCRKWOp7Vj3oxsOiQlkDu3kB2HjvodSSTqqSBI1OrZqT0FedkcO15DTn4hh49W+R1JJKqpIEhUG9o9hcemZ/LRwWPMLCim4niN35FEopYKgkS9swZ25X+vGsPKDw9x09PvUKPHcIq0iAqCxISLRvXgrs8P5+8b9nDXwnV6DKdIC+gBORIzcs8ZwK7SCh75xza6pwa48TOD/Y4kElVUECSm3HLBMPaWVvLLv28iIzXAFZl9mv4hEQFUECTGxMUZP/vimewvr+TWP68lLSWJyUO7+R1LJCroGILEnMSEOB768niGdU9hzh9Wsvqjw35HEokKKggSk5KTEpg7I4uuyYnkzSti+/4jfkcSiXgqCBKzuqUEeDIvm1rnyJlbyP7yyqZ/SKQNU0GQmDYwPZn83Cz2lFaQN6+II5XVfkcSiVgqCBLzxvbtzIPXjGPdzhLmzF/J8ZpavyOJRCQVBGkTppyRwU8uHcU/Nu3j1ufW6sI1kQbotFNpM67K7svu0gr+99XNdO+YxHcvGOZ3JJGIooIgbco3pgxmT2kFDy7dSvfUANdN7O93JJGIoYIgbYqZce+0kewrq+TOhetJTwkwdWR3v2OJRARPjiGY2VQze8/MtpjZrQ18nmRmzwY/f9vM+nvRr0hLJMTH8burxzGmTydueuYdirYf9DuSSEQIuSCYWTzwIHAhMBy42syGn9BsJnDIOTcI+A3ws1D7FQlF+8R4nsjJonen9lxfUMzmPWV+RxLxnRdbCNnAFufcNudcFfAMMO2ENtOAguD8n4ApZmYe9C3SYl06JFKQl01iQhw5+YXsLqnwO5KIryzU0+/M7HJgqnPu+uDydcAE59yN9dqsC7bZEVzeGmyzv4H1zQJmAaSnp49fsGBBSPlaW3l5OcnJyX7HaJJyNu6D0hrue7uCtPbGbRPa06Fd099VNJ7eUk7vTJ48eYVzLrMlPxtxB5Wdc48CjwIMHTrUTZo0yd9ATVi2bBmRnhGUsymnn7Gf3LmFPPV+gIK8bJIS4k/aXuPpLeWMDF7sMtoJ1L/pfO/gew22MbMEoCNwwIO+RTxx7uA0fnnFaN7adpCbF6ymVo/hlDbIi4JQBAw2swFmlghcBSw8oc1CICc4fznwmtOlohJhLhnbi9suHMaLa3bxoxc36mpmaXNC3mXknKs2sxuBl4F4IN85t97MfggUO+cWAk8AT5nZFuAgdUVDJOLMOm8gu0sryP/X+/ToGOCG8wb6HUkkbDw5huCcWwwsPuG9O+vNVwBXeNGXSGsyM+74n+HsLa3kx4s30i01iWljevkdSyQsIu6gsojf4uKMX105mv3llXznj6tJS07inEFpfscSaXW626lIAwLt4nl0eiYD05L5ylMrWP9xid+RRFqdCoJIIzq2b8e8vCxSAgnkzi3io4NH/Y4k0qpUEEROokfH9hTkZVN5vIacuYUcOlLldySRVqOCINKEIRkpPJ6TxY5Dx5hZUMSxqhq/I4m0ChUEkWbIHtCF+68awzsfHebrT79DjS5ckxikgiDSTFNH9uCei0fw6sY9PLWhSheuScxRQRA5BdMn9mfOpNNZtqOa3722xe84Ip5SQRA5Rd+9YCjn9Ezg169s4tmiD/2OI+IZXZgmcorMjBkjE4lP7sz3n19HekoSnxmW4XcskZBpC0GkBRLijN9fO47hPVL52vx3WPXRYb8jiYRMBUGkhZKTEsjPzSI9JYm8eUVs21fudySRkKggiIQgPSWJgrxsAHLmFrKvrNLnRCItp4IgEqIBaR3Iz81if1kVM+YVUl5Z7XckkRZRQRDxwJg+nfj9tePYuKuMr/5hBVXVtX5HEjllKggiHpk8rBv3XTqKf27ez63PrdGFaxJ1dNqpiIeuzOrD7tIKfv3KJjI6Brhl6jC/I4k0mwqCiMe+/plB7C6t4KFlW+meGiDn7P5+RxJplpB2GZlZFzN7xcw2B187N9KuxsxWBaeFofQpEezuu/1OEBHMjHunjeSzwzO4+6/reWntLr8jiTRLqMcQbgWWOOcGA0uCyw055pwbE5wuDrFPiVT33ON3gogRH2fcf9VYxvbpxDeeXUXh+wf9jiTSpFALwjSgIDhfAFwS4vpEYkb7xHieyMmiT+f2XF9QxKY9ZX5HEjkpC+VMCDM77JzrFJw34NAnyye0qwZWAdXAT51zfznJOmcBswDS09PHL1iwoMX5wqG8vJzk5GS/YzSptXL2nzeP/gUF//X+9pwctufmnvL6YnE89x+r5UdvVRBn8IOzAnQJhO/kvlgcTz9FQ87JkyevcM5ltuiHnXMnnYBXgXUNTNOAwye0PdTIOnoFXwcC24HTm+rXOceQIUNcpFu6dKnfEZolLDkh5FXE6niu23nYjbjzb+5zv/6HO3y0qnVCNSBWx9Mv0ZATKHbN+Pva0NTkVxXn3PnOuZENTC8Ae8ysB0DwdW8j69gZfN0GLAPGtqh6iUSpET078uh149m2v5wbniym4rgewymRJ9Rt14VATnA+B3jhxAZm1tnMkoLzacA5wIYQ+5VIdNddfieIaGcPSuOXV4ym8P2D3LxgFbV6DKdEmFALwk+Bz5rZZuD84DJmlmlmjwfbnAEUm9lqYCl1xxBUEGKRTjtt0rQxvbj9ojNYvHY3P1y0QVczS0QJ6cI059wBYEoD7xcD1wfn3wRGhdKPSCy54byB7C6t4Ik33qd7xwCzP32635FEAF2pLOKL2y86gz2lFfz0pXfJSE3i0rG9/Y4kooIg4oe4OONXV47mQHkV3/3jGtKSk/jU4HS/Y0kbp7udivgkKSGeR6aPZ1C3ZGY/tYJ1O0v8jiRtnAqCiI9SA+0oyMum02mJ5M4t4qODR/2OJG2YCoKIzzJSAxTkZXG8ppbp+YUcPFLldyRpo1QQRCLAoG4pPJGTyceHj5E3r4ijVXoMp4SfCoJIhMjs34X7rx7Lmh2H+fr/vUN1jR7DKeGlgiASQS4Y0Z17po1kybt7+cFf1unCNQkrnXYqEmGuO6sfe0oqeGDpFjJSA3zrs0P8jiRthAqCSAT69ueGsLu0gt8u2Uz3jgGuzu7rdyRpA1QQRCKQmXHfZaPYX17J7c+vJT05ifOHZ/gdS2KcjiGIRKh28XE8eM04RvbqyI1Pr2Tlh4f8jiQxTgVBJIJ1SEogPzeLjNQAM+cVsXVfud+RJIapIIhEuLTkJJ7MyybOjJz8QvaWVvgdSWKUCoJIFOjXtQNzZ2Rx8EgVuXOLKKs47nckiUEqCCJR4szenXjw2nG8t6eMr/5hJVXVunBNvKWCIBJFJg/txk8vG8UbW/bzvT+t1mM4xVM67VQkylyR2Ye9ZZX84uX3yOgY4LYLz/A7ksSIkLYQzOwKM1tvZrVmlnmSdlPN7D0z22Jmt4bSp4jAnEmnc91Z/XjkH9uY+6/3/Y4jMSLUXUbrgMuA1xtrYGbxwIPAhcBw4GozGx5ivyJtmplx98UjuGBEBj9ctIFFaz72O5LEgJAKgnNuo3PuvSaaZQNbnHPbnHNVwDPAtFD6FRGIjzN+e9VYxvftzM3PruatbQf8jiRRzry4m6KZLQO+45wrbuCzy4Gpzrnrg8vXAROcczc2sq5ZwCyA9PT08QsWLAg5X2sqLy8nOTnZ7xhNUk5vRVLO8irHT94+xqFKx/cntKdPyn++50VSzpNRTu9Mnjx5hXOu0V34J9PkQWUzexXo3sBHtzvnXmhJpyfjnHsUeBRg6NChbtKkSV534ally5YR6RlBOb0WaTnHZh/jst//iwfXOv48ZwI9O7UHIi9nY5QzMjS5y8g5d75zbmQDU3OLwU6gT73l3sH3RMQjvTq1Z96MbI5UVpOTX0jJUV24JqcuHNchFAGDzWyAmSUCVwELw9CvSJtyRo9UHpk+ng8OHOWGJ4upOF7jdySJMqGednqpme0AJgIvmtnLwfd7mtliAOdcNXAj8DKwEVjgnFsfWmwRacjZp6fxqytHU7j9IN98ZhW1euKanIKQLkxzzj0PPN/A+x8DF9VbXgwsDqUvEWmeL4zuyd6ySu5dtIHjZQlMnuQwM79jSRTQrStEYtDMcwdww6cGsOTDah76x1a/40iUUEEQiVG3XXgGZ/WI5+d/e4/nVuzwO45EAd3LSCRGxcUZ149KIr5De255bg1pKUl8eki637EkgmkLQSSGJcQZD395PIMzUvjqH1awdkeJ35EkgqkgiMS4lEA7CmZk0fm0RGbMK+SDA0f8jiQRSgVBpA3olhqgIC+b6lpHTn4hB8or/Y4kEUgFQaSNGNQtmSdyMtlVUkHevCKOVlX7HUkijAqCSBsyvl8XHrhmHGt3lvC1+Ss5XqPHcMp/qCCEqP+8eX5HEDklnx2ewb2XjGTpe/u4/fm1eHHHY4kNKggh6l9Q4HcEkVN27YR+3PSZQSwo3sFvXtnkdxyJELoOQaSN+tZnh7CntJL7X9tCRscA107o53ck8Zm2EFri7rvBrG6C/8zffbefqUROiZnx40tHMnloOnf8ZR1/X7/b70jiMxWElrj7bnCuboL/zKsgSJRJiI/jwWvHMap3J77+9Dus+OCg35HERyoIIn6JkC8QpyUmkJ+TSc9O7ZlZUMyWveV+RxKfqCCEaHtOjt8RJFrdc4/fCf6ta3ISBTOySYgzcvIL2VNa4Xck8YEKQoi25+b6HUHEE327nsbc3GwOH60id24RpRV6DGdbo4IgEk4RfkLCqN4deejL49m8p4zZT62gslqP4WxLVBBEwikKTkg4b0g6P7/8TN7ceoDv/nENtbW6cK2tCPWZyleY2XozqzWzzJO0225ma81slZkVh9KniLS+y8b15ntTh7Jw9cfc99JGv+NImIR6Ydo64DLgkWa0neyc2x9ifyKx4667/E5wUl/99OnsKangsX++T0ZqgOs/NdDvSNLKQioIzrmNgB7gLdISEbSbqCFmxp1fGMHeskp+9OJGMlIDfGF0T79jSSsK1zEEB/zdzFaY2aww9SkiIYqPM37zpTFk9+/Ctxes5s2t2siPZdbUnQ7N7FWgewMf3e6ceyHYZhnwHedcg8cHzKyXc26nmXUDXgG+7px7vZG2s4BZAOnp6eMXLFjQ3P8WX5SXl5OcnOx3jCYpp7faWs4jxx0/fvsYhyoc35/Qnj4p3n6XbGvj2ZomT568wjnX6DHdk3LOhTwBy4DMZra9m7ri0WTbIUOGuEi3dOlSvyM0i3J6qy3m3HnoqJvw41dd9o9fcTsOHfVsvc61zfFsLUCxa+Hf8lbfZWRmHcws5ZN54HPUHYwWkSjSs1N7CvKyOVpVQ05+IYePVvkdSTwW6mmnl5rZDmAi8KKZvRx8v6eZLQ42ywDeMLPVQCHwonPub6H0KyL+GNo9hcemZ/LhgaNcX1BMxXFduBZLQioIzrnnnXO9nXNJzrkM59wFwfc/ds5dFJzf5pwbHZxGOOd+7EVwEfHHWQO78psvjWHFh4e46el3qNGFazFDVyqLyCn7nzN7cOfnh/P3DXu4a+E6PYYzRuiJaSLSIjPOGcDu0goe+cc2enRsz9cmD/I7koRIBUFEWuyWC4axp6SCX7z8Ht1Skrgis4/fkSQEKggi0mJxccbPLx/NgSNV3PrntaSlJDF5aDe/Y0kL6RiCiIQkMSGOh748nmHdU5jzh5Ws/uiw35GkhVQQRCRkyUkJzJ2RRdfkRPLmFbF9/xG/I0kLqCCIiCe6pQR4Mi+bWufImVvI/vJKvyPJKVJBEBHPDExP5oncLPaUVpA3r4gjldV+R5JToIIgIp4a17czD1w9jnU7S5gzfyXHa2r9jotoU0wAAAbYSURBVCTNpIIgIp47f3gGP7l0FP/YtI9bn1urC9eihE47FZFWcVV2X3aXVvC/r26mR8cA37lgqN+RpAkqCCLSar4xZTB7Sit4YOkWMjoGuO6sfn5HkpNQQRCRVmNm3DttJPvKKrnzhXWkJycxdWRDz9uSSKBjCCLSqhLi4/jd1eMY3bsTNz3zDkXbD/odSRqhgiAira59Yjz5uVn07tSe6wuK2bynzO9I0gAVBBEJiy4dEinIyyYxIY6c/EJ2l1T4HUlOoIIgImHTp8tpzM3NouTYcXLnFlJacdzvSFKPCoKIhNXIXh15+LrxbNlbzqwni6ms1mM4I4UKgoiE3acGp/PLK0bz1raD3LxgNbW6cC0ihFQQzOwXZvauma0xs+fNrFMj7aaa2XtmtsXMbg2lTxGJDZeM7cVtFw7jxTW7ePbdKr/jCKFvIbwCjHTOnQlsAm47sYGZxQMPAhcCw4GrzWx4iP2KSAyYdd5Acs/uz8sfVPPY69v8jtPmhVQQnHN/d859cjvDt4DeDTTLBrY457Y556qAZ4BpofQrIrHBzLjz88PJ6h7Pjxdv5IVVO/2O1KZ5eaVyHvBsA+/3Aj6qt7wDmNDYSsxsFjAruFhpZus8S9g60oD9fodoBuX0lnJ6Kw3Yf8nP/I7RpGgYzxbfNKrJgmBmrwINXWt+u3PuhWCb24FqYH5Lg3zCOfco8GhwvcXOucxQ19maoiEjKKfXlNNbyukdMytu6c82WRCcc+c30Xku8Hlgimv4Hrc7gT71lnsH3xMRkQgS6llGU4HvARc754420qwIGGxmA8wsEbgKWBhKvyIi4r1QzzJ6AEgBXjGzVWb2MICZ9TSzxQDBg843Ai8DG4EFzrn1zVz/oyHmC4doyAjK6TXl9JZyeqfFGU1PMhIREdCVyiIiEqSCICIiQIQVhGi4FYaZXWFm682s1swaPf3MzLab2drgsZUWnwbWUqeQ09fbiphZFzN7xcw2B187N9KuJjiWq8wsbCclNDU+ZpZkZs8GP3/bzPqHK9sJOZrKmWtm++qN4fU+ZMw3s72NXVtkde4P/jesMbNx4c4YzNFUzklmVlJvLO/0IWMfM1tqZhuC/59/o4E2pz6ezrmImYDPAQnB+Z8BP2ugTTywFRgIJAKrgeFhzHgGdRd+LAMyT9JuO5Dm41g2mdPvsQxm+Dlwa3D+1ob+zYOflfswhk2ODzAHeDg4fxXwbITmzAUeCHe2EzKcB4wD1jXy+UXAS4ABZwFvR2jOScAin8eyBzAuOJ9C3a2DTvw3P+XxjKgtBBcFt8Jwzm10zr0Xrv5aqpk5I+G2ItOAguB8AXBJmPs/meaMT/38fwKmmJmFMSNExr9jk5xzrwMne37mNOBJV+ctoJOZ9QhPuv9oRk7fOed2OedWBufLqDuDs9cJzU55PCOqIJwgj7rqdqKGboVx4kBEAgf83cxWBG/HEYkiYSwznHO7gvO7gYxG2gXMrNjM3jKzcBWN5ozPv9sEv8yUAF3Dkq6BDEGN/Tt+Mbjr4E9m1qeBz/0WCb+PzTXRzFab2UtmNsLPIMHdlGOBt0/46JTH08t7GTVLuG+F0RLNydgM5zrndppZN+qu03g3+M3DMx7lbHUny1l/wTnnzKyx86D7BcdzIPCama11zm31OmsM+yvwtHOu0sy+Qt1WzWd8zhStVlL3+1huZhcBfwEG+xHEzJKB54BvOudKQ11f2AuCi4JbYTSVsZnr2Bl83Wtmz1O3We9pQfAgZ1huK3KynGa2x8x6OOd2BTdn9zayjk/Gc5uZLaPuG1FrF4TmjM8nbXaYWQLQETjQyrlO1GRO51z9TI9Td+wm0kTFbW7q/+F1zi02s9+bWZpzLqw3vTOzdtQVg/nOuT830OSUxzOidhlZjNwKw8w6mFnKJ/PUHSyPxLu2RsJYLgRygvM5wH9t2ZhZZzNLCs6nAecAG8KQrTnjUz//5cBrjXyRaU1N5jxh3/HF1O1zjjQLgenBs2POAkrq7U6MGGbW/ZPjRGaWTd3f0bB+CQj2/wSw0Tn360aanfp4+nmkvIEj51uo2+e1Kjh9cvZGT2DxCUfPN1H3DfH2MGe8lLp9cZXAHuDlEzNSd7bH6uC0PtwZm5vT77EM9t8VWAJsBl4FugTfzwQeD86fDawNjudaYGYY8/3X+AA/pO5LC0AA+GPwd7cQGBjuMWxmzvuCv4urgaXAMB8yPg3sAo4HfzdnArOB2cHPjbqHaW0N/js3ehafzzlvrDeWbwFn+5DxXOqOU66p9/fyolDHU7euEBERIMJ2GYmIiH9UEEREBFBBEBGRIBUEEREBVBBERCRIBUFERAAVBBERCfp//lPH6lLoXYcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution here\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(params, lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for i in range(10000):\n",
    "  # in your training loop:\n",
    "  # 1. zero the gradient buffers\n",
    "  optimizer.zero_grad()\n",
    "  # 2. compute the output by given the input x\n",
    "  out = my_neuron.forward(x)\n",
    "  my_neuron.zero_grad()\n",
    "  # 3. compute the loss\n",
    "  outputs = torch.cat((out,input), 1)\n",
    "  loss = criterion(outputs,y)\n",
    "  # 4. computing the gradient \n",
    "  loss.backward(retain_graph=True)\n",
    "  # 5. update the parameter by calling step on the optimizer\n",
    "  optimizer.step()\n",
    "\n",
    "print(\"loss =\", loss.item())\n",
    "params = list(my_neuron.parameters())\n",
    "plot_soln(x, y, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXQQ5Vxjue5D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework9(1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
