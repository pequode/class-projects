{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndQNBUG-O___"
   },
   "source": [
    "# Homework 3: Optimization, KNN and Decision Trees\n",
    "by Rachel Manzelli and Brian Kulis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTV9mcpnPAAJ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbqLsA-NPAAK"
   },
   "source": [
    "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
    "\n",
    "To use Google Colab:\n",
    "\n",
    "1. Download this `ipynb` file.\n",
    "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
    "3. Upload this file. It will then open in Colab.\n",
    "\n",
    "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
    "\n",
    "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
    "\n",
    "**Submission instructions**: please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by **February 24th at midnight**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js5teW_jPAAM"
   },
   "source": [
    "### 1. Maximum Likelihood without a Closed-Form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G66uA3ldPAAM"
   },
   "source": [
    "Assume that we are given $n$ IID samples ${x_1,...x_n}$ from the following $P(X|\\theta)$:\n",
    "\n",
    "$$P(X|\\theta) = \\frac{1}{\\pi}\\bigg [\\frac{1}{(x-\\theta)^2+1}\\bigg ]$$\n",
    "\n",
    "#### 1.a. Try to compute the MLE via maximizing the log-likelihood function directly, and briefly explain why this won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSxfTav9PAAN"
   },
   "source": [
    "\n",
    "\n",
    "*   $p(X|\\theta) = \\frac{1}{\\pi}[\\frac{1}{(x-\\theta)^2+1}]$\n",
    "*   $ln(p(X|\\theta)) = ln(\\frac{1}{\\pi})+ln(\\frac{1}{(x-\\theta)^2+1})$\n",
    "*   $ln(p(X|\\theta)) = ln(\\frac{1}{\\pi})+ln(1) - ln([x-\\theta]^2+1)$\n",
    "*   $ln(p(X|\\theta)) =$~$-1.1 - ln([x-\\theta]^2+1)$\n",
    "*   $\\nabla ln(p(X|\\theta)) =\\nabla($~$-1.1 - ln([x-\\theta]^2+1))$\n",
    "*   $\\nabla ln(p(X|\\theta)) =(0- \\frac{1}{(x-\\theta)^2+1}(0+2(x-\\theta))$\n",
    "*   $0 = -\\frac{2(x-\\theta)}{[x-\\theta]^2+1} $\n",
    "*   $0 = \\frac{(x-\\theta)}{[x-\\theta]^2+1} $\n",
    "*   $0 = (x-\\theta)$\n",
    "*   $x = \\theta$\n",
    "\n",
    "The only place this is maximizeds is when x = theta and that wont work because the xs are IID (there may be stuff here about taking the seccond derivitive). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksa2a1XEPAAO"
   },
   "source": [
    "#### 1.b. Convert the objective (log-likelihood) function into a cost function, $J(\\theta)$, that we can minimize using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KlIt8F5PAAO"
   },
   "source": [
    "*    $J(\\theta) =  MSE = \\frac{1}{n}\\sum_{i=0}^{n}((f(xi|θ)−yi)^2)$\n",
    "*    $J(\\theta)= \\frac{1}{n}\\sum_{i=0}^{n}(($~$-1.144 - ln([x-\\theta]^2+1)−yi)^2)$\n",
    "*    $J(\\theta)= \\frac{1}{n}\\sum_{i=0}^{n}(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3QXCiT0PAAP"
   },
   "source": [
    "#### 1.c. Compute the gradient descent update rule, where $\\theta_{n+1}=\\theta_n - \\alpha\\frac{d}{d\\theta_n}J(\\theta_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkcD_lB_PAAP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "*    $J(\\theta)= \\frac{1}{n}\\sum_{i=0}^{n}(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)^2)$\n",
    "*    $\\nabla J(\\theta)= \\nabla \\frac{1}{n}\\sum_{i=0}^{n}(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)^2)$\n",
    "*    $\\nabla J(\\theta)=  \\frac{1}{n}\\sum_{i=0}^{n}\\nabla(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)^2)$\n",
    "*    $\\nabla J(\\theta)=  \\frac{1}{n}\\sum_{i=0}^{n}2(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)\\nabla(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi))$\n",
    "*    $\\nabla J(\\theta)=  \\frac{1}{n}\\sum_{i=0}^{n}2(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)(0 - \\nabla ln([x-\\theta]^2+1)-0))$\n",
    "*    $\\nabla J(\\theta)=  \\frac{1}{n}\\sum_{i=0}^{n}2(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)( -\\frac{1}{([x-\\theta]^2+1)} \\nabla ([x-\\theta]^2+1)))$\n",
    "*    $\\nabla J(\\theta)=\\frac{1}{n}\\sum_{i=0}^{n}2(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)( -\\frac{1}{([x-\\theta]^2+1)} 2(x-\\theta)))$\n",
    "*    $\\nabla J(\\theta)=\\frac{1}{n}\\sum_{i=0}^{n}2(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1)−yi)( -\\frac{2(x-\\theta)}{([x-\\theta]^2+1)} ))$\n",
    "*    $\\nabla J(\\theta)=\\frac{2}{n}\\sum_{i=0}^{n}(ln(\\frac{1}{\\pi}) - ln([x-\\theta]^2+1−yi)( -\\frac{2(x-\\theta)}{([x-\\theta]^2+1)} ))$\n",
    "*    $\\theta_{n} = 0.5, \\alpha = 0.1$\n",
    "*    $\\theta_{n+1} = 0.5 + 0.1(\\frac{2}{n}\\sum_{i=0}^{n}(ln(\\frac{1}{\\pi}) - ln([x-0.5]^2+1−yi)( -\\frac{2(x-0.5)}{([x-0.5]^2+1)} ))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cziySqKlPAAQ"
   },
   "source": [
    "#### 1.d. Write the pseudocode for gradient descent with this update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agcloTp0PAAQ"
   },
   "source": [
    "\n",
    "*   theta = guess\n",
    "*   alpha = learnrate\n",
    "*   for i in 1000:\n",
    "*    - sum:\n",
    "*   - for j in n:\n",
    "*   - - sum = sum+((-ln(pi)-ln((x[j]-theta)^2+1-y[j])(2(x[j]-theta)/((x[j]-theta)^2+1))):\n",
    "*   - sum = sum*2/n\n",
    "*   - changeamount = sum*alpha\n",
    "*   - if(changeamount<threshold):\n",
    "*   - - break loop\n",
    "*   - theta = theta + changeamount\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh0IBSfEPAAR"
   },
   "source": [
    "#### 1.e. (Bonus) Compute the stochastic gradient descent (SGD) update rule, and write pseudocode for SGD with this update rule. (Assume a mini-batch size of 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LC1b7twKPAAR"
   },
   "source": [
    "\n",
    "*   theta = guess\n",
    "*   alpha = learnrate\n",
    "*   for i in 1000:\n",
    "*    - sum:\n",
    "*    - randInd = rand int in range (0-n)\n",
    "*   - - sum = sum+((-ln(pi)-ln((x[randInd]-theta)^2+1-y[randInd])(2(x[randInd]-theta)/((x[randInd]-theta)^2+1))):\n",
    "*   - sum = sum*2/n\n",
    "*   - changeamount = sum*alpha\n",
    "*   - if(changeamount<threshold):\n",
    "*   - - break loop\n",
    "*   - theta = theta + changeamount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLd_f7FSPAAR"
   },
   "source": [
    "### 2. Decision Trees\n",
    "\n",
    "The following dataset contains information about different weather attributes, along with whether the golf team decided to play. Each row represents the characteristics of one day.\n",
    "\n",
    "\n",
    "| Temp        | Humidity    | Wind        | Play?       |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| hot         | normal      | strong      | no          |\n",
    "| mild        | high        | strong      | yes         |\n",
    "| hot         | normal      | strong      | no          |\n",
    "| hot         | normal      | weak        | yes         |\n",
    "| mild        | normal      | strong      | yes         |\n",
    "\n",
    "\n",
    "We will construct a decision tree that predicts whether or not the current weather attributes are appropriate for playing golf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR2Jhgw3PAAS"
   },
   "source": [
    "#### 2.a. Choose a root node.\n",
    "Follow the method of using information gain to choose a root node for our decision tree, as described in class (and posted in the slides on Blackboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcMSncT9PAAS"
   },
   "source": [
    "\n",
    "\n",
    "*   Temp\n",
    "    - p1 = 4/5\n",
    "    - p0 = 1/5 \n",
    "  - - p1hot = 2/3\n",
    "  - - p0hot = 1/3\n",
    "  - - p1mild = 2/2\n",
    "  - - p0mild = 0/2\n",
    "  - hs(temp) = -p1log_2(p1)-p2log_2(p2)\n",
    "  - - hs(hot) = -p1hotlog_2(p1hot)-p2log_2(p2hot)\n",
    "  - - hs(mild) = -p1mildlog_2(p1mild)-p2log_2(p2mild)\n",
    "    - G = hs(temp) - 3/5*hs(hot)-2/5hs(mild)= 0.14739311883324124\n",
    "*   Humidity\n",
    "  - p1(normal)2/4\n",
    "  - p0(normal) 2/4\n",
    "  - p1(high) 0/1\n",
    "  - p0(high) 1/1\n",
    "  - ect\n",
    "  - G = 0.04217935649972371\n",
    "*   wind \n",
    "  - ect\n",
    "  - G = 0.12877123795494477\n",
    "\n",
    "  - p0(strong) 2/4\n",
    "  - p1(strong) 2/4\n",
    "  - p0 (weak) 0/1\n",
    "  - p1( weak) 1/1\n",
    "\n",
    "  ***Temp is the best node***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBy0Qx_W_iDr",
    "outputId": "ed70aece-fbf3-476d-b240-950e75762522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14739311883324124\n",
      "0.04217935649972371\n",
      "0.12877123795494477\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def hs(pos, neg, total):\n",
    "  p1 = pos/total\n",
    "  p0 = neg/total\n",
    "  \n",
    "  if p0 == 0.0:\n",
    "    p0 = 0\n",
    "  else:\n",
    "    p0 = -p0*math.log(p0,2)\n",
    "  if p1 ==0.0:\n",
    "    p1 = 0\n",
    "  else:\n",
    "    -p1*math.log(p1,2)\n",
    "  return p1+p0\n",
    "def Gain(posb1,posb2,negb1,negb2):\n",
    "  total = float(posb1+posb2+negb1+negb2)\n",
    "  totalb1 = float(posb1+negb1)\n",
    "  totalb2 = float(posb2+negb2)\n",
    "  hsmain = hs(posb1+posb2,negb1+negb2,total)\n",
    "  hsb1 = hs(posb1,negb1,totalb1)\n",
    "  hsb2 = hs(posb2,negb2,totalb2)\n",
    "  G = hsmain - (totalb1/total)*hsb1 -(totalb2/total)*hsb2\n",
    "  return G\n",
    "tempG = Gain(2,2,1,0)\n",
    "print(tempG)\n",
    "humidG = Gain(2,0,2,1)\n",
    "print(humidG)\n",
    "windG = Gain(2,1,2,0)\n",
    "print(windG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZdBe-FOPAAT"
   },
   "source": [
    "#### 2.b. Complete the tree.\n",
    "Repeat the method of using information gain to split on another feature, as described in class (and posted in the slides on Blackboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pifRx3aHPAAT"
   },
   "source": [
    "**split on wind because it has the second highest gain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1S22ijrPAAT"
   },
   "source": [
    "### 3. K-Nearest Neighbors on NIST\n",
    "\n",
    "We're going to build a K-nearest neighbors classifier from scratch, including validating for the best K, and test it out on NIST, a handwritten digits dataset.\n",
    "\n",
    "The 64 features of this dataset are the values of each pixel in the 8x8 image grid of one handwritten digit, where the digits are written in white (pixel value 255) and the surrounding space is black (pixel value 0). Each sample represents one image.\n",
    "\n",
    "#### First, install the latest release of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "xTDIkpsGPAAU",
    "outputId": "d1eedb53-5af4-4d7b-e60f-d90afb538c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling scikit-learn-0.24.1:\n",
      "  Successfully uninstalled scikit-learn-0.24.1\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/74/eb899f41d55f957e2591cde5528e75871f817d9fb46d4732423ecaca736d/scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1) (1.0.1)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-0.24.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "sklearn"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These datasets require the latest release of sklearn (0.24.1).\n",
    "# We are going to uninstall the default Colab version (if you are using Colab) or your current version, and install version 0.24.1.\n",
    "# AFTER RUNNING THIS CELL, YOU MAY NEED TO RESTART THE RUNTIME. GO TO Runtime/Kernel -> Restart Runtime to do this. \n",
    "# (Or, in Colab, hit the RESTART RUNTIME button at the bottom if there is an error message when you run this cell.)\n",
    "# You only need to do this once, but if the Colab runtime disconnects, you will need to do it again!\n",
    "\n",
    "!pip uninstall scikit-learn -y\n",
    "!pip install scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7e9DI9FPAAV"
   },
   "source": [
    "#### Verify we're using the correct version. (If it is not 0.24.1, restart the runtime, or try installing again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7rMPbq0PAAV",
    "outputId": "a9f5db19-5c1c-4a2b-872d-b628d619d54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKJSWMmUPAAV"
   },
   "source": [
    "#### Import the data and take a look at some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "LCGF8ydRPAAW",
    "outputId": "ac174baa-a7e6-4dd3-abf2-d718a01bf69c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>pixel_1_2</th>\n",
       "      <th>pixel_1_3</th>\n",
       "      <th>pixel_1_4</th>\n",
       "      <th>pixel_1_5</th>\n",
       "      <th>pixel_1_6</th>\n",
       "      <th>pixel_1_7</th>\n",
       "      <th>pixel_2_0</th>\n",
       "      <th>pixel_2_1</th>\n",
       "      <th>pixel_2_2</th>\n",
       "      <th>pixel_2_3</th>\n",
       "      <th>pixel_2_4</th>\n",
       "      <th>pixel_2_5</th>\n",
       "      <th>pixel_2_6</th>\n",
       "      <th>pixel_2_7</th>\n",
       "      <th>pixel_3_0</th>\n",
       "      <th>pixel_3_1</th>\n",
       "      <th>pixel_3_2</th>\n",
       "      <th>pixel_3_3</th>\n",
       "      <th>pixel_3_4</th>\n",
       "      <th>pixel_3_5</th>\n",
       "      <th>pixel_3_6</th>\n",
       "      <th>pixel_3_7</th>\n",
       "      <th>pixel_4_0</th>\n",
       "      <th>pixel_4_1</th>\n",
       "      <th>pixel_4_2</th>\n",
       "      <th>pixel_4_3</th>\n",
       "      <th>pixel_4_4</th>\n",
       "      <th>pixel_4_5</th>\n",
       "      <th>pixel_4_6</th>\n",
       "      <th>pixel_4_7</th>\n",
       "      <th>pixel_5_0</th>\n",
       "      <th>pixel_5_1</th>\n",
       "      <th>pixel_5_2</th>\n",
       "      <th>pixel_5_3</th>\n",
       "      <th>pixel_5_4</th>\n",
       "      <th>pixel_5_5</th>\n",
       "      <th>pixel_5_6</th>\n",
       "      <th>pixel_5_7</th>\n",
       "      <th>pixel_6_0</th>\n",
       "      <th>pixel_6_1</th>\n",
       "      <th>pixel_6_2</th>\n",
       "      <th>pixel_6_3</th>\n",
       "      <th>pixel_6_4</th>\n",
       "      <th>pixel_6_5</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  ...  pixel_7_5  pixel_7_6  pixel_7_7\n",
       "0        0.0        0.0        5.0  ...        0.0        0.0        0.0\n",
       "1        0.0        0.0        0.0  ...       10.0        0.0        0.0\n",
       "2        0.0        0.0        0.0  ...       16.0        9.0        0.0\n",
       "3        0.0        0.0        7.0  ...        9.0        0.0        0.0\n",
       "4        0.0        0.0        0.0  ...        4.0        0.0        0.0\n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Import the data into a pandas dataframe\n",
    "nist = load_digits()\n",
    "nist_df = pd.DataFrame(nist.data, columns = nist.feature_names)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = nist.data\n",
    "y = nist.target\n",
    "\n",
    "# View the raw data\n",
    "nist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "EO0CYccDPAAX",
    "outputId": "8f6c5839-a5b1-4514-fff9-46c265c17030"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize 10 random samples as 8x8 images\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(20, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(nist.images[i*4], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # Label the image with the target value\n",
    "    ax.text(0, 7, str(nist.target[i*4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqQ2PWg9PAAY"
   },
   "source": [
    "#### 3.a. Split the data into train, test, and validation sets.\n",
    "\n",
    "Use a 60/20/20 split. Make sure to set `random_state=42` to shuffle the dataset in a consistent manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k12JT_YFPAAZ",
    "outputId": "91db43cb-c5e2-439f-f1a7-4de81696381c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (1077, 64) \n",
      "y train (1077,) \n",
      "X test: (360, 64) \n",
      "y test (360,) \n",
      "X val: (360, 64) \n",
      "y test (360,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "testSize = 0.20\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(nist.data, nist.target, test_size=testSize, random_state=42)# -- Code Required --\n",
    "testSize = 0.25\n",
    "xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=testSize, random_state=42)# -- Code Required --\n",
    "print(\"X train:\", xtrain.shape,\"\\ny train\", ytrain.shape,\"\\nX test:\", xtest.shape,\"\\ny test\", ytest.shape,\"\\nX val:\", xval.shape,\"\\ny test\", yval.shape)\n",
    "### ADD CODE HERE:\n",
    "# Split the data into training, test, and validation sets, just like we did on HW 2\n",
    "# Make sure to set random_state=42!\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sJ6CTldPAAZ"
   },
   "source": [
    "#### 3.b. Implement a Euclidean distance function.\n",
    "We'll need to calculate the Euclidean distance between points of arbitrary dimensions, which you'll implement by filling in the function below. (*Hint: use [this](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html).*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "4uKb1bFZPAAa"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    '''\n",
    "    A function that calculates and returns the Euclidean distance between vectors a and b.\n",
    "    '''\n",
    "    dist = distance.euclidean(a,b)\n",
    "    ### ADD CODE HERE: calculate dist, the Euclidean distance between a and b\n",
    "\n",
    "    return dist\n",
    "# vect1 = [0,0,0,0]\n",
    "# vect2 = [1,1,1,1]\n",
    "# dis = euclidean_distance(vect1,vect2)\n",
    "# print(dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx19DUfuol7Z"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_I3e8sQPAAa"
   },
   "source": [
    "#### 3.c. Implement the K-nearest neighbors algorithm.\n",
    "The two functions below make up a skeleton of the KNN algorithm, which you will complete.\n",
    "\n",
    "The first, `compute_neighbors_and_classify()`:\n",
    "1. Computes the neighbors given `k` (the number of neighbors)\n",
    "2. Finds the `k` neighbors with the smallest Euclidean distance to the test point (using `euclidean_distance()` above)\n",
    "3. Returns the most common classification label among those `k` neighbors.\n",
    "\n",
    "The second, `compute_accuracy_on_dataset()`:\n",
    "1. Gets the KNN classification predictions for each test point using `compute_neighbors_and_classify()`.\n",
    "2. Compares the predictions to the real values.\n",
    "3. Returns the accuracy score (how many predictions the model got right divided by total number of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6scJjGMPAAb",
    "outputId": "2252ceae-2ce1-46a6-f85b-87723c07fafc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def compute_neighbors_and_classify(X_train, y_train, test_point, k):\n",
    "    '''\n",
    "    A function that computes the k neighbors in X_train closest to one test point, \n",
    "    and returns the classification.\n",
    "    '''\n",
    "    # This is a list to hold all of the distances associated with the points and their labels\n",
    "    distances = []\n",
    "    # Loop over the training points\n",
    "    for i, train_point in enumerate(X_train):\n",
    "\n",
    "        dist = euclidean_distance(train_point,test_point)\n",
    "        # Add the distance, the point, and its label (as a tuple) value to our list\n",
    "        # We use a tuple so we can sort the list later while keeping each neighbor next to its distance\n",
    "        distances.append((dist, train_point, y_train[i]))\n",
    "\n",
    "    # Now that we have all the distances, we need to return the labels with the k smallest distances\n",
    "    # First, sort the list we made by distance\n",
    "    distances = sorted(distances, key=lambda x: x[0])\n",
    "    \n",
    "    # Now, pull out the labels associated with the first k neighbors and add them to a list\n",
    "    k_labels = []\n",
    "    for i in range(k):\n",
    "        k_labels.append(distances[i][2])\n",
    "    \n",
    "    classif = mode(k_labels)\n",
    "    classification = int(classif[0])\n",
    "    ### ADD CODE HERE:\n",
    "    # Get the label that appears the most times in k_labels using mode(). This is our `classification`.\n",
    "    # If there is no mode, just return any label within the k_labels list.\n",
    "    # Refer to the documentation of mode() to ensure you're getting the correct value:\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html\n",
    "    ###\n",
    "    \n",
    "    return classification\n",
    "k = compute_neighbors_and_classify(xtrain,ytrain,xtest[0],10)\n",
    "print(k)\n",
    "def compute_accuracy_on_dataset(X_train, y_train, data, labels, k):\n",
    "    '''\n",
    "    A function that computes the accuracy of KNN on a (test) dataset.\n",
    "    '''\n",
    "    accuracy_numerator = 0\n",
    "    # Loop over the dataset we'd like to get the accuracy on\n",
    "    for i, point in enumerate(data):\n",
    "        \n",
    "        ### ADD CODE HERE:\n",
    "        # Compute the neighbors and `classification` for this point with the training data\n",
    "        ###\n",
    "        classification = compute_neighbors_and_classify(X_train,y_train,point,k)\n",
    "        # Compare this classification to the real value in `labels`. \n",
    "        # If the model got it correct, add to a running sum of correct predictions.\n",
    "        accuracy_numerator += 1 if classification == labels[i] else 0\n",
    "    \n",
    "    ### ADD CODE HERE:\n",
    "    # Compute the accuracy: divide the count of correct predictions by the number of samples in the dataset\n",
    "    ###\n",
    "    accuracy = accuracy_numerator/float(len(data))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQAkOeLOPAAc"
   },
   "source": [
    "#### 3.d. Run KNN on the NIST dataset with k = 1, and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4sHVs05PAAd",
    "outputId": "6074fb56-0bc5-4ed4-bd27-1a36ba05cf52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set with k = 1: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Call compute_accuracy_on_dataset() to get the accuracy on the test set with k=1\n",
    "###\n",
    "k = 1\n",
    "knn_acc = compute_accuracy_on_dataset(xtrain, ytrain, xtest, ytest, k)\n",
    "print(\"Accuracy on test set with k = 1:\", knn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrk5S1rQPAAe"
   },
   "source": [
    "#### 3.d. Find the best value of K by using the validation set.\n",
    "Ideally, we'd like to programmatically find the best value of `k` instead of just guessing what it is. Using the **validation set**, find the best value of `k` by running our KNN algorithm for `k = [1, 2, 3 ... 10]`, and saving the `k` with the best accuracy. \n",
    "\n",
    "Fill in the function below, and then call it to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1EA36f3KPAAe"
   },
   "outputs": [],
   "source": [
    "def validate_k_on_dataset(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    A function that finds the best K using a (validation) dataset.\n",
    "    '''\n",
    "    # Initialize the best accuracy and associated k\n",
    "    best_knn_acc = 0\n",
    "    best_k = 0\n",
    "    # In Python, range doesn't include the last element; this is the list [1,2,...10]\n",
    "    k_vec = list(range(1, 11))\n",
    "    # Loop through each k\n",
    "    for i in k_vec:\n",
    "        \n",
    "        ### ADD CODE HERE:\n",
    "        # Compute the accuracy on the validation set using the current k\n",
    "        ###\n",
    "        knn_acc = compute_accuracy_on_dataset(X_train, y_train, X_val, y_val, i)\n",
    "        # If this k is better, replace the current best accuracy and k\n",
    "        if knn_acc > best_knn_acc:\n",
    "            best_knn_acc = knn_acc\n",
    "            best_k = i\n",
    "            \n",
    "    return best_knn_acc, best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJNV7qUTPAAf",
    "outputId": "d4cbc26d-f909-4060-b77b-0214128d2cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on validation set was 0.9944444444444445 with k = 3\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Call the above function and print the resulting accuracy and best k for the validation set\n",
    "###\n",
    "retval = validate_k_on_dataset(xtrain, ytrain, xval, yval)\n",
    "best_knn_acc = retval[0]\n",
    "best_k= retval[1]\n",
    "print(\"Best accuracy on validation set was\", best_knn_acc, \"with k =\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_ZCW-cuPAAf"
   },
   "source": [
    "#### 3.e. Merge the training and validation sets, and report the new test accuracy using the best `k`.\n",
    "Since we've already used the validation set to obtain the best `k`, we can now merge the training set and the validation set and recompute KNN on the test set with the best `k`. \n",
    "\n",
    "Make sure to merge the data points in the same order that you merge the labels!\n",
    "\n",
    "##### Merge the training and validation sets. *(Hint*: Use [np.vstack](https://numpy.org/doc/stable/reference/generated/numpy.vstack.html) and [np.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vprYgr5cPAAg",
    "outputId": "0946b47b-a8b3-40dd-f52f-400fa0c862a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (1437,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "### ADD CODE HERE:\n",
    "# Use np.vstack to stack X_train and X_val\n",
    "###\n",
    "xtrainval = np.vstack((xtrain,xval))\n",
    "ytrainval = np.concatenate((ytrain,yval))\n",
    "print(xtrainval.shape, ytrainval.shape)\n",
    "### ADD CODE HERE:\n",
    "# Use np.concatenate to concatenate y_train and y_val\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FldBznPlPAAg"
   },
   "source": [
    "###### Now, recompute the accuracy on the test set using this merged dataset (instead of just the training set) and the best `k` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mceNdPBVPAAh",
    "outputId": "66bab127-1a25-49df-9e6c-7ac05df4b9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy on test set using merged train and validation sets and best K value: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Compute the accuracy on the test set using the merged train and validation set and the best k\n",
    "###\n",
    "knn_acc = compute_accuracy_on_dataset(xtrainval, ytrainval, xtest, ytest, best_k)\n",
    "print(\"KNN accuracy on test set using merged train and validation sets and best K value:\", knn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I88eMIyWPAAh"
   },
   "source": [
    "### 4. Comparing Decision Trees and K-Nearest Neighbors on Raw Wine Data\n",
    "We're going to compare the performance of our KNN algorithm and the decision tree algorithm on a new dataset.\n",
    "\n",
    "The wine dataset is the result of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are 13 different measurements (features) taken for different constituents found in the three types of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kspMl6RPAAh"
   },
   "source": [
    "#### Import the wine dataset and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "jANbVsMdPAAi",
    "outputId": "930dbf23-014b-4b72-f57e-35c7e496407e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  ...   hue  od280/od315_of_diluted_wines  proline\n",
       "0    14.23        1.71  2.43  ...  1.04                          3.92   1065.0\n",
       "1    13.20        1.78  2.14  ...  1.05                          3.40   1050.0\n",
       "2    13.16        2.36  2.67  ...  1.03                          3.17   1185.0\n",
       "3    14.37        1.95  2.50  ...  0.86                          3.45   1480.0\n",
       "4    13.24        2.59  2.87  ...  1.04                          2.93    735.0\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Import the data into a pandas dataframe\n",
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
    "\n",
    "# Split data into features and labels\n",
    "X_w = wine.data\n",
    "y_w = wine.target\n",
    "# View the raw data\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7mLo0NjzW19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGuqUvNQPAAi"
   },
   "source": [
    "#### 4.a. Split the data into train, test and validation sets.\n",
    "\n",
    "Use a 60/20/20 split. Make sure to set `random_state=42` to shuffle the dataset in a consistent manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtiT4gQRPAAi",
    "outputId": "2b2a1cab-3290-46e4-f60b-a8ba8c268e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: (106, 13) \n",
      "y train (106,) \n",
      "X test: (36, 13) \n",
      "y test (36,) \n",
      "X val: (36, 13) \n",
      "y test (36,)\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Split the data into training, test, and validation sets, just like we did on HW 2\n",
    "# Make sure to set random_state=42!\n",
    "###\n",
    "testSize = 0.20\n",
    "xtrainw, xtestw, ytrainw, ytestw = train_test_split(X_w, y_w, test_size=testSize, random_state=42)# -- Code Required --\n",
    "testSize = 0.25\n",
    "xtrainw, xvalw, ytrainw, yvalw = train_test_split(xtrainw, ytrainw, test_size=testSize, random_state=42)# -- Code Required --\n",
    "print(\"X train:\", xtrainw.shape,\"\\ny train\", ytrainw.shape,\"\\nX test:\", xtestw.shape,\"\\ny test\", ytestw.shape,\"\\nX val:\", xvalw.shape,\"\\ny test\", yvalw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCy8sqLxPAAj"
   },
   "source": [
    "#### 4.b. KNN: Find the best `k` and accuracy for this dataset.\n",
    "Use the same process as in question 3 to:\n",
    "\n",
    "##### 1. Validate `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSXHOs0lPAAj",
    "outputId": "c4a5e9a7-d9ab-4d24-9338-c2ceb68c706b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy on validation set was 0.8333333333333334 with k = 4\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Call validate_k_on_dataset() and print the resulting accuracy and best k for the validation set\n",
    "###\n",
    "retval = validate_k_on_dataset(xtrainw, ytrainw, xvalw, yvalw)\n",
    "best_w_knn_acc = retval[0]\n",
    "best_w_k= retval[1]\n",
    "print(\"Best accuracy on validation set was\", best_w_knn_acc, \"with k =\", best_w_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVhxUm4VPAAj"
   },
   "source": [
    "##### 2. Merge the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyhZgd3_PAAk",
    "outputId": "0f1bc0db-b17b-47cc-e629-7018c50d3606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 13) (142,)\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Use np.vstack to stack X_train and X_val\n",
    "###\n",
    "\n",
    "### ADD CODE HERE:\n",
    "# Use np.concatenate to concatenate y_train and y_val\n",
    "###\n",
    "xtrainvalw = np.vstack((xtrainw,xvalw))\n",
    "ytrainvalw = np.concatenate((ytrainw,yvalw))\n",
    "print(xtrainvalw.shape, ytrainvalw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7vUSZUuPAAk"
   },
   "source": [
    "##### 3. Report the new accuracy on the test set; recompute KNN using the merged dataset and the best `k` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-F5K3D6ZPAAk",
    "outputId": "e00f3426-0258-484b-e8cd-176767c11de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy on test set using merged train and validation sets and best K value: 0.75\n"
     ]
    }
   ],
   "source": [
    "### ADD CODE HERE:\n",
    "# Compute the accuracy on the test set using the merged train and validation set and the best k\n",
    "###\n",
    "w_knn_acc = compute_accuracy_on_dataset(xtrainvalw, ytrainvalw, xtestw, ytestw, best_w_k)\n",
    "print(\"KNN accuracy on test set using merged train and validation sets and best K value:\", w_knn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oS5J2kkPAAl"
   },
   "source": [
    "#### 4.c. Decision Trees: Use `sklearn`'s built-in decision tree on this dataset and report the accuracy.\n",
    "You can find the documentation for instantiating and fitting `sklearn`'s `DecisionTreeClassifier` [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "You can find the documentation for using `metrics.accuracy_score` to compute the accuracy given predictions and true labels [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "*Make sure to use the merged training and validation set to fit the decision tree (for a fair comparison to KNN)!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvoxq0KmPAAl",
    "outputId": "7a7c3f2a-7436-4db6-f92a-c43458b54d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy on test set: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "### ADD CODE HERE:\n",
    "# Instantiate the decision tree.\n",
    "###\n",
    "clf.fit(xtrainvalw,ytrainvalw)\n",
    "### ADD CODE HERE:\n",
    "# Fit the tree with the merged training and validation set.\n",
    "###\n",
    "yGuess = clf.predict(xtestw)\n",
    "### ADD CODE HERE:\n",
    "# Use the tree to predict on the test set.\n",
    "###\n",
    "tree_acc = metrics.accuracy_score(ytestw, yGuess)\n",
    "### ADD CODE HERE:\n",
    "# Use metrics.accuracy_score to get the accuracy of this decision tree on the test set.\n",
    "###\n",
    "\n",
    "print(\"Decision tree accuracy on test set:\", tree_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjvfwzxoPAAl"
   },
   "source": [
    "#### 4.d. Explain the difference in performance between these two algorithms; why does one perform better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCVcQXhXPAAm"
   },
   "source": [
    "As you can see by the bellow graphs not all of the features are equally good at classifing the types of wine. While *proline* and  *od280/od315_of_diluted_wines* seem to be good features to work with because relitively few of their values of x overlap for different labels, features like *proanthocyanins* are bad because they have many overlapping x values for different values of y. In a decicion tree, the features that are good pradictors of labels get promoted and bad features are deprioritized. However in knn it is a wrote euclidan distance from one data point to another which equally weights all features. But while were on the topic of equally weighting all features. Because this data set has not been normalized the KNN estimate favors features with higher data values regardless of statistical signifigance. this also impacts is poor performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "IAOFqeKszYxF",
    "outputId": "cbdeb794-9f24-4ea7-a01f-666215c8501c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkYAAAKrCAYAAACk3jGyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfbBfd30f+Pf3Pki2bMmWZNnyI0Y8LUgQg1UwKWPMbMnalBQSthSXJXHdFOiU2WR2Z6dsZjcEMpOh08lMmm0bh1Im20xwQkugaYq90BbHKWBAMgZLOH5AyLJsS5ZkWc8P9+HsH/f+rr/3d3/3QdLVvdc+r9fMHev3O+d3ft9zzvt8z/ecj+89pWmaAAAAAAAAtEHfYjcAAAAAAABgoSiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsMLHYDernsssua66+/frGbwRnaunXr/qZp1s33cuXhpUkeqMkDNXmgJg/U5IGaPFCTB2ryQE0eqMkDtZnysCQLI9dff322bNmy2M3gDJVSnjwfy5WHlyZ5oCYP1OSBmjxQkwdq8kBNHqjJAzV5oCYP1GbKgz+lBQAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtMashZFSyrWllG+WUn5cStleSvnVHvOUUsrvlVKeKKX8qJTylmraL5dSHh//+eX5XgEW1lNPPZV3vetdecMb3pCNGzfmX/yLfzFlHnloD3mgJg/U5IGaPFCTB2ryQE0e6CYT1OSBmjxwrgbmMM9wkv+9aZoHSykrk2wtpXyjaZofV/PcluQ14z9vS/L7Sd5WSlmT5FNJNidpxj/7503THJzXtThPtj55MA/sOJAjJ4ay/dnDuW3TlXnd+pV5YMeBrF6xLAePn86RE0P5zo4DWT7Ql1dfsTIfeMs1SZIvP7g7T+w9kh37j+XoyeEM9pe8+brVaZLctunK/P23XZcvfndX7tn2bEqSx/YeyXVrVuT9b74m2585lH1HTuXg8dM5eHwog30lh08N58KBvvyt11+RI6eG0yRZtXxg0ndvuuqSbH/mUJokH3jLNbnxFasnrcufPbh7yrTOOt60Ye2k+aczMDCQ3/md38lb3vKWHDlyJDfeeGPe/e535w1veEM926LnYeuTB/PlB3enJNl41SU5ePz0lHXsbP/O/uh8bqbt0b3cbc8cyg+ePDix/NdcsTKrVyyb2A+dffLY3iM5PTyat29YmyOnhvPY3iN55tDJXDjYn7/1P1w+sU83XXVJvvnoc3nu8Mm8fcParLxwMDdtWJsk+Wf3PJJdzx/P+2+4Op98z+untH+6fdyZb+OVqyaW173/O3nufFevbVBvm848r7546CWRh8XSa5utXrEs9z36XHbsO5o1Fy1Lk+TZF07k5PBohkdG09dXsmKwP8+8cDIpyZqLluV/e/frJvqe7+44kB89fSi3vHZd3vrKtRP79vCp4Yk+Z7RJPnjjWF/01YeezuoVy3LtmhW5bOXynn3DmfQBM3mp9A8Lpdf+r/ffR95+/cT0P/rOztz32L4cPzWc0yNN1l28LHd9ZPOUzyTJfY/ty8XL+nP09MikHNy26cokmXK8f2P7nty7fU+uW7Ni4hy068Cx3Lt9T27duD7v3rh+2gycSz5eannota5bnzyYu/7yJ3nu8MmUJNufPZzR0SavW78y/8tN1086li9ZsSyP7z2SZw6dSH9KLlzenw/eeO3E9q3HC5esWJanDx7PnkMn8+rLL84/ve31ufEVq7P1yYP5Z/c8kif2Hc2r101+v25b3a8fPjWckuQXu47tpWYp5GG6PHcfq3/24O7sO3Iql61cnk1XXZLf+6+P5bkjp3L5xctz06vW5v7H9+fU8EiWD/Tl5tesy/HTI9n+zKGcHh7N8OhohkeavP7KVblgsD8P7jqYa1evyEfefn0OHj+d1SuWZdszh/LE3iM5NT4umO7cXLen19hurus327TFsNTy8Mn/8MM8vu/YlHkG+0vSJIMDfdn8itU5dGIoV6y6IB9756vy6J4juWfbs1l70bL8dP+xnB4ezdBokzUrBieuCXodv0y1FPIwk7lcH3SPp+u+ur6GrK8rO+P32fIxl76rzlpn3PCmqy/J2zasnTTO6Lw3XVvXXrQsB46dnmjfZ7/2yMR4IUnu3b4nx04N5+DxoWy6alW++ol3zNqe+ju+8K2fJk2TO9+xYcryP/me1ydZmnmo1+XRPUfyp9/flStWXZBbXnd5tj9zKA8+eTC7nj+epmly/WUX5YbrVmf388fz4K6DWbl8IH19JRcuG8imq1blW0/sz9FTw/mfxrfpf3lkby67eHl+9tWXZdXygfyXv34uaZpsuvqSiX3xvZ8eyH2P7cstr12X3/3Qm+fUzrn2Nwv1mXMxXSa6LJkx5Vx17i3sr8Yc2545NPH6A2+5Jo/uOTJx3Pyt11+RHfuPZce+o1k20JdDJ4eTpsnGqy7Jx975qlmv8b743V2TsvvVH+zOE88dzfpLLsgN162euJfV6c+++ehz+en+Y9lw2UX52DtflSQT7U0ybZuTF8cuvabP1I/OJVM/3nM0b/vQr+Xv3npzXrtmYMHzMN01w2fveSQPPfVChkebLB/oS2mS0yOjuWb1inzsna/K9mcO5b8/vj/PHTmZ69deNOne4soLBvL0CydyamQ0aZJLVyzLnX/zlXnd+pWTtmVnLHnfo89lb9c9o7aOORa7f+hcv2175nBKkjdctSq/8OZrpoz7nz92Os8fO53DJ4eyYtlABvpKlvX35fTIaFZdOJhjp4Zz7PRI+pIsGxy7xuh1f7FzH3r7s4ez8cpVE/cS53qfp40ZmU1pmubMPlDKf0zyL5um+Ub13h8kua9pmrvHXz+a5JbOT9M0H+s133Q2b97cbNmy5YzaNd+2PnkwH/78Azk1NJp6Cw32l4yMNhmdZrMN9JekaTI8OvPy33/DVfnqQ8/MW3u7Lesvufujb5/oHG//Nw/k9HijOtOS5MOfH3t/2UBf/vhXbjrjA+N973tfPvGJT+Td7353Silbm6bZvNh52Prkwdz+ue/k9MiLO6kkWT744jp+8bu78utfeXhi+m//whvzuvUrZ9wevZZ7vpWMXYwPj4xOytzNr7ks9z++f+L1x2/ekC98e+eUffzoniOT1jNJLhjfDkkmZbyvJAN9JSklwyOTt0HneDg9PDrtPMnSzMNimbTN+vvG+4Xp+47ZDPaXDM1D9rr7hnPtA2bS5jz02v/dfUdfefHfZ5uL2fT3JSOznI/GmzclA/Odj6Wch17rmiR/7w++Pev5fDZz3Qe/9b435v/+jw9Pmrfz/mf+YvtE2+54+/W56/4dU5axbKAvd/+j+T2Gz6eFzsN0ee4+vzXJvPS1vZQkvZZcj1GSTNueuv/uNtPxer77+vmwmHk4m/63r8zeby/rL/nNv7Np0vG7FLf9UrSUzhezHT/d14x9JT376sH+kuGRZlIf8PGbN+QPv7NzxnzMpe9aNtCX33jvxnzmL7bn5NDUE850ee30PdOdV7qvN3q54ZpL8tVPvGPa9sx07up1PdMpjtQWOw/1uvX1je3HxfT+G67qWRw5m75+oT4z3zqZ+Lmf+7mtTdNsTub3ntRCXF/M5d7CXMaQHYP9JX8ywzVer/sC3Qb6S/oyNu7oblV/31ifMdu4eKAvKWX669ZeY5kzyVSveT/zv96xYHmY7prhg3d9O+eja+jOQK+xZKcv7+532zrmWMj+YeuTB/PBP/j2nI/T82ku93mWQv+9WDrjh17TzugZI6WU65O8Ocl3uyZdneSp6vXu8feme7/Xsj9aStlSStmyb9++M2nWefHAjgM5PTw6pdMZGpn5xubwyOxFkWTs//g9n4ZGmjyw40CSsXUZqhrVmdZZx9EmGRoenZh/rnbu3Jkf/OAHedvb3tY9aVHz8MCOA1NOxE0mr+M9256dNP2ebc/Ouj16Lfd867S7O3Pf2/n8pNf3bt/Tcx93r2fy4rp1Z3y0GfvcUI9tMGnbTDPPUs3DYunO02x9x2zmK3vdfcO59AEzaXseeu3/bqPNiz/ny1wGaSOj6ZmB+czHUs9Dr3V9YMeBcy6KJHPfB/dse3bKvJ3367bdu31Pz2XM9zF8Pi1GHqbL85Tz23k8z0+35HqMMlN76v6720zH6/ns6+fDYufhbMzlc0MjzZTjd6lt+6VoqZ0v5nJ9MGU83aOv7nWj8d7te2bNx5z6ruHRiaz1Ml1eO33PdOeV7uuNXrY9c3jG9sx07up1PdNtKeShXrfFLook099HOJu+fqE+M5/OVyYW+vpiLvcWzuRm62zXeL3uC3QbHh939GrVyOjsRZFkbJ6Z1qvXWOZMMtU973/+9g8XNA/TXTOcr66hOwO9vqbTlxtzLHz/8MCOA0uiKJLM7T7PYvffS9WcCyOllIuTfDnJrzVNc3i+G9I0zeeaptncNM3mdevWzffiz9hNG9Zm2UBfStf7g/1l0v/l222gv2RgDlu18ydRzpfB/jLxJxhu2rA2g1WjOtM669hfxn4roTP/XBw9ejQf+MAH8ru/+7tZtWrVvLf/XPJw04a1Y3/6oNKXyevY+ZMzHbdtunLW7dFruedb5zdGujP31uvXTHp968b1Pfdx93p2llfv/86n+srY5wZ7bINJ26bHPEs5D4ulO0+z9R2zma/sdfcNZ9sHzEQeeu//bn0l6S85p1zMpGTs/zKaTX9femZgvvLxUshDr3W9acPaOZ3PZ9P5P+xmm+e2TVdO2V+d9+u2df6cSbf5PIbPp8XKw3R5nnJ+O4/n+emW3Ovc3Ks9df/dbabj9Xz19fNhKeThbMyl3x7sL1OO36W07ZeipXi+mMv1wZTxdI++erC/TOkDbt24ftZ8zKnvGuibyFov0+W1c3003Xml+3qjl01XrZqxPTOdu3pdz9SWSh7qdRtY4GvBXqa7j3A2ff1CfWa+nM9MLPT1xVzuLcxlHN8x2zVe932BXgbGxx29vra/L3MaFw/0zXzd2msscyaZquftHz2Vu3/71xY0D9NdM5yvrqE7A72+pjOObPuYYzH6h5s2rJ32OF3os8Vc7vMs5WuCxTSXZ4yklDKYsaLIHzdN82c9Znk6ybXV62vG33s6Y7+aVL9/39k0dKHd+IrV+eNfuem8PmOk83fhz/czRm58xerc/Y9u6vk3qjvreCZ/X25oaCgf+MAH8uEPfzi/+Iu/2GuWRc3Dja9Ynbs/+vYZnzHSeaZI9zNGZtoevZa7lJ4x8u6N66fs4846TPeMkc76zvaMkfp46J7nTVddnPe+971LNg+LZbpttpSeMdLdxvn4Ncql3j8slOn2/0vpGSPzkY+XSh6mW9c//djPLugzRl63fmXPZ4x0jv9O265be9FL7hkjyeLmYbp93OtYXexnjPRqz2zPGJnpeD0fff18WEp5OJ/PGOk+fultqZ4vZjt+6undz+3o9NUzPWNkpud8zfT9vd7vHiueyTNGOm0922eMzNSe+jvm8oyRZGnloXvdluozRs6mr1+oz8yHpZSJ+VDfW5jvZ4z02kcTY9sl+IyRM8lUZ95vPbYnd//WP8nf/Qe/tKB5mK6tX/r4zy6JZ4y0dcyxWP3Dja9YnS997GeX3DNGzmTsQJKmaWb8yVih698l+d0Z5vnbSe4Zn/emJN8bf39Nkp8mWT3+89Mka2b7zhtvvLFhaRodHW0+8pGPNL/6q786ZVqSLY08tIo8UJMHavJATR6oyQM1eaAmD3SbLhOdPDTznAl5WNrkgZo8MBd1Hrp/5vIbI38zyUeSPFxKeWj8vV9Pcl2SNE1zV5KvJXlPkieSHE/yD8anPV9K+a0k3x//3Geappn9j5WyZH3rW9/KH/3RH+WNb3xjbrjhhiTJb//2b2fXrl1J0vmdMnloCXmgJg/U5IGaPFCTB2ryQE0e6DZdJpKsK6V83D2pdpEHavLAuSpjhZOlZfPmzc2WLVsWuxmcoVLK1qZpNs/3cuXhpUkeqMkDNXmgJg/U5IGaPFCTB2ryQE0eqMkDtZnyMA+PFQUAAAAAAHhpUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1pi1MFJK+UIp5blSyrZppv8fpZSHxn+2lVJGSilrxqftLKU8PD5ty3w3noV355135vLLL8+mTZt6TpeHdpEHavJATR6oyQPdZIKaPFCTB2ryQE0eqMkD52ouvzHyh0lunW5i0zT/vGmaG5qmuSHJ/5nkL5umeb6a5V3j0zefW1NZCu64447ce++9006Xh3aRB2ryQE0eqMkD3WSCmjxQkwdq8kBNHqjJA+dq1sJI0zT3J3l+tvnG3Z7k7nNqEUvazTffnDVr1sx1dnl4mZMHavJATR6oyQPdZIKaPFCTB2ryQE0eqMkD52renjFSSlmRsd8s+XL1dpPk66WUraWUj87y+Y+WUraUUrbs27dvvprFIpEHavJATR6oyQM1eaDbuWRCHl5+5IGaPFCTB2ryQE0emM58Pnz955N8q+tXkt7RNM1bktyW5J+UUm6e7sNN03yuaZrNTdNsXrdu3Tw2i0UiD9TkgZo8UJMHavJAt7POhDy8LMkDNXmgJg/U5IGaPNDTfBZGPpSuX0lqmubp8f8+l+QrSd46j9/H0iYP1OSBmjxQkwdq8kA3maAmD9TkgZo8UJMHavJAT/NSGCmlXJLknUn+Y/XeRaWUlZ1/J/m5JNvm4/tY2uSBmjxQkwdq8kBNHugmE9TkgZo8UJMHavJATR6YycBsM5RS7k5yS5LLSim7k3wqyWCSNE1z1/hsv5Dk603THKs+ekWSr5RSOt/zxaZp7p2/prMYbr/99tx3333Zv39/rrnmmnz605/O0NBQ92zy0BLyQE0eqMkDNXmg2wyZqP9GgUy0hDxQkwdq8kBNHqjJA+eqNE2z2G2YYvPmzc2WLVsWuxmcoVLK1qZpNs/3cuXhpUkeqMkDNXmgJg/U5IGaPFCTB2ryQE0eqMkDtZnyMJ/PGAEAAAAAAFjSFEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNRRGAAAAAACA1lAYAQAAAAAAWkNhBAAAAAAAaA2FEQAAAAAAoDUURgAAAAAAgNZQGAEAAAAAAFpDYQQAAAAAAGgNhREAAAAAAKA1FEYAAAAAAIDWUBgBAAAAAABaQ2EEAAAAAABoDYURAAAAAACgNWYtjJRSvlBKea6Usm2a6beUUg6VUh4a//mNatqtpZRHSylPlFI+OZ8NZ3Hceeedufzyy7Np06ae0+WhfWSCmjxQkwdq8kBNHqjJAzV5oCYP1OSBmjxwrubyGyN/mOTWWeb5q6Zpbhj/+UySlFL6k/yrJLcleUOS20spbziXxrL47rjjjtx7772zzSYPLSIT1OSBmjxQkwdq8kBNHqjJAzV5oCYP1OSBczVrYaRpmvuTPH8Wy35rkieaptnRNM3pJH+S5H1nsRyWkJtvvjlr1qw5m4/Kw8uUTFCTB2ryQE0eqMkDNXmgJg/U5IGaPFCTB87VfD1j5O2llB+WUu4ppWwcf+/qJE9V8+wef6+nUspHSylbSilb9u3bN0/NYpHIA93OKRPy8LIjD9TkgZo8UJMHavJATR6oyQM1eaAmD0xrPgojDyZ5RdM0P5Pk/0ny1bNZSNM0n2uaZnPTNJvXrVs3D81ikcgD3c45E/LwsiIP1OSBmjxQkwdq8kBNHqjJAzV5oCYPzOicCyNN0xxumubo+L+/lmSwlHJZkqeTXFvNes34e7yMyQPdZIKaPFCTB2ryQE0eqMkDNXmgJg/U5IGaPDCbcy6MlFLWl1LK+L/fOr7MA0m+n+Q1pZRXllKWJflQkj8/1+9jaZMHuskENXmgJg/U5IGaPFCTB2ryQE0eqMkDNXlgNgOzzVBKuTvJLUkuK6XsTvKpJINJ0jTNXUn+5yT/uJQynOREkg81TdMkGS6lfCLJ/5ekP8kXmqbZfl7WggVz++2357777sv+/ftzzTXX5NOf/nSGhobqWeShZWbIROd3DGWiReSBmjxQkwdq8kBNHqjJAzV5oCYP1OSBc1XG8rC0bN68udmyZctiN4MzVErZ2jTN5vlerjy8NMkDNXmgJg/U5IGaPFCTB2ryQE0eqMkDNXmgNlMe5uPh6wAAAAAAAC8JCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGrMWRkopXyilPFdK2TbN9A+XUn5USnm4lPLtUsrPVNN2jr//UClly3w2nMVx55135vLLL8+mTZt6TpeHdpEHavJATR6oyQPdZIKaPFCTB2ryQE0eqMkD52ouvzHyh0lunWH6T5O8s2maNyb5rSSf65r+rqZpbmiaZvPZNZGl5I477si999470yzy0CLyQE0eqMkDNXmgm0xQkwdq8kBNHqjJAzV54FzNWhhpmub+JM/PMP3bTdMcHH/5QJJr5qltLEE333xz1qxZM+10eWgXeaAmD9TkgZo80E0mqMkDNXmgJg/U5IGaPHCu5vsZI/8wyT3V6ybJ10spW0spH53n72Lpkwdq8kBNHqjJAzV5oJtMUJMHavJATR6oyQM1eWCKgflaUCnlXRkL2Tuqt9/RNM3TpZTLk3yjlPLX47+B0uvzH03y0SS57rrr5qtZLBJ5oCYP1OSBmjxQkwe6nUsm5OHlRx6oyQM1eaAmD9TkgenMy2+MlFLelOTzSd7XNM2BzvtN0zw9/t/nknwlyVunW0bTNJ9rmmZz0x6mpZ0AACAASURBVDSb161bNx/NYpHIAzV5oCYP1OSBmjzQ7VwzIQ8vL/JATR6oyQM1eaAmD8zknAsjpZTrkvxZko80TfNY9f5FpZSVnX8n+bkk2871+1ja5IGaPFCTB2ryQE0e6CYT1OSBmjxQkwdq8kBNHpjNrH9Kq5Ryd5JbklxWStmd5FNJBpOkaZq7kvxGkrVJ/nUpJUmGm6bZnOSKJF8Zf28gyRebprn3PKwDC+j222/Pfffdl/379+eaa67Jpz/96QwNDdWzyEOLyAM1eaAmD9TkgW4zZKLzv+LJRIvIAzV5oCYP1OSBmjxwrkrTNIvdhik2b97cbNmyZbGbwRkqpWwd72DmlTy8NMkDNXmgJg/U5IGaPFCTB2ryQE0eqMkDNXmgNlMe5uUZIwAAAAAAAC8FCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGgojAAAAAABAayiMAAAAAAAAraEwAgAAAAAAtIbCCAAAAAAA0BoKIwAAAAAAQGsojAAAAAAAAK2hMAIAAAAAALSGwggAAAAAANAaCiMAAAAAAEBrKIwAAAAAAACtoTACAAAAAAC0hsIIAAAAAADQGnMqjJRSvlBKea6Usm2a6aWU8nullCdKKT8qpbylmvbLpZTHx39+eb4azuK58847c/nll2fTpk09p8tDu8gDNXmgJg/U5IGaPFCTB2ryQE0eqMkDNXngXM31N0b+MMmtM0y/Lclrxn8+muT3k6SUsibJp5K8Lclbk3yqlLL6bBvL0nDHHXfk3nvvnWkWeWgReaAmD9TkgZo8UJMHavJATR6oyQM1eaAmD5yrgbnM1DTN/aWU62eY5X1J/l3TNE2SB0opl5ZSrkxyS5JvNE3zfJKUUr6RsQLL3WfSyK1PHswDOw7kpg1rc+Mrps/p1icP5g/+8ifZe/hk/t7fuC6vW78yf/CXP8kPdh3MyGiTD26+Nu/euD7/11cezo79x3LBYF/e9sq1ueV1l+crP9idnzx3NKeGR3NyaCQbLrsob92wNo/tPZKDx4dy9ORQ9h89nZHRJk3GKkqvW78y73ztumx/9nA2XrkqR04N57G9R7Jz/7GcHB7J69evyj+97fWT2rz1yYP58oO788TeI3n++FBeedlF+fg7XzVlnnp9e63/XN87H26++ebs3LlzplkWLA/f2L4nf/y9XTk9PJqVFw5ksJS8cGIol1wwmP6BvpweGsnp0dE0o8nqFcvy3OGTOTUymlUXDOTE0GiGR0ezfKA/l69cnoG+kleuuzjvet3lOXj8dB7feyT3P7Yv/X0lb75udT42vp/qnL19w9qsvHAwN21YmyQT7UqSP3twd5okH3jLNTPu3yT54nd35Z5tz+a2TVfm77/tuinzHjkxlO3PHp4yvVYv43XrVy5IFpLkole8MX/x7R/mxNDIdLMsif5hNp/92iO5d/ue3Lpxfd69cf2kfdn979UrluWrP9idR549nEsuHMxlFy/Po3uPZPlAf/7GK9dMHNOd433/kVNJkoPHT+fg8aEcPHoqL5wYyrL+vlx0wUBOnh7JiaGRNEnWXrQsH3jLNTl8ajhP7D2Spw+dzIUDfdl09SU5cOx0btt0ZXYdOJavPvR0rl2zIr/w5muy7ZlD2X/kVJokl69cnl98yzVJki8/uDslycrlAxP5mSkb87EtXy556F7mdMfzL/3b7+Z7O5/PW69fk3/3D9+WL353V/70+7tyxaoLcst4X3LThrX51bsfzNMvnMxFy/pz2crlueHaS3P/Y/ty8PhQXr3uovyPr78i927fk0svHMwLJ4Zyw7WXZsXygTyx90ge3n0op0ZG86arL8mqCwfzvZ3P57KLlmWgvy+3blyfPYdP5r7H9uVNV1+Sa9esmGhnkkn9x8YrV/Xsr3qdQ3pN72W6vqtjqeehPu73HD6Z//LI3qy9aHne8ZrLcuzUcO7ZtienhkcnfebiZf158ytW57ZNVyZJ7tn2bEqSXc8fzw3XXprjp0fy4FMHc3poNFdfemGSTOTg+OmR7Nh3NK9cd3E+/s5XJXnxON141SU5ePx0Vq9YNjF/rzafa8Y7n+/+nrNZ7pl+ZrHzcLbt7+R87UXLJvrhTt4/+7VH8tWHns4Fg/0Z6CtZc9GyXLJiWUqSy1YunzgW/9k9j+SJ545m1YWDGRptcvTkUJomufrSC3PxBQN59oUTuWCwP3e+Y0O+9P1d+dHuQ1mxvD+//p435O+/7bpJ44Hv7DiQ08OjOXJyKE2SkZEmR08P59XrLs7KCwcn2td9fHb6p+UDfXn1FSuz6apLsv2ZQ3nuyKmJc0e9Hbo/P1MfMVtf0Mtijyd7mS0T9flg0/gxe9OGtXl0z5H8628+nkMnhnLtmhW5evWKiQz8+OlDefjpQyml5IpVy7NuYszQl9desTKXrliWHz31QvYfO51VFwxktGlyfGgky/v7cnJoNIMDffmlm16Rd29cnz97cHce33skzx87nQ3rLs6KZf156KkXcsO1l+Y1V6zM6hXL8s1Hn8tz42PUw6eGJ8YHdSbncrwu1HVFx2L3DzONuWe7NuucSzr7oX7/qw89nevWrMgFg/350dOHcv2aFVl54eDEOfnxvUfy0FMvZLCvZN+x07nltevyux96c37tT36Q+x7bl1teuy7rV13Qc/nv/5f/PdueOZxNV63KVz/xjonjsHNOunXj+nzyPa/PZ7/2SL7wrZ9maKTJz1xzyaR5u4/v7vWv1/XRPUdyz7ZnJ40nZjp/9PqO7vHUdJ9d7DzU44NPvuf1U3LSa92T9Jx25MRQ/tOPnsmJodF88MZrJq4z6vNwMnbteM/Dz+bgiaH0jbU9/SXZsO7irLxgIKeGR3Pg6Kk8c+hkLhzsz0duekUOnxrOQ7sO5vljL/ZFOw8cy6UXDuai5QMZGmlyangk77/h6ly39qIZrzd7jQumG//O9Nnu7TCTufYzi52HXvv9yw/uzg+ePJgn9h3NyGiTSy8czGjT5IUTwz2X0V+S0Wbs3z9zzSU5NTyanQeO5fKVF+Qdr7ls4hz8xe/uyhe+9dOcGBrJ1ZdckNdcsXJi2nT5m23s+HKzFMcPZ6rXeKLuf5PkC9/6adI0ufMdGyau4R/fe2Rin1+7ZkWSpEny+N4jeeaFE+kvJQP9JRcvH8jRU8O55MLBvPdNV2XrroN5dM+RlDJ2b+CqSy/MpSuWZd3K5RP3CjZeuSoP7jqYXc8fz/tvuDpJJs49P9j1Qp49dCJvuHJV3nD1Jdl/5FQuW7k8q6rPrrxwcNbrzvrf85XVxe4fztRs19xzGX93jvnVK5Zl+zOHJl3/3/WXP8lP9x/L0PDoRP//yfe8fuI+5o/H57/60gsnrge6l7XpqkvyzUefy7d/sj8nh0ZzyQUDuXbNirzysosm3ZPq5KNz3+L5Y6dz8PjpvHB8KP19JYP9fbl0xWB+/k1X5cFdY/3lq9ddnPeP38MqyZR7Vxur8XX3+p6vPm5OhZE5uDrJU9Xr3ePvTff+nG198mA+/PkHcnp4NMsG+vLHv3LTtCflv/e572R4ZOxs88PdD6evOvkkyV3378hd9++YeH1qeDRf//HefP3He6cs7/F9x/L4vmPTtms0ySN7juSRPUeSJH/1+P4p83xv58F88A++ky997O0TJ67bP/ednB55sVFPPHc03/zrvfnTj/3sxDz1+v7GezfmM3+xfdL6J5myTXq9t4gnxQXJQyklI9UOPnXk9MS/TwydmvLZwydfHKTUA5bjp0ey88DxJMkT+47lGz3y8PUf781/e/S5fObvbMqn/nxbhiZyNnYwD/aXpJQMj4xmoL8vo00zkcX/sOWp3P3Rt/fcv3/8Kzfl0T1H8utfeTjJiznqdIAf/vwDOTU0ms5a1tNrX/zurknLGOgby/75zkKnjcf2P5vnDp7I1icP9vquRe8fZvPZrz0y0Tfcdf+O/Jv/viNNkwz0Td6vaZoMjzaT+pUjp0ay+4WTSZITQ6P5xo/35pt/vTefed8b85t/vm3S8d7txPBoThw9Pem9fUdPT+qnOp4Y74/qvmbP4VP5/s6DU+b90pan0jRNuu7n5q8e35/B/rHjpnt7zce2fLnkoXuZt/+bsWUmk4/nX/q338394/vj/sf3592/c1913jiUr/94b/pK0jSZOIaPnh7J0QPHJ/qcpHO+mbzP6+kdD+0+NPHvTubqrNxfZeNL39+Vvr6+DA1P7j9KksGBF7Pc6xxS536m7djd7ySZcqG9lPPQfdx3HD11PE9+d9e0yz16eiR/9fj+nuf97v12eHyMkCRffeiZiX8/se9Y/tsje1P6ysS5otZXevff55rx+vOjzdiN0uWDvccac7mZcSZtWew8nG3765x3dPb9rgPHpvbXXWPHL215KqPVeeP540OTptcZSTLpu46eGsmvf+Xh7DpwLH/4nZ2TxgO9dPqIv3p8f7730wMTmet+nYyNUbv9+627c/c/umnixkx9fHfa0KuPuOPt109sh+nGKWdpwfKQzJ6J7vNBMnYM9fdl0vn2x88eyY+fnbxfkyRNk6dfOJmnqzFD936o83F6eOwCf+j0SO66f0c+91c7Jo0/nqiy1uuc8cPqnFGrz2PTOR/n05ksdv8w05j7detXznhtduvG9RPHVmc/XDA4+f09h1+8Lnno+KFJy+/21YeeyZadz0+c5+vjtl7+tZdeODHmeGj3obzjs/914jMdd92/Iw/sODBp/NA9b318d69/fdz3VeerTtsvmOH80asP+cK3d04aT/3m39nU87OLnYde44POTaXpstDdL3amdffbneuM0dGxsWFfyZRrxyQZScbGasnE/YbasfF+oVZnZexa+MXc1fPOdL3ZGX/8xns35jf/0/ae49+OXp+d6xiy/vxc7vMshf6h3u+9rvG6z+/d6tnrY/LJ58fGnf9+6+7c+bPXT9pXTx88ke/tPJh/v3V3fvPnJx9r3RmbbuzYUgs6fjhTvcYTte7zw69/5eEMdI019hw+1bNvSJpkeOweRTL23+6+4tCJ4Snni+7vrT9TjzEe2n1oUn6na/PEdWd/7/sp83mtvpSuL+ba3tPDvbdH973BXuPvzvV9SSadX7605ak0o026Ly3vun9H9hw+mb94+NlJ55mnXzg5MQ7tXla3548P5fnjhybGlvX+7jUGTcb+x63TIyNTzlffO3Zw0vi3172r7vNJfd16Pvq4JfPw9VLKR0spW0opW/bt2zfx/gPj/1fcaJMMDY/mgR0Hen7+gR0HptxcGJ1pzy6QkdFmos0P7DgwcUO9NjyaSfPU63vPtmenrH+vbTLX7fRSMZc8jCzwDh4eaXLPtmen7MMmydBIk6Fq+9dZHBpppt2/D+w4kHu2PTtpeZ3XnXm717J7/l7vDY9mQbJQt7FpmvP2XefaP8zm3u17Jr0e6Wy/rv06NNLMqV8ZHk3PrCyUoZGpRZF6Wq/tNR/b8uWSh9oDOw5kqNqY9fH8vZ3PT5r3J/unFtNHm5kHGefL8GgmFUU6mkzOcs9zSFfup9uO0/VdHUs9D93H/UIbadKzKJJM33+fa8brzycv5qHXWONMljWXzyx2HqZrz5nmvH5/LhkanuN5Yyb3bt/Tczwwk/se2zfj617q7dC93p029OojurfDdNtsMcw1D8nsmeg+HyRjx9B059v5Nl/D3vo8Np2Fvq5Y7P5hpjH3bNdmvY6t6d6fq6d73LDqXn73mGO6z2x75vCsy5+uj6mP+17nq5nOH736kO7x1HSfXew8dPdpndczZqGrX+xM63XYjoy+ODbsde24EKa73qzbP934t6PnZ+c4hqw/P5f7PEuhf5i0bc7D/up1Pq2ndR8v3Rl7udwHWirOZPxwpnqNJ2azUGON+dTrPtl09yLO1mL3D2equz/p3h5zGX93jvnuXmh4ZGpRpOO+x/bNeJ5ZzFvnve5ddZ9PkhevW8/HPp6vwsjTSa6tXl8z/t5070/RNM3nmqbZ3DTN5nXr1k28f9OGtVk20Jf+MvZ/uXZ+nabbTRvWZqC/THqvr/ScdUH195WJNt+0Ye1YxbTLQF8mzVOv722brpyy/r22yVy30wJZkDz0L/AOHugvuW3TlVP2YacSPlht/zqLg/1l2v1704a1E78q2dF53Zm3ey275+/13kBfFiQLnTZ2ft17mu9a9P5hNrduXD/pdX9n+3Xt18H+Mqd+ZaAvPbOyUAb7Swam6d0H+0vP7TUf2/LlkofaTRvWjv2Gxbj6eH7r9Wsmzfuqyy6a8vm+kinH8EIY6BvbBt3f3fmNke4cTNp2XbmfbjtO13d1LPU8dB/3C62/ZMq4pfOqb5o2n2vGJ/bJ+Bf1ZfqxxlyXNdfPLHYezrb9vc65nffnkqGBOZ43ZnLrxvU9xwMzueW162Z83Uu9HbrXu9OGXn1E93aYbpudhQXLQzJ7JrrPB8nYMTTd+Xa+zdewtz6PTWehrysWu3+Yacw927VZr2Nruvfn6upLL5hx+uBA35Qxx3Sf2XTVqlmXP10fUx/3vc5XM50/evUh3eOp6T672Hno7tM6r2fMQle/2JnW67Dt73vxJkznfN+9fc+37uvN7vbctunKace/HT0/O8cxZP35udznWQr9Q71vz8c1Xq/zaT2t+3jpvO7ed4t8H2ipWNDxw5nqNZ6YzUKNNeZTr/tk092LOFuL3T+cbXun2x5zGX93otDdCw2ML6uXW167bsbzzGLeOu9176o+n3Rft56PPq6M/Zm1Ocw49oyRv2iaZlOPaX87ySeSvCdjD675vaZp3jr+MJutSd4yPuuDSW7s/A236WzevLnZsmXLxGvPGFlazxhJkp07d+a9731vtm3bNvFeKWVr0zSbFzIPnjHyosV6xsjWJw/mP3/7h/l/f/Mf5yePPjLx/mLkwTNGFv8ZIy+XPHSvk2eM9DbbcwWWeh48Y2RhnzGy2Hk42/Z7xsj5ecZIsrjjyV48Y2Tu22K+LXb/4BkjS+sZI4udB88YWVrPGFnsPHjGyNKz1MYPZ8ozRub3Wn0pXV/Mpb0zbQ/PGDk/zxjp5KHnxKZpZv3J2MNnnk0ylLG/u/YPk3w8ycfHp5ck/yrJT5I8nGRz9dk7kzwx/vMP5vJ9N954Y8PS9aEPfahZv359MzAw0Fx99dXN5z//+eb3f//3myRPNvLQOvJATR6oyQM1eaAmD9TkgZo8UJMHavJATR6YiyRbmmn26Zx/Y2QhnY+KLOffjBW4cyAPL03yQE0eqMkDNXmgJg/U5IGaPFCTB2ryQE0eqM2Uh5fgX6oDAAAAAAA4OwojAAAAAABAayiMAAAA/P/t3X2UnFdhJvjndrdatmxZ/sTYkr+EjIPl8YAtE7PZQ0iWwSabFUySk4hJZsI6hCQLM5kzOTsD5BwPCzMTz+TsJNl1NglDGLITiGGYZOxkg4lDIOzssTGyw4cF8ZcMtoSN5W/ZsiV16+4fXS1ftbqlltTqqur39zunj7qqq6rvW+/Tt+5bj6oKAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpjXsVIKeXaUsq9pZQHSinvm+Xnv1FK+Wrv675SyjPNzyabn92ykIOnP2699dZccsklWbduXW644YaDfi4P3SIPtOSBljzQkgda8kBLHmjJAy15oCUPtOSBY1ZrPeRXktEkDyZZm2Q8ydeSXHqIy//jJB9rTj9/uN8x8+vKK6+sDKaJiYm6du3a+uCDD9bdu3fXyy+/vG7ZsqXWWmuSzVUeOkUeaMkDLXmgJQ+05IGWPNCSB1ryQEseaMkD8zVbHqa/5vOKkdcneaDWurXWuifJTUnedojLvyPJH83jdhlCd955Z9atW5e1a9dmfHw8mzZtys0333yoq8jDEiYPtOSBljzQkgda8kBLHmjJAy15oCUPtOSBhTCfYmR1kkea09t65x2klHJBkouS/FVz9gmllM2llDtKKW8/6pEyELZv357zzjtv/+k1a9Zk+/bts15WHpY+eaAlD7TkgZY80JIHWvJASx5oyQMteaAlDyyEsQW+vU1JPlNrnWzOu6DWur2UsjbJX5VSvlFrfXDmFUsp707y7iQ5//zzF3hY9Ik80JIHWvJASx5oyQMteaAlD7TkgZY80JIHWvLArObzipHtSc5rTq/pnTebTZnxsqRa6/bev1uTfDHJ62a7Yq31I7XWDbXWDWedddY8hkU/rF69Oo888vILiLZt25bVq2d9AVEiD0uePNCSB1ryQEseaMkDLXmgJQ+05IGWPNCSBxbCfIqRryS5uJRyUSllPFNhumXmhUop35fktCS3N+edVkpZ3vv+zCQ/kOSbCzFw+uOqq67K/fffn4ceeih79uzJTTfdlI0bNx50OXnoBnmgJQ+05IGWPNCSB1ryQEseaMkDLXmgJQ8shMO+lVatdaKU8t4kn0symuRjtdYtpZQPZepT3adLkk1Jbup92vu01yT5vVLKvkyVMDfUWgVtiI2NjeXGG2/MNddck8nJyVx33XVZv359rr/++iRZ1VxUHjpAHmjJAy15oCUPtOSBljzQkgda8kBLHmjJAwuhHJiLwbBhw4a6efPmfg+DI1RKuavWumGhb1cehpM80JIHWvJASx5oyQMteaAlD7TkgZY80JIHWofKw3zeSgsAAAAAAGBJUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ0xr2KklHJtKeXeUsoDpZT3zfLzd5ZSdpRSvtr7elfzs58tpdzf+/rZhRw8/XHrrbfmkksuybp163LDDTcc9HN56BZ5oCUPtOSBljzQkgda8kBLHmjJAy15oCUPHLNa6yG/kowmeTDJ2iTjSb6W5NIZl3lnkhtnue7pSbb2/j2t9/1ph/udV155ZWUwTUxM1LVr19YHH3ywmLloLwAAIABJREFU7t69u15++eV1y5YttdZak2yu8tAp8kBLHmjJAy15oCUPtOSBljzQkgda8kBLHpiv6TzM9jWfV4y8PskDtdattdY9SW5K8rZ5XC9JrklyW631qVrr00luS3LtPK/LALrzzjuzbt26rF27NuPj49m0aVNuvvnm+V5dHpYYeaAlD7TkgZY80JIHWvJASx5oyQMteaAlDyyE+RQjq5M80pze1jtvph8vpXy9lPKZUsp5R3jdlFLeXUrZXErZvGPHjnkMi37Yvn17zjvvvP2n16xZk+3bt892UXnoAHmgJQ+05IGWPNCSB1ryQEseaMkDLXmgJQ8shIX68PU/TXJhrfXyTLVsf3CkN1Br/UitdUOtdcNZZ521QMOiT+SBljzQkgda8kBLHmjJAy15oCUPtOSBljzQkgcOaT7FyPYk5zWn1/TO26/W+mStdXfv5EeTXDnf6zJcVq9enUceeblU3bZtW1avPrBUlYfukAda8kBLHmjJAy15oCUPtOSBljzQkgda8sBCmE8x8pUkF5dSLiqljCfZlOSW9gKllHOakxuTfKv3/eeSvKWUclop5bQkb+mdx5C66qqrcv/99+ehhx7Knj17ctNNN2Xjxo0HXEYeukMeaMkDLXmgJQ+05IGWPNCSB1ryQEseaMkDC2HscBeotU6UUt6bqYCMJvlYrXVLKeVDmfpU91uS/JNSysYkE0meSvLO3nWfKqV8OFPlSpJ8qNb61HHYDhbJ2NhYbrzxxlxzzTWZnJzMddddl/Xr1+f6669PklW9i8lDR8gDLXmgJQ+05IGWPNCSB1ryQEseaMkDLXlgIZRaa7/HcJANGzbUzZs393sYHKFSyl211g0LfbvyMJzkgZY80JIHWvJASx5oyQMteaAlD7TkgZY80DpUHhbqw9cBAAAAAAAGnmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHTGvIqRUsq1pZR7SykPlFLeN8vP/1kp5ZullK+XUj5fSrmg+dlkKeWrva9bFnLw9Mett96aSy65JOvWrcsNN9xw0M/loVvkgZY80JIHWvJASx5oyQMteaAlD7TkgZY8cMxqrYf8SjKa5MEka5OMJ/lakktnXOaHkqzoff9LST7V/Oz5w/2OmV9XXnllZTBNTEzUtWvX1gcffLDu3r27Xn755XXLli211lqTbK7y0CnyQEseaMkDLXmgJQ+05IGWPNCSB1ryQEsemK/pPMz2NZ9XjLw+yQO11q211j1JbkrytvYCtdYv1Fp39U7ekWTNPG6XIXTnnXdm3bp1Wbt2bcbHx7Np06bcfPPNB1xGHrpDHmjJAy15oCUPtOSBljzQkgda8kBLHmjJAwthPsXI6iSPNKe39c6by88l+Wxz+oRSyuZSyh2llLfPdaVSyrt7l9u8Y8eOeQyLfti+fXvOO++8/afXrFmT7du3H+oq8rCEyQMteaAlD7TkgZY80JIHWvJASx5oyQMteWAhjC3kjZVSfibJhiQ/2Jx9Qa11eyllbZK/KqV8o9b64Mzr1lo/kuQjSbJhw4a6kOOiP+SBljzQkgda8kBLHmjJAy15oCUPtOSBljzQkgfmMp9XjGxPcl5zek3vvAOUUt6c5FeTbKy17p4+v9a6vffv1iRfTPK6YxgvfbZ69eo88sjLLyDatm1bVq8++AVE8tAN8kBLHmjJAy15oCUPtOSBljzQkgda8kBLHlgI8ylGvpLk4lLKRaWU8SSbktzSXqCU8rokv5epkD3enH9aKWV57/szk/xAkm8u1OBZfFdddVXuv//+PPTQQ9mzZ09uuummbNy48YDLyEN3yAMteaAlD7TkgZY80JIHWvJASx5oyQMteWAhHPattGqtE6WU9yb5XJLRJB+rtW4ppXwoU5/qfkuSX09ycpL/XEpJkodrrRuTvCbJ75VS9mWqhLmh1ipoQ2xsbCw33nhjrrnmmkxOTua6667L+vXrc/311yfJqt7F5KEj5IGWPNCSB1ryQEseaMkDLXmgJQ+05IGWPLAQSq2D9/ZoGzZsqJs3b+73MDhCpZS7aq0bFvp25WE4yQMteaAlD7TkgZY80JIHWvJASx5oyQMteaB1qDzM5620AAAAAAAAlgTFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGYoRAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADojHkVI6WUa0sp95ZSHiilvG+Wny8vpXyq9/Mvl1IubH72/t7595ZSrlm4odMvt956ay655JKsW7cuN9xww0E/l4dukQda8kBLHmjJAy15oCUPtOSBmWSCljzQkgeOxdjhLlBKGU3y20n+XpJtSb5SSrml1vrN5mI/l+TpWuu6UsqmJP82yU+VUi5NsinJ+iTnJvnLUsqra62TC70hg+qu7zydO7Y+mavXnpEk+7//rb+8L3d++6m8/sLT88tvfnX++O5tue97O/PtJ17IZK35ySvPy/lnnJTP3vNo1p9zSrY+8UK2fPfZlCQrT1iWvZP7smx0JI8++1J27ZnIyuVj2bOvZqQkJ5+wLOvPOSVvuuQV+eK9j++/3qXnrsqK8dF8/m+/l5KSH/6+V+Sk5WO5+ztP5+lde/L2167O+37kNYfcjqsuODXv+oVfys//m9/PNa9fn1/4ibdk48aNufTSS9uLD1Qe7vrO0/kvd2/LEzt355GndmX7My9mz8S+vDSxb9bLj4+WrDpxWS4686ScumI8NckrVi7Pj12xJldecNoB+3S204cax9Fcbnr8JcmPXbEm9z62M5/6ysM5+5QT8gs/+KpD3tbxNjk5mfe85z257bbbsmbNmlx11VUDn4fjrd1/9z62Mx/7b1vz7It7s+rEZXnza87OyhOX5bQV43l6156ctmI8X7z38dz98NPZ+dJERkdKLjv3lJx76om5Y+uTWT42ml17JvLCnsmMJBlfNpI3XnxWvvvMi3lgxws5e+XynHf6itQkJcl939uZJ57fnXVnnZwdz+/Ok8/vyUjvNv/FW1+zPyvTYzxtxXi2fPfZ1CQ/3sv3sbhz6xP5h9e9Ox/91M156/ev73seZv7tJNm/3U/v2jPn3/Anv/xwPnvPo3nrZefkkleu3H+dL977eL733EtZdeKyPPzUrly7/pX5/Le+l/t3vJBlIyWrTlqWiYmaF/dOptaaUkpOOWEsF555UkqS3RP78lNXnZ8k+e0v3J+dL01k9akn7t+Hr1i5PC/snshXH3km165/5QGPAStPXDbn3DHfueVYr3OkluL80GZq/bmrcs93n80TO3fn6V178ugzL2bn7onUmqxYNpqU5NVnr8zXtj2TF3ZP5rQVy/L87oksHxvJq89emVUrxvPsrj3Z+uQL2bcvuezcU/LdZ17Mcy/tzbLRkVx67qr84g++av888tLeyYyOjGT3xGSuXntGTlo+tv9vN8kBa40/vntbHt+5+5CPXYttKebheJltX02f9+WtT+aOrU9mZKTkojNOyprTV+zfz7/1l/flvz3wREpJ/s65q1KTLB8bybqzV+ab25/NN7Y/m1KSV55yQv6XH7o4t97zaL50/xNJkovPOim3/cqb9s9/6885Jc/tnphady4fy+1bn8yeiX0ZHxvJT111/v658WjzdOfWJ/KOd/583vEvP5J/9OYrhmI9uZhmHj/83l8/mC3ffTYnjo/l3FUn5OGnduXUE5flm48+l72TNeOjJcvGRnLF+aflpb2T+fq2Z7NnYl9GR0pOWDaSV55yQh597qWMj45kYl/N8rGRrBgf2z+frFg+lid27k6SPLNrT3ZP7Msb1p5xwGPP8ZxDBn1+uOHPv5VbtzyWa9e/Mu/7kdfsvy92vrg3Wx59LmecNJ4nX9iTnS/uzbef2pU3vfqs/Oam1+WTX354/5r9oR3P56End+Xk5aNJKTl5fDTP75lMas3zuyf3n3/h6Suy8sRleeyZF/PIMy/m+85emd0T+/LI07vy5tecnd/c9Lr945pr/TJz3XDXd57OH9+97ZDrvbn2bz8eOwY9DwtpOiOvOOWE/OIPvir/6fZv54v37cjlq1flpb2TeeDx57PuFSfnivNPy5ZHn8v6c07JX9+3Iw/ueD7joyNZddJ41p9zStaeeVJu3/pkntm1Nzt3T+Tslcuz/Zld2fnSZFadOJZTV4zn1BOX5bHnXsr5p6/Yf3tvveycJNmfm7sffjoPP7UrV689IxefvXLe+33m2vlweTtSc2VihqHIxPRa8oHv7cz2Z15MSsn6c07pPU/zePZM7MvKE8ey9oyXn4u4+ztP5ekX9uak5aM5YdloJvfVrBgfzWPP7c4JYyO59NxTkiRbn3ghu/ZM5vQVy/Kjl5+blScuy84X9+b2rU9m+dhITl0xPuscn2TOv/P2uLE9fuqnpZSH+Zi5Jrjhs9/K3z62M6ecMJarLjw992x/Ni9O7MvqVSfk1BXjOXPl8lx27qo8vWvqcekv//bxfPuJ5zOxb+q5rnNWnZiduyeyd2IyL+7dl/HRkr37akZGSi5fvSp//3Vrcs93n80D39t5wPHrp77ycB5/7qXs3D2RK84/Ld+/9gx5GFJzPU+dZP/89NQLezI+NpJloyNZdeKy3PPd5/L8S3un1pGjI1l9+oqcPD6aB3Y8n/NPX5EPv/3vHHBb9z62M7/9hfvz4t59eePFZ+bis1fm/u/tzB1bn0xqsmvvZMbHRrL2zJOyasV4SqbWoE+9sCcT+2qee3Fv1r3i5PyLt77mgNs93HOyMy87H6XWeugLlPKGJB+stV7TO/3+JKm1/lpzmc/1LnN7KWUsyWNJzkryvvay7eUO9Ts3bNhQN2/ePK8NGGR3fefp/PRH78ieiX0ZGx1Jas3Evppak0Pf6/3zi29ce1A50m7H3kfvzVNf+kTO/qkPZXxsJG/efXtWn3Zi3v/+96eUcletdcMg5eGu7zydd3zk9uyZPPZ7fHxsJB/8n9bnQ3+2Zf+TA9f/6IGnP/Guq+c82Ji+D4/kctf/6Pp88JZ79o9/dCSZbPqcZaMlN737DX17MLr99tvzwQ9+MJ/73OeSJL/2a1PTwqDm4Xhr99/oSMneQ+SuZHHngdGR5NO/8N8lSX76o3dk9959B/z+8dGSPzqGLN31nafzYx/8j9nx13+Y83/6X+cT77o6f/HJ303SnzzM/NsfGy0ZKSV7J6a2e6Rk1r/hd77hwvzul7buv52x3t/cIMzZJyw7eO6Y79zSOprrHI2lNj8s5OPJfI0kmb3Cf9nYSDIyMpKJyam1xr5aM9GMcbbHruO1zw9lqeXheJnt7zOZmrdf2nu4NBybs04ez47n98zrsstGSyb31aPK013feTpvv/5jeeJLn8jZP/XhjI+N5C17Bns9uZgOOH4YKZmsyeS+/jwKlSTLl81/vXu0Bnl+uOHPv3XAuuDtrz03t2557KB11EyvXbMqX9327GFv/0i9/bXn5jc3ve6guWLm+qXddx/806l9l8y+3ptrXbBY64WZBjkPC+mTX344H/iTbyzq7zwS0xk63H6fuR2jI2X/nHWsxxfT5srEBz7wgbtqrRuShX1O6njloR9ryUMpSZaNvfw81cy/8+k5YHq+mz5+6sc6srVU8jAfM59TnJjYd9hjg2mL8XzDbMeni61LeVgIcz1PPTY6kn379mWO/z9+WCMlGRspmdhXD/tc2NHe7mzP37Snx0ZKUkomJg9et0yvH2b9HfMYx+okjzSnt/XOm/UytdaJJM8mOWOe150e5LtLKZtLKZt37Ngxj2ENvjt6/7tuX032TuzL3smafQNciiTJrVseO+i8djt2P7MjIyvP3L9NO0dPyfbt22deZWDycMfWJxfsD3LvxL589p5HD9inM0/fsfXJOcdxNJf77D2PHjD+yRmT1N7JOudtLYbt27fnvPPO2396zZo1A52H4+2A/XeY3C32PDC5b2p802Oc+fuPNUt3bH0yLz2zI6Mrz9qf8X7mYebf/sRk3V+KJJnzb3jmHDgxIKVIMvvcMd+55VivczSW2vywkI8n8zWfdenEvqn9OL0/J2aM8Ugeq46npZaH42W2v8/p8463+ZYiSfavaY8mT3dsfTK7n30iY6ecNXVbQ7CeXEwz1xL9KkWSqce/xZhDBnl+mLku+OJ9O2ZdR810z3efm9ftH6kv3jc17plzxcxxtvtubzN/zLbem2tdsFjrhZkGOQ8L6bP3PLrov/NITGfocPt95na0c9ZCHasuRiYWIw/9WEseyvQ+nusxfeZxYz/Xka2lkof5mDkPH8lqcDGSJg/DZ67nqfdOHH0pkmT/unU+z4Ud9e0e7jnZ3nM+RzpXDcyHr9daP1Jr3VBr3XDWWWf1ezgL4uq1Z2R8bCSjZaqJXzZaMlqmmttBde36Vx50Xrsdo6MjGSnZv02vesXJx2UcC5WHq9eekWWjC3OPLxsbyVsvO+eAfTrz9PRLt2Ybx9Fc7q2XnXPA+Edn/MUuGy1z3tZSMizzwwH77zC5W+x5YHRkanzTY5w5+R9rlq5ee0bGRkf2/8+j45nL+eRh5t/+WO9tRqa3e2SOv+GZc+DYyGA8UM51v853bjnW6wyyxZofFvLxZL7mk72xkan9OL0/x2aM8Ugeq5aCYXm8mMtsf5/T5x1vZ508Pu/LTq9pjyZPV689I6MjL+d0GNaTi2nmWqK9rxbb9GPPUplDjiYPM9cFb3r1WRkfGznsOu6y3lvbLLQ3vXpq3DPnipnjbPfdsmb+mG29N9e6YKmtF2bq9/ww/TZWg2q+a/qZ23HA/D5Ex6qLkYd+rCUPZXofz/WYPvO4cWSJzgWz6ff8MG3mPHwkq8HFSJo8DJ+5nqdeNjaSYzncGCnN8cECznMH3O7hnpPtPedzpOuWw37GSJLtSc5rTq/pnTfbZbb1Xpa0KsmT87zuknXlBaflE++6eug/Y6TdjhVXjeUjv3l7fuYtl+TqtWfkLz751axefVChOjB5uPKC0/JH737Dgn7GyMz31Z7P+2zPzMKRXO6SV64c2M8YWb16dR555OWCfdu2bQOdh+Nt5v4b1M8YmR7jQn7GyJUXnJZ/9Q/emH/3a1/KH/ZesvgXfcxD+7d/uM8Ymfk3PD33DsNnjMx3bjnW6xyNpTY/zMzUsH3GyLF+JsSxWmp5OF7m+vucPm8pfMbIlRecln/zMz+YD3/4r/OO7z8/P3bFmoFfTy6m2Y4fBuEzRo7nHDLI88P0sdGgfcbIbHPFXOuGw33mw1zzzmKtF2Ya5DwspH/w/S+/b/8wf8bI9HYcz88YWSqZaNeSw/AZI+0cMEifMbJU8jAfs60JfMbIgbqUh4VwqOepk+H4jJFDPSc787LzUms95FemypOtSS5KMp7ka0nWz7jMe5L8bu/7TUk+3ft+fe/yy3vX35pk9HC/88orr6wMpr1799aLLrqobt26te7evbtefvnl9Z577qm11ppkc5WHTpEHWvJASx5oyQMteaAlD7TkgZnmysR0HuoCZ0IeBps80JIH5qPNw8yvw75ipNY6UUp5b5LPJRlN8rFa65ZSyod6N3xLkt9P8p9KKQ8keaoXtPQu9+kk30wykeQ9tdbJw/1OBtfY2FhuvPHGXHPNNZmcnMx1112X9evX5/rrr0+mWtdEHjpDHmjJAy15oCUPtOSBljzQkgdmmisTSc4tpWz0nFS3yAMteeBYlaniZLBs2LChbt68ud/D4AiVUu6qtW5Y6NuVh+EkD7TkgZY80JIHWvJASx5oyQMteaAlD7Tkgdah8jAInykLAAAAAACwKBQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6AzFCAAAAAAA0BmKEQAAAAAAoDMUIwAAAAAAQGcoRgAAAAAAgM5QjAAAAAAAAJ2hGAEAAAAAADpDMQIAAAAAAHSGYgQAAAAAAOgMxQgAAAAAANAZihEAAAAAAKAzFCMAAAAAAEBnKEYAAAAAAIDOUIwAAAAAAACdoRgBAAAAAAA6QzECAAAAAAB0hmIEAAAAAADoDMUIAAAAAADQGaXW2u8xHKSUsiPJd/rwq89M8kQffu/xtJjbdEGt9ayFvlF5mNUgjy2ZGt9JSywP8zHo++VoLNQ2He/5YSnd90tpW5LZt2epPV4cylLbn61hmR8G1VLOxtHq6vohGZ48LPY4B31+GNT9tlTHNeh5mE2/98VS/v3DmIej1e/9OF/9HGc/8jDo+2XQx5ccvzEu1flhGPZpvx3R8w8DWYz0Syllc611Q7/HsZCW4jYtlkG+7wZ5bMngj+94WYrbPSzbNCzjnI+ltC3J0tueI7WUt38pb9ticP8drMv3ybBs+7CMc7EM6v1hXIOj39vc9d+/VAzL/Tgs41wog769gz6+ZDjGOEjcX4d3pPeRt9ICAAAAAAA6QzECAAAAAAB0hmLkQB/p9wCOg6W4TYtlkO+7QR5bMvjjO16W4nYPyzYNyzjnYyltS7L0tudILeXtX8rbthjcfwfr8n0yLNs+LONcLIN6fxjX4Oj3Nnf99y8Vw3I/Dss4F8qgb++gjy8ZjjEOEvfX4R3RfeQzRgAAAAAAgM7wihEAAAAAAKAzFCMAAAAAAEBndLYYKaV8rJTyeCnlnua8Xy+l/G0p5eullD8ppZzazzEeqdm2qfnZr5RSainlzH6MbViUUs4rpXyhlPLNUsqWUsov93tMM5VSRkspf1NK+bN+j2WmUsqppZTP9P6OvlVKeUO/x3S8DUNmjkYp5YRSyp2llK/1tut/6/eYkjnn7tNLKbeVUu7v/XtaP8d4JObKzzBu01yZKaVcVEr5cinlgVLKp0op4/0e60I6kkyWKf9H7774einliv6N/NDm2K4PllK2l1K+2vv6keZn7+9t172llGv6M+rBt1QfMxbCIK9vjpdhy0MX99G0Usq1vfntgVLK+2b5+T/r7cevl1I+X0q5YBDG1Vzux3vHghsGYUyllJ9scv/J4z2mfjjUsfki/f6+zS+DehwxjEop3y6lfKO37trc7/G0ltpx2VzmMf+/s5Syo1kfv2uRx3fIuWYQjj/mMcY3lVKebe7D6xd7jINs2NaL/XQ0a9XOFiNJPp7k2hnn3Zbkslrr5UnuS/L+xR7UMfp4Dt6mlFLOS/KWJA8v9oCG0ESSX6m1Xprk6iTvKaVc2ucxzfTLSb7V70HM4beS3Fpr/b4kfzeDO86FNAyZORq7k/xwrfXvJnltkmtLKVf3eUzJ7PPc+5J8vtZ6cZLP904Pi7nyM4zbNFdm/m2S36i1rkvydJKf6+MYj4ePZ/6ZfGuSi3tf707yO4s0xqPx8cyypsjUvnxt7+vPk6SX2U1J1veu83+VUkYXbaTDZak+ZiyEQV7fHC/Dlocu7qP05rPfztQcfmmSd8yyn/4myYbeceRnkvy7ARlXSikrM7XvvjwIYyqlXJyp4+wfqLWuT/JPj/e4+uTjmf1xdLH0c34Z1OOIYfVDvXXXcS82j9DHs7SOyw4y33k2yaea9fFHF3UuUlxIAAAHtklEQVSQh59rBuH44+M5/Hz4/zb34YcWYUzDZNjWi/10xGvVzhYjtdYvJXlqxnl/UWud6J28I8maRR/YMZhtm3p+I8k/T1IXd0TDp9b6aK317t73OzP1B7W6v6N6WSllTZL/McliP9geVillVZI3Jvn9JKm17qm1PtPfUR1/g56Zo1WnPN87uaz31fc5ZI557m1J/qD3/R8kefuiDuoYHCI/Q7dNh8jMD2fqSaJkSLblSBxhJt+W5P/u3Vd3JDm1lHLO4oz0yBxiTTGbtyW5qda6u9b6UJIHkrz+uA1uiC3Vx4xjNcjrm+NpmPLQ1X3U8/okD9Rat9Za9yS5KVPz3n611i/UWnf1Ti7WceRhx9Xz4Uz9J4WXBmRMP5/kt2utTydJrfXxRRjXojvCx9Hj8fv7Nr8M6nEEC2upHZfNYb7zbN/MY67p+/FHv+fDYTdM68V+Otq1ameLkXm4Lsln+z2IY1VKeVuS7bXWr/V7LMOmlHJhktdlEf531RH4zUyVXPv6PZBZXJRkR5L/2Hvp2kdLKSf1e1CLaUAzc9R6L0P8apLHk9xWax3U7Tq71vpo7/vHkpzdz8EcrRn5GcptmpmZJA8meab5Twfb0o1F3Fz7b3WSR5rLDeP98d7ey/A/1rw9wlLYrkW31B4zjtEgr28WxRDkocv76EjnuJ/L4hxHHnZcvbdMOa/W+v8swnjmNaYkr07y6lLK/1dKuaOU0s9XVXRCP+aXITqOGHQ1yV+UUu4qpby734OZh6E8hjmE+c7/P95bH3+m944tg2RY1ulv6L393mdLKev7PZhBNQTrxX46qrWqYmQWpZRfzdRLlT7R77Eci1LKiiQfSOL9+Y5QKeXkJP8lyT+ttT7X7/EkSSnlR5M8Xmu9q99jmcNYkiuS/E6t9XVJXsiQv3T2SAxiZo5VrXWy1vraTP2vx9eXUi7r95gOp9ZaM4T/I+1Q+RmmbZqZmSTf1+ch9d0w7b95+J0kr8rU22I8muR/7+9whtdSfMw4WkOwvjnuBj0P9tH8lVJ+JsmGJL8+AGMZSfLvk/xKv8cyw1im3tLlTUnekeQ/lCH7bM9h0q/5ZRiPIwbUf19rvSJTb4f0nlLKG/s9oPlaYmvgQ/nTJBf23krxtrz8ihnm7+4kF/Tefu//TPJf+zyegTTo68V+Opa1qmJkhlLKO5P8aJKf7k3kw+xVmfpf/F8rpXw7U4uSu0spr+zrqAZcKWVZpiabT9Ra/7jf42n8QJKNvX15U5IfLqX8YX+HdIBtSbY1/xvoM5kqSpa8Ac7Mgui9JdoX0t/3ST6U702/HLj371C9JcMc+RnqbWoy84ZMvVx7rPejNUm2921gi2eu/bc9Sfu/yIbq/qi1fq/3RMe+JP8hL79d1lBv12Jb6o8ZR2HQ1zfH1ZDkodP7KPOc40opb07yq0k21lp3D8C4Via5LMkXe/vu6iS3lOP7Aezzua+2Jbml1rq39/aL92WqKGGBDcL8MgTHEQOt1rq99+/jSf4kg/9WpUN9DDOLw85ptdYnmzn/o0muXKSxzdfAr9Nrrc9Nv/1enfoMw2WllDP7PKyBMgjz+YA76rWqYqTRexnvP8/UYnbX4S4/6Gqt36i1vqLWemGt9cJMLUKvqLU+1uehDaxSSsnUZ2R8q9b67/s9nlat9f211jW9fbkpyV/VWn+mz8Par5erR0opl/TO+h+SfLOPQ1oUg5yZY1FKOWv6f++VUk5M8veS/G1/RzWnW5L8bO/7n01ycx/HckQOkZ+h26Y5MvOtTB0M/0TvYkOxLQtgrv13S5J/VKZcneTZ5u0GBt6M9yP++0nu6X1/S5JNpZTlpZSLMvUE152LPb5hsFQfM47FoK9vjqdhyUOX91HPV5JcXEq5qJQynqn74Jb2AqWU1yX5vUwdRy7WE4GHHFet9dla65nNseAdvfFt7teYev5rpl4tkt4TX69OsvU4jqmT+jm/DNlxxMAqpZxUSlk5/X2St+TltdegGrpjmMOYz/zfro835gg/+HkRDPzxRynllb05K6WU12fqueon+zuqwTEs68V+Opa16tjhL7I0lVL+KFMLsjNLKduS/Msk70+yPMltvb/JO2qtv9i3QR6h2bap1vr7/R3V0PmBJP8wyTd674maJB/otdYc3j9O8oneomFrkv+5z+NZDEs1M+ck+YNSymimFiafrrX+WZ/HNNfcfUOST5dSfi7Jd5L8ZP9GeMRmzU+Gc5tmzUwp5ZtJbiql/Kskf5OpRd2ScYSZ/PMkP5KpDyfflQGeI+fYrjeVUl6bqbdF+HaSX0iSWuuWUsqnM1WGTyR5T611sh/jHgJL9TGDoyMPQ6DWOlFKeW+SzyUZTfKx3rz3oSSba623ZOqts05O8p97x5EP11o3DsC4FtU8x/S5JG/prQ8mk/yvtdYl9wTYAByb93N+GcjjiCF0dpI/6c0pY0k+WWu9tb9DetkSPC47yDzntH9SStmYqTXwU0neuZhjnGM/LOuN/3czAMcf8xjjTyT5pVLKRJIXk2xaAu/gs5CsF4+jImsAAAAAAEBXeCstAAAAAACgMxQjAAAAAABAZyhGAAAAAACAzlCMAAAAAAAAnaEYAQAAAAAAOkMxAgAAAAAAdIZiBAAAAAAA6Iz/H4HacfNv5g0sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 2016x864 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wine_features = ['malic_acid',\t'ash',\t'alcalinity_of_ash', 'magnesium','total_phenols','flavanoids','nonflavanoid_phenols', 'proanthocyanins',\t'color_intensity',\t'hue'\t,'od280/od315_of_diluted_wines',\t'proline']\n",
    "\n",
    "# Create a figure to hold our plots\n",
    "plt.figure(figsize=(28, len(wine_features)))\n",
    "for i, feature in enumerate(wine_features):\n",
    "    data_p = X_w[:,i]\n",
    "    \n",
    "    # Create subplots for each feature within this figure\n",
    "    plt.subplot(1, len(wine_features) , i+1)\n",
    "    color =[\"blue\",\"red\",\"green\",\"black\",\"orange\"]\n",
    " \n",
    "    plt.scatter(data_p, y_w, marker='.')\n",
    "    # plt.title(wine_features)\n",
    "    # plt.xlabel(wine_features)\n",
    "    # plt.ylabel(\"wine type\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "js5teW_jPAAM"
   ],
   "name": "EC414_HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
