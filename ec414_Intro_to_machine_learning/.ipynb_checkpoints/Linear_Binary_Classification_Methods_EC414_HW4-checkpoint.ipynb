{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_ih-R1fJ6VG"
   },
   "source": [
    "# Homework 4: Linear Binary Classification Methods\n",
    "by Junyu Liu and Brian Kulis\n",
    "\n",
    "**Due date**: March 3, Wednesday by 11:59pm\n",
    "\n",
    "**Late** due date: March 6, Saturday by 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBxGJCzSKovR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mxor3LzKpqj"
   },
   "source": [
    "To run and solve this assignment, you must have access to a working Jupyter Notebook installation. We recommend Google Colab. If you are already familiar with Jupyter and have your own installation, you may use it; however, you will have to tweak Colab-specific commands we've entered here (for example, file uploads).\n",
    "\n",
    "To use Google Colab:\n",
    "\n",
    "1. Download this `ipynb` file.\n",
    "2. Navigate to https://colab.research.google.com/ and select `Upload` in the pop-up window.\n",
    "3. Upload this file. It will then open in Colab.\n",
    "\n",
    "The below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find Week 1 discussion material useful.\n",
    "\n",
    "To run code in a cell or to render Markdown+LaTeX press Ctrl+Enter or \"`Run`\" button above. To edit any code or text cell, double-click on its content. Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. You can add cells via `+` sign at the top left corner.\n",
    "\n",
    "**Submission instructions:** please upload your completed solution file as well as a scan of any handwritten answers to Gradescope by the due date (see Schedule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtN4g49nWrB"
   },
   "source": [
    "This homework is scored out of 100.\n",
    "\n",
    "\n",
    "**Important:** unless otherwise specified, you should **NOT** use loops. This is not to say loops are always bad, but avoiding them should help you:\n",
    "1.   get more familiar with common language features and libraries\n",
    "2.   write more efficient code\n",
    "3.   \"think in higher dimensions\" (get more comfortable with vectors, matrices, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_32VwlhsN9ny"
   },
   "source": [
    "## **Question 1:** Creating a Dataset (10 points)\n",
    "\n",
    "For this assignment, we will create a simple linearly-separable dataset for binary classification. We have provided you with the code to generate the feature vectors. Notice that one class has significantly more samples than the other.\n",
    "\n",
    "**Important:** Although this dataset has only 1 feature, **ALL** the code you write in this assignment should be able to run as intended with more features. The only exception is where you are producing plots. Many functions you need to write will be tested for compatibility with more features in question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS0rlO3eOCZK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lw5AUAsV5MQr"
   },
   "outputs": [],
   "source": [
    "# Do NOT change\n",
    "rng = default_rng(1)\n",
    "x1 = rng.uniform(-4, 2, 460)\n",
    "x2 = rng.normal(5, np.sqrt(3), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7rbmFam-xWg"
   },
   "source": [
    "### Problem a. (4 points)\n",
    "We need to create appropriate labels for the two classes. x1 is the feature vectors of the negative class and x2 the positive class. Also produce a colored feature-label scatter plot (the negative class should be blue and the positive class should be red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "1i19iWdkAVpY",
    "outputId": "43482206-b706-4779-951a-edad3f602993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 1) (460,) (40, 1) (40,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcb29035f50>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WRITE CODE HERE: \n",
    "y2 = np.ones((40,1))\n",
    "y1 = np.ones((460,1))\n",
    "y1 = y1*-1\n",
    "\n",
    "print(y1.shape,x1.shape, y2.shape,x2.shape)\n",
    "\n",
    "plt.scatter(x1, y1, marker='o',color =\"blue\")\n",
    "plt.scatter(x2, y2, marker='o',color =\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRLTGhczArfW"
   },
   "source": [
    "### Problem b. (3 points)\n",
    "Combine the data into X and y, and then perform a 75-25 train-test split. Use 32 as the random_state.\n",
    "\n",
    "Hint: X has to be a 2D array, so you need to use reshape at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofFs0MR9Dp7z",
    "outputId": "787964b4-f3df-4411-a306-7fa8a1711357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "(125, 2) (125,)\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE:\n",
    "xinter =np.append( x1,x2)\n",
    "Bias = np.ones((500,1))\n",
    "X = np.append(xinter, Bias)\n",
    "X = np.reshape(X,(2,500))\n",
    "X = X.T\n",
    "yinter = np.append( y1,y2)\n",
    "print(X.shape)\n",
    "random_state=32\n",
    "test_size=0.25\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, yinter, test_size=test_size, random_state=random_state)#\n",
    "print(xtest.shape,ytest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTlkYTs619A_"
   },
   "source": [
    "### Problem c. (3 points)\n",
    "To conveniently account for the bias term in later parts, we will also store the feature vectors in extended form (each feature vector appended with 1). Apply this to both X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LFCgyVU3EKY",
    "outputId": "ec6a5427-37d9-4d75-c49e-3a5e58b98859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 3)\n"
     ]
    }
   ],
   "source": [
    "#WRITE CODE HERE:\n",
    "baisTr = np.ones((xtrain.shape[0],1))\n",
    "Xtr_ext = np.append(xtrain,baisTr,1) # X_train, extended\n",
    "baisTe = np.ones((xtest.shape[0],1))\n",
    "Xte_ext = np.append(xtest,baisTe,1)# X_test, extended\n",
    "\n",
    "print(Xtr_ext.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf7z43OcK2ax"
   },
   "source": [
    "## **Question 2:** OLS Regression for Binary Classification (20 points)\n",
    "\n",
    "In class, we talked about one way to implement clasification is to use one of the linear regression methods we learned. We will investigate how to use OLS for binary classification and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxZX2J8wLVix"
   },
   "source": [
    "### Problem a. (3 points)\n",
    "\n",
    "When OLS takes a feature vector, it gives you a real-valued scalar. Assuming OLS works well (i.e., it gives a value close to the binary label (either -1 or 1) for most of the samples), how can you translate the value into the label? Suggest a simple method.\n",
    "\n",
    "Hint: we talked about this in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gdm7yHm72aa"
   },
   "source": [
    "use the sign operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEFD30vtfu3T"
   },
   "source": [
    "### Problem b. (7 points)\n",
    "For simplicity, we will use the OLS implementation from sklearn whose documentation can be found here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Fit the model to X_train, y_train and then extract w and b. You should do so by using get_wOLS_ext, which you need to complete. Note that the function should return a single vector, which is the regular w vector with the bias term appended to it.\n",
    "\n",
    "Lastly, create the following plot:\n",
    "* a scatter plot of the training data overlayed by the OLS solution. For this, You can assume that X_train has only one feature.\n",
    "\n",
    "Hint: w is the coeficient and b is the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "GvgEoJUMcpEh",
    "outputId": "7b09a4f7-f7bd-49d4-ccb6-aec9042f0039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15042363 0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb28fbb890>,\n",
       " <matplotlib.lines.Line2D at 0x7fcb28fbbad0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD5CAYAAAAuneICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdA0lEQVR4nO3deZhU1bX38e+imcURUJGpHTARcaSDOFzUAIpAQA0avDGKxpBgTIwaE9B7NSFXr169GYw8JqgoRiIqKKLi6wCi8RqRRmYQRBBoRGmHKFFkXO8fuzrV3fRQTVXXqarz+zxPPV171+lzVjXFWlV7n9rH3B0REYmfJlEHICIi0VABEBGJKRUAEZGYUgEQEYkpFQARkZhSARARiammmdiJmU0ABgOb3L1HDY+fATwFrEl0PeHuY+vaZ7t27by4uDgT4YmIxMa8efM+cvf2qWybkQIAPAjcDTxUxzZ/c/fBqe6wuLiY0tLSdOMSEYkVM1ub6rYZGQJy91eBTzKxLxERyY5szgGcbGYLzew5Mzu6pg3MbKSZlZpZaXl5eRZDExGJn2wVgLeAru5+HPBHYFpNG7n7eHcvcfeS9u1TGsISEZE9lJUC4O6fu/s/E/dnAM3MrF02ji0iIjXLSgEws4PNzBL3eyWO+3E2ji0iIjXL1GmgjwBnAO3MrAy4GWgG4O5/AoYBo8xsB7AFGO5ahlREJFIZKQDuflE9j99NOE1URERyhL4JLCKSK3btgl//GubMycrhVABERHLBrbdCURH86lfw859n5ZCZ+iawiIjsicceg+98J9k+7TSYOTMrh1YBEBGJwpw50Lt3sr3ffrBqFbRtm7UQVABERLLpvffg0EOr9q1cCd26ZT0UzQGIiGTD559D585Vk//s2eAeSfIHFQARkca1YwecfTbsuy+UlYW+Bx8Mif/00yMNTQVARKQxuMO110KzZvDCC6FvzJjQf+ml0caWoDkAEZFMGz8efvjDZPvcc2HKlHCaZw5RARARyZSXXoL+/ZPtI46A+fOhTZvoYqqDCoCISLqWLYOjq13mZP166NQpmnhSpDkAEZE9VV4OrVpVTf7z5oVx/hxP/qACICLScF99BT17woEHhvsA06aFxH/iidHG1gAqACIiqao4g6dVK3jrrdB3552hf+jQaGPbAyoAIiKpuP12aNIEHnootC+/PKzeed110caVBk0Ci4jUZepUGDYs2e7dO3yDt0WLyELKFBUAEZGazJ0LvXol23vvDatXQ7vCuZy5CoCISGXr1kHXrlX73n4bvva1aOJpRJoDEBGBsFhb165Vk/+sWWGCtwCTP6gAiEjc7dgBgwaFxdrWrQt9998fEv+ZZ0YbWyNTARCR+PrFL8JibTNmhPb114fEf/nl0caVJZoDEJH4uf9+uOKKZHvw4PBFrhxbrK2xqQCISHzMmgV9+ybbxcWwaFE4wyeGVABEpPC9/TYcdVTVvrVroUuXaOLJEZoDEJHC9dFHYSnmysn/zTfDOH/Mkz+oAIhIIdq6FU46Cdq3hy++CH1Tp4bE/41vRBtbDlEBEJHC4Q6XXQYtW4Z3+hDW8HGH88+PNrYcpDkAESkMd94ZTuOsMGIETJgAZpGFlOtUAEQkv02bBuedl2yXlMDf/hY+BUidVABEJD/NmxeSfYXWreG998K4v6REBUBE8ktZGXTuXLVv2bLdT/OUemkSWETyw+bNcPjhVZP/iy+GCV4l/z2SkQJgZhPMbJOZLanlcTOzu8xslZktMrP8uWimxMqkSeHLoU2ahJ+TJjX+scygadPws+KYmYrjyiuT+27aFPr1q32/leNp0iT8rLi1bBm+LFvRbtcu7Lt6/O3ahVuTJlXv78nzqoi9yHbytA2BffYJ6/EDow8Yz6SHHfr1222fFXHVd4xUYsnm6yGK4+Huad+APsCJwJJaHh8IPAcY0BuYU98+e/bs6SLZ9PDD7q1bu4e3lOHWunXoz8axKm7Nmrk3b55+HKNG1bz/mvZbVzyZujVvHp5bKs+rIvZbGV3lF+7gOodd//rdUaPqj7umY6Tyb53N10MmjweUeqq5O9UN690RFNdRAP4MXFSpvQLoUNf+VAAk27p2rTmBdO2avWPVdWtoHEVFqe93T+LJ1K2m53WZPVBloxkM8CK27/a7DXmOqfz9K2+XzddDJo/XkAJgYfv0mVkx8Iy796jhsWeA29z9tUR7JvBLdy+ttt1IYCRAly5deq5duzYjsYmkokmT8F+uOrNw7e9sHKsuDY0j1dPfK7bLUCposCrPa/bsKmvwr6ULx7CYzeyTuWOQ2r91Nl8PmTyemc1z95L6t8yxSWB3H+/uJe5e0l6nckmW1bY0TGMsGbMn+2zo76S6snGXLtEui9OlC7ByZch0lZJ/V96jmLV1Jv+GPMe62jX1Z/P1EMXxIHsFYANQ+bytTok+kZxxyy3hVPLKWrcO/dk4VoVmzaB58/TjGDmy/m0q9ltXPJnSvHl4bpV1avUxKzftW/WSi3PmcOUoZx1d69xf69bhOdYXd01/u1T+rbP5eojieEDW5gAGUXUS+M369qc5AInCww+HMVez8LOxJvwqH6vyWHbFMTMVx6hRyX0XFbn37Vv7fivHY1Z1HLpFC/c2bZLttm3DvqvH37ZtuJlVvV/9ebXgKy9tcXLVgzz22B7FXv1vVRFXfX+7VP7G2Xw9ZOp4ZHsOwMweAc4A2gEfAjcDzRIF5k9mZsDdwADgS+Ayrzb+X11JSYmXlta5iYjkG/fwtv2++5J9t94KY8ZEF1OBacgcQEa+CezuF9XzuAM/zsSxRCRP/e53cO21yfbFF8PEiWH2UyKhpSBEpHFNnw5Dhybbxx8Pr78OrVpFF5MAKgAi0ljmz4cTK33pv3lzWLcODjooupikChUAEcmsDRugU6eqfUuWwNFHRxOP1EqDbyKSGf/8Jxx5ZNXk//zzYeJXyT8nqQCISHp27gwXZNl7b3jnndD3pz+FxH/WWdHGJnVSARCRPfcf/xGW7Jw2LbR/9rOwbsEPfxhtXJISzQGISMP95S9wySXJdv/+8Oyzu3/VV3KaCoCIpO7VV+H005PtQw4JV+Pad9/oYpI9pgIgIvVbtQq6davat3o1HHpoNPFIRmgOQERq98kn0LZt1eT/+uthglfJP++pAIjI7rZtg3/7t5D8P/kk9D3ySEj8J58cbWySMSoAIpLkDqNGQYsW8NproW/s2NA/fHi0sUnGaQ5ARIK77oKrr062hw8PVyXXYm0FSwVAJO6efRYGD062jz0W/v73xr9CjEROBUAkrhYuDCtzVigqgrIyOPjg6GKSrFIBEImb99+Hjh2r9i1aBMccE008EhkN7onExRdfQPfuVZP/jBlhglfJP5ZUAEQK3a5dMGwYtGkDy5eHvrvvDon/nHOijU0ipQIgUshuvjmM7U+dGtpXXRUKwo91hVbRHIBIYfrrX+G73022zzwzrM2vxdqkEhUAkULyf/8Hp52WbB94ILz9Nuy/f3QxSc5SARApBO++C0ccsXvfYYdFE4/kBc0BiOSzTz8N7/IrJ//XXgsTvEr+Ug8VAJF8tH17GNc/4AAoLw99kyaFxH/qqdHGJnlDBUAkn7iHM3maN4fZs0PfzTeH/n//90hDk/yjOQCRfDFuXEj+FS64ACZP1mJtssdUAERy3XPPwcCByXb37jB3rhZrk7SpAIjkqsWLw8qclb3/PnToEE08UnD02VEk13zwQRjWqZz8FywI4/xK/pJBKgAiueLLL8OibB06hGQP8Mwz4f5xx0UbmxQkFQCRqO3aFa6+tddesGRJ6PvDH0LiHzQo2tikoKkAiETpN78Ji7U9+mhoX3llKAg//Wm0cUksZKQAmNkAM1thZqvMbHQNj48ws3IzW5C4XZGJ44rkrcmTwQxuuim0+/SBrVvDqZ5m0cYmsZH2WUBmVgSMA/oDZcBcM5vu7suqbfqou1+12w5E4uTvf4dTTkm227aFlSvDN3pFsiwTp4H2Ala5+2oAM5sMDAWqFwCR+FqzZve1ed55Z/cF3ESyKBNDQB2B9ZXaZYm+6r5tZovMbIqZda5pR2Y20sxKzay0vGJ9E5F89o9/hLN6Kif/V14JE7xK/hKxbE0CPw0Uu/uxwIvAxJo2cvfx7l7i7iXt27fPUmgijWD7dujXL6zD/8EHoe+hh0Li79Mn2thEEjJRADYAld/Rd0r0/Yu7f+zuWxPN+4CeGTiuSO5xh6uvDou1zZwZ+m68MfR/73vRxiZSTSbmAOYC3czsUELiHw5UWZbQzDq4+8ZEcwiwPAPHFckt99wTTuOscP758Nhj4TRPkRyUdgFw9x1mdhXwPFAETHD3pWY2Fih19+nAT81sCLAD+AQYke5xRXLGCy/A2Wcn20ceCfPmQZs20cUkkgLziq+c55iSkhIvLS2NOgyR2i1dCj16VO0rK4OONZ0DIZIdZjbP3UtS2VbfBBZpqA8/DGP8lZP/W2+FcX4lf8kjKgAiqdqyBY4/Hg4+OJzlAzB9ekj8J5wQbWwie0AFQKQ+u3bBxReHC7AsXBj6fvvbkPi/9a1oYxNJgwqASF1uuSWcxTNpUmj/4AehIFxzTbRxiWSArggmUpPHH4cLL0y2TzkFZs2CFi2ii0kkw1QARCqbMwd6906299sPVq0Ki7aJFBgVABGAtWuhuLhq34oV4Zx+kQKlOQCJt88+g06dqib/2bPDBK+SvxQ4FQCJpx07YMCAMMSzIbF01QMPhMR/+unRxiaSJSoAEi/ucO210KwZPP986BszJvSPGBFpaCLZpjkAiY9774WRI5PtIUPgiSe0WJvElgqAFL6XXoL+/ZPtww+HBQu0WJvEngqAFK7ly6F796p969eHSV8R0RyAFKBNm6Bly6rJv7Q0jPMr+Yv8iwqAFI6vvoKSEjjoINiauADdk0+GxN9TF6ETqU4FQPKfO1xyCbRqFS7EAnDHHaH/3HOjjU0kh6kASH67/XZo0gT+8pfQvvzysFjbz38ebVwieUCTwJKfpk6FYcOS7ZNOglde0WJtIg2gAiD5Ze5c6NUr2W7TBtasgXbtootJJE+pAEh+WLcOunat2rd8OXz969HEI1IANAcguW3z5rBQW+XkP3NmmOBV8hdJiwqA5KYdO2DQINhnn7BUM8B994XE/81vRhubSIFQAZDc84tfhMXaZswI7euvD4n/+9+PNi6RAqM5AMkd998PV1yRbA8cCE89BU31MhVpDPqfJdGbNQv69k22i4th0SLYe+/IQhKJAxUAic7bb8NRR1XtW7sWunSJJh6RmNEcgGTfRx+F8/crJ/833wzj/Er+IlmjAiDZs3Ur9O4N7dvDF1+EvilTQuL/xjeijU0khlQApPG5hzV6WraEOXNC3223hf5vfzva2ERiTHMA0rjuvDOcxlnhkkvgwQfBLLKQRCRQAZDGMW0anHdest2zJ7z2WvgUICI5QQVAMmvevHBRlgotW4Yzew48MLqYRKRGGZkDMLMBZrbCzFaZ2egaHm9hZo8mHp9jZsWZOK7kkLKyMKxTOfkvXQpbtij5i+SotAuAmRUB44BzgO7ARWZW7UrcfB/41N2PAH4H3J7ucSVHbN4Mhx8OnTsn+154IUzwVr8gu4jklEx8AugFrHL31e6+DZgMDK22zVBgYuL+FKCvmWYB89rOnTBkSFisbfXq0Dd+fEj8/ftHG5uIpCQTBaAjsL5SuyzRV+M27r4D+AxoW31HZjbSzErNrLS8vDwDoUmjuOGGsD7P00+H9rXXhssw/uAH0cYlIg2SU5PA7j4eGA9QUlLiEYcj1T34IFx2WbI9YEAoAlqsTSQvZeJ/7gag0gAwnRJ9NW1TZmZNgX2BjzNwbMmGV16BM85Itjt1giVLYN99IwtJRNKXiSGguUA3MzvUzJoDw4Hp1baZDlyauD8MmOXueoef6959N5zZUzn5r1kD69cr+YsUgLQLQGJM/yrgeWA58Ji7LzWzsWY2JLHZ/UBbM1sFXAvsdqqo5JAtW8JSDUcckex7440wwVtcHFlYIpJZGRm8dfcZwIxqfTdVuv8VcEEmjiWNaOfOMM5/882wYQN861swejScckrUkYlII9DsnYR39s88E5L9smVhxc6//hX69Ik6MhFpRFoNNO7eeANOPz2c079jB0ydCq+/ruQvEgMqAHG1YkVYivnkk2HlSrjnnnBmz/nna6VOkZjQEFDcbNwIY8fCvfdCq1bh/jXXhCt0iUisqADExebNcMcd8L//C9u2wahR8J//qYXaRGJMBaDQbdsW1ugZOxbKy+E734H/+q+qp3iKSCxpDqBQ7doFjz4aVuT8yU+gR49w4fXJk5X8RQRQAShML78MJ50Ew4dD69YwYwbMnKkLr4tIFSoAhWTRIjjnHPjmN2HTJpg4EebPD306s0dEqlEBKARr18Kll8Lxx8OcOeFC7CtWhAuwFxVFHZ2I5ChNAuezTz6BW2+Fu+8O7euvD9/m3X//aOMSkbygApCPtmyBP/4R/vu/4bPPYMQI+PWvq16WUUSkHhoCyic7d8IDD8CRR8IvfwmnnhrG/SdMUPIXkQZTAcgH7vDss2GM//LL4ZBDYPbssIBbjx5RRycieUoFINfNmQNnngmDB8PWrfD448kF3ERE0qACkKveeQcuuCAszbx8OYwbB0uXwrBhOqVTRDJCk8C55sMPw4TuvfdCixbwq1/BdddpsTYRyTgVgFyxeXNYqO3OO8NQz8iRcNNNcNBBUUcmIgVKBSBq27cnF2vbtCkM+9xyC3TrFnVkIlLgVACi4g5TpsANN8CqVWFS9+mnoVevqCMTkZjQJHAUZs8Oi7VdeCG0bBlO8Xz5ZSV/EckqFYBsWrwYBg0Kp3Vu3Bi+1LVgAQwcqDN7RCTrVACyYd06uOwyOO64cMH1//mfcB3eESO0WJuIREZzAI3p00/Dej133RXa110HY8bAAQdEG5eICCoAjePTT+HEE8MlGL/8MizLPHYsdOkSdWQiIv+iApBJ27fDWWeFSd4KCxfCscdGFpKISG00B5AJ7nDVVdC8eTL533RT6FfyF5EcpU8A6Ro3LiT/CsOGhYuxN1FtFZHcpgKwp557Lpy+WaF7d3jzTdhrr+hiEhFpABWAhlq8ePdhnQ0bwhr9IiJ5ROMUqfrgg3DOfuXkP39+GOdX8heRPKQCUJ8tW0LS79ABdu0KfU8/HRL/8cdHG5uISBrSKgBmdoCZvWhm7yR+7l/LdjvNbEHiNj2dY2bNrl1w0UXQunUY9gH4/e9D4h88ONrYREQyIN1PAKOBme7eDZiZaNdki7sfn7gNSfOYje83vwnDPZMnh/aPfhQKwtVXRxuXiEgGpTsJPBQ4I3F/IjAb+GWa+4zOo4/C8OHJ9mmnwcyZ4fx+EZECk24BOMjdNybufwDUdvmqlmZWCuwAbnP3aTVtZGYjgZEAXbK5bMIbb8DJJyfbBxwQrsmrNXtEpIDVWwDM7CXg4BoeurFyw93dzLyW3XR19w1mdhgwy8wWu/u71Tdy9/HAeICSkpLa9pU5a9bAYYdV7Vu5UlfjEpFYqLcAuHu/2h4zsw/NrIO7bzSzDsCmWvaxIfFztZnNBk4AdisAWfPZZ3DUUWFN/gqvvAJ9+kQWkohItqU7CTwduDRx/1LgqeobmNn+ZtYicb8dcCqwLM3j7pnt26F/f9hvv2TynzgxnNmj5C8iMZNuAbgN6G9m7wD9Em3MrMTM7ktscxRQamYLgZcJcwDZLQDucM01YTL3pZdC3403hv5LLslqKCIiuSKtSWB3/xjoW0N/KXBF4v7rwDHpHCctf/5zOI2zwnnnweOP60pcIhJ7hbsW0IsvhrX5Kxx5JMybB23aRBeTiEgOKcwCcPDB8OGHyXZZGXTsGF08IiI5qPDWAtq5M1yGEcI7fnclfxGRGhTeJ4CiIvj886ijEBHJeYX3CUBERFKiAiAiElMqACIiMaUCICISUyoAIiIxpQIgIhJTKgAiIjGlAiAiElMqACIiMaUCICISUyoAIiIxpQIgIhJTKgAiIjGlAiAiElMqACIiMaUCICISUyoAIiIxpQIgIhJTKgAiIjGlAiAiElMqACIiMaUCICISUyoAIiIxpQIgIhJTKgAiIjGlAiAiElMqACIiMaUCICISU2kVADO7wMyWmtkuMyupY7sBZrbCzFaZ2eh0jlmfo48GM910q/vWogVMmtSYr0SR3JfuJ4AlwPnAq7VtYGZFwDjgHKA7cJGZdU/zuDU6+mhYtqwx9iyFZts2uPhiFQGJt7QKgLsvd/cV9WzWC1jl7qvdfRswGRiaznFro+QvDXXjjVFHIBKdbMwBdATWV2qXJfp2Y2YjzazUzErLy8uzEJrE3bp1UUcgEp2m9W1gZi8BB9fw0I3u/lQmg3H38cB4gJKSEs/kvkVq0qVL1BGIRKfeAuDu/dI8xgagc6V2p0RfxnXvrmEgaZhbbok6ApHoZGMIaC7QzcwONbPmwHBgemMcaOnSUARE6tO8OTz8MHz3u1FHIhKdej8B1MXMzgP+CLQHnjWzBe5+tpkdAtzn7gPdfYeZXQU8DxQBE9x9adqR12Jpo+1ZRKSwpFUA3P1J4Mka+t8HBlZqzwBmpHMsERHJLH0TWEQkplQARERiSgVARCSmVABERGLK3HPz+1ZmVg6s3cNfbwd8lMFwsiUf487HmCE/41bM2ZOPcVfE3NXd26fyCzlbANJhZqXuXuvqpLkqH+POx5ghP+NWzNmTj3HvScwaAhIRiSkVABGRmCrUAjA+6gD2UD7GnY8xQ37GrZizJx/jbnDMBTkHICIi9SvUTwAiIlIPFQARkZgq+AJgZteZmZtZu6hjSYWZ3WFmb5vZIjN70sz2izqm2pjZADNbYWarzGx01PHUx8w6m9nLZrbMzJaa2dVRx5QqMysys/lm9kzUsaTKzPYzsymJ1/NyMzs56pjqY2bXJF4bS8zsETNrGXVMNTGzCWa2ycyWVOo7wMxeNLN3Ej/3r28/BV0AzKwzcBaQTxf+exHo4e7HAiuBMRHHUyMzKwLGAecA3YGLzCzXr8awA7jO3bsDvYEf50HMFa4GlkcdRAP9Afh/7v514DhyPH4z6wj8FChx9x6E5euHRxtVrR4EBlTrGw3MdPduwMxEu04FXQCA3wG/APJmptvdX3D3HYnmG4QrqOWiXsAqd1/t7tuAycDQiGOqk7tvdPe3Evc3ExJSjdenziVm1gkYBNwXdSypMrN9gT7A/QDuvs3d/xFtVClpCrQys6ZAa+D9iOOpkbu/CnxSrXsoMDFxfyJwbn37KdgCYGZDgQ3uvjDqWNJwOfBc1EHUoiOwvlK7jDxIphXMrBg4AZgTbSQp+T3hjcyuqANpgEOBcuCBxNDVfWa2V9RB1cXdNwB3EkYMNgKfufsL0UbVIAe5+8bE/Q+Ag+r7hbwuAGb2UmKsrvptKHADcFPUMdaknrgrtrmRMGQxKbpIC5OZtQGmAj9z98+jjqcuZjYY2OTu86KOpYGaAicC97j7CcAXpDAkEaXEmPlQQvE6BNjLzC6ONqo94+H8/npHPtK6IljUartgvZkdQ/hHXGhmEIZR3jKzXu7+QRZDrFFtcVcwsxHAYKCv5+4XNTYAnSu1OyX6cpqZNSMk/0nu/kTU8aTgVGCImQ0EWgL7mNnD7p7riakMKHP3ik9YU8jxAgD0A9a4ezmAmT0BnAI8HGlUqfvQzDq4+0Yz6wBsqu8X8voTQG3cfbG7H+juxe5eTHgxnpgLyb8+ZjaA8HF/iLt/GXU8dZgLdDOzQ82sOWGybHrEMdXJwruB+4Hl7v7bqONJhbuPcfdOidfxcGBWHiR/Ev/X1pvZ1xJdfYFlEYaUinVAbzNrnXit9CXHJ66rmQ5cmrh/KfBUfb+Q158ACtTdQAvgxcSnlzfc/UfRhrQ7d99hZlcBzxPOlpjg7ksjDqs+pwLfAxab2YJE3w2Ja1ZL5v0EmJR4g7AauCzieOrk7nPMbArwFmH4dT45uiSEmT0CnAG0M7My4GbgNuAxM/s+YSn9C+vdT+6OMIiISGMqyCEgERGpnwqAiEhMqQCIiMSUCoCISEypAIiIxJQKgIhITKkAiIjE1P8HaHqvnwb5UFcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# WRITE CODE HERE: \n",
    "# DO NOT assume that X has only one featur\n",
    "def get_wOLS_ext(X, y):\n",
    "    reg =linear_model.LinearRegression().fit(X, y)\n",
    "\n",
    "    print(reg.coef_)\n",
    "    return reg.coef_\n",
    "\n",
    "wOLS_ext = get_wOLS_ext(xtrain, ytrain)\n",
    "yguess = wOLS_ext[0] * xtrain + wOLS_ext[1] \n",
    "plt.scatter(xtrain.T[0], ytrain, marker='o',color =\"blue\")\n",
    "plt.plot(xtrain,yguess,color =\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhgVuFW9w773"
   },
   "source": [
    "### Problem c. (6 points)\n",
    "\n",
    "Complete the linear_binary_predict function to predict the labels of the test set using the OLS solution from part a. Also complete compute_CCR and report the correct classification rate (CCR).\n",
    "\n",
    "linear_binary_predict takes Xext, a matrix of extended feature vectors, and wext, an extended weight vector; it performs linear binary classification and returns a vector of predicted binary labels.\n",
    "\n",
    "compute_CCR takes Xext, wext, along with y, the expected binary labels to compute CCR. It should make use of linear_binary_predict.\n",
    "\n",
    "Note: \n",
    "*   We assume Xext to be row-major, meaning each row is a sample.\n",
    "*   Do not change the function prototype of linear_binary_predict. This also applies to all other functions we ask you to complete\n",
    "*   The OLS from sklearn has a builtin predict function. We are asking you to complete linear_binary_predict instead of using the builtin predict because linear_binary_predict will be important for later parts.\n",
    "\n",
    "Hint: use the idea from part a.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrhLQ-0_x8OX",
    "outputId": "3faba44a-0841-4178-b9bb-ccd80903fb60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 2)\n",
      "The test CCR using OLS is 0.72\n"
     ]
    }
   ],
   "source": [
    "# WRITE CODE HERE:\n",
    "def linear_binary_predict(Xext, wext):\n",
    "    yguess = []\n",
    "    for i in range(len(Xext)):\n",
    "      # print(\"x:\",Xext[i].shape)\n",
    "      \n",
    "      # print(\"w:\",wext.shape,wext[-1])\n",
    "      num = wext @ Xext[i] +wext[-1]\n",
    "      # print (num.shape)\n",
    "      if num <0: num =-1\n",
    "      else : num =1\n",
    "      yguess.append(num)\n",
    "    return yguess\n",
    "\n",
    "def compute_CCR(Xext, wext, y):\n",
    "    yguess = linear_binary_predict(Xext, wext)\n",
    "    total = [(item*0)+1 for i,item in enumerate(y) if item ==yguess[i] ]\n",
    "    sumOfcorrect = sum(total)\n",
    "    Rate = float(sumOfcorrect/len(y))\n",
    "    return Rate\n",
    "print(xtrain.shape)\n",
    "CCR = compute_CCR(xtrain,wOLS_ext,ytrain)\n",
    "print(\"The test CCR using OLS is\", CCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX-NCA9sZ5XO"
   },
   "source": [
    "### Problem d. (4 points)\n",
    "Explain why CCR is not a good metric in this case. Suggest an alternative that better captures the performance of OLS on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZmVPImh-XSk"
   },
   "source": [
    "CCR is a bad metric because it weights all negitive values and positive values with equal amplitude. This means that it is hard to solve for. In this case it is a bad mesure because the data shares x values for each classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF7u7IoGC96w"
   },
   "source": [
    "## **Question 3:** Fisher's Linear Discriminant (27 points)\n",
    "Fisher's Linear Discriminant is a method that takes d dimensional feature vectors and projects them into 1 dimension, and it tries to do so in a fashion where the resulting 1D values are well-separated by class. Here our d happens to be 1, but as mentioned above, the code you write needs to be able to accomodate higher dimensions for full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaVLlwwJWrpX"
   },
   "source": [
    "### a. Separate the training set by class (4 points)\n",
    "\n",
    "Write a function seperate that takes the inputs X, y and separates X based on y.\n",
    "\n",
    "For full credit, create a vectorized implementation (no for loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "991ZOOYCY02l"
   },
   "outputs": [],
   "source": [
    "def seperate(X, y):\n",
    "    # WRITE CODE HERE:\n",
    "    \n",
    "    X1 = [ x for i,x in enumerate(X) if y[i] == -1]\n",
    "    X2 = [ x for i,x in enumerate(X) if y[i] == 1]\n",
    "    X1 = np.asarray(X1,dtype=np.float32)\n",
    "    X2 = np.asarray(X2, dtype=np.float32)\n",
    "    # X1 should be of the negative class, and X2 the positive class\n",
    "    return X1, X2\n",
    "\n",
    "X1, X2 = seperate(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMMqIr0xB9BP"
   },
   "source": [
    "### b. Calculate mean vectors (4 points)\n",
    "Write a function get_means that takes the inputs X1, X2 and calculates the mean vectors of the two classes.\n",
    "\n",
    "For full credit, create a vectorized implementation (no for loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxuPgvLQKTPF",
    "outputId": "8551315f-4e86-4ad5-b4a6-af46341ffb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1, m2: [-1.0708544  1.       ] [5.731858 1.      ]\n"
     ]
    }
   ],
   "source": [
    "def get_means(X1, X2):\n",
    "    # WRITE CODE HERE:\n",
    "    m1 =np.mean(X1,axis=0)\n",
    "    m2 =np.mean(X2,axis=0)\n",
    "    return m1, m2 \n",
    "\n",
    "m1, m2 = get_means(X1, X2)\n",
    "print('m1, m2:', m1, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3xQqSq_Ogmd"
   },
   "source": [
    "### c. Calculate within-class covariance (5 points)\n",
    "$S_w$, the total within-class covariance matrix, is a d by d matrix given by $S_1 + S_2$ where $S_1 = \\sum_{x_i \\in X_1}(x_i-m_1)^T(x_i-m_1) = (X_1-m_1)^T(X_1-m_1)$. \n",
    "\n",
    "Write a function get_Sw that takes the inputs X1, X2, m1, m2 and calculates Sw.\n",
    "\n",
    "For full credit, create a vectorized implementation.\n",
    "\n",
    "**Note:** depending on the schema of the dataset (row-major vs column-major), the formula to use might differ slightly from those on the lecture slides. We assume row-major (each sample is a row) here, as many public datasets are organized this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5NaX0uwUXll",
    "outputId": "076f558b-1fd2-4d07-f15b-a1a550e07a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sw: 1106.736\n"
     ]
    }
   ],
   "source": [
    "def get_Sw(X1, X2, m1, m2):\n",
    "    # WRITE CODE HERE:\n",
    "    X1m1 = (X1-m1)\n",
    "    \n",
    "    X2m2 = (X2-m2)\n",
    "    S1 = X1m1.T @ X1m1\n",
    "    S2 = X2m2.T @ X2m2\n",
    "    Sw = S1+S2\n",
    "    return Sw\n",
    "\n",
    "Sw = get_Sw(X1.T[0].T, X2.T[0].T, m1[0], m2[0])\n",
    "print('Sw:', Sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnV8WWDBdZAG"
   },
   "source": [
    "### d. Calculate $w_{FLD}$ and $b_{FLD}$ (5 points)\n",
    "Write a function get_wFLD_ext that takes the inputs Sw, m1, m2 and calculates the extended $w_{FLD}$.\n",
    "\n",
    "Recall that in class we mentioned the average of the averages of the two classes after the linear transformation can serve as the bias term. However, since we want the outputs of $Xw + b$ on the two classes to be roughly separated by 0, we actually want the effect of adding b to be the same as subtracting that average. Thus, use $b = -(m_1w + m_2w)/2$.\n",
    "\n",
    "For full credit, create a vectorized implementation, and do not use the inverse function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1BuMyNvdXm9",
    "outputId": "d5d9eccd-ef60-47da-ac2f-643d04bf7988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wFLD_ext: [[0.00614664 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def get_wFLD_ext(Sw, m1, m2):\n",
    "    # WRITE CODE HERE: \n",
    "    sinv = np.linalg.inv(Sw)\n",
    "    w = sinv*(m2-m1)\n",
    "    b = -(m1*w+m2*w)*0.5\n",
    "    np.append(w,b)\n",
    "    return w\n",
    "\n",
    "wFLD_ext = get_wFLD_ext(Sw.reshape(1,1), m1, m2)\n",
    "print('wFLD_ext:', wFLD_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5FyLF32e9t0"
   },
   "source": [
    "### e. Evaluation (4 points)\n",
    "\n",
    "First, create a plot of the FLD solution using the training set. Then use the compute_CCR function that you wrote earlier to get the FLD CCR on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "9hWd2qTM_xun",
    "outputId": "4a2fa804-34d7-4571-91a1-fbd663e99cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "0.72\n",
      "The test CCR using FLD is 0.72\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WRITE CODE HERE: \n",
    "print(wFLD_ext.shape)\n",
    "CCR = compute_CCR(xtrain,wFLD_ext.T,ytrain)\n",
    "ys =[]\n",
    "for i in range(len(xtrain)):\n",
    "      num = wFLD_ext.T[0] * xtrain[i][0] +wFLD_ext.T[1]\n",
    "      ys.append(num)\n",
    "plt.scatter(xtrain.T[0].T,ytrain)\n",
    "plt.plot(xtrain.T[0].T,ys)\n",
    "print(\"The test CCR using FLD is\", CCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kG8hSp7WHCD"
   },
   "source": [
    "### f. Reflection (5 points)\n",
    "Recall that in class we noted that OLS and FLD can be equivalent under certain situations. Is that the case here? If not, briefly comment on why they performed differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa2X0WzPCFKw"
   },
   "source": [
    "It should be the case here because it is already 1 dementional so the FLD is doing nothing. However, I think I messed up this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hte8xdEpZNZ"
   },
   "source": [
    "## **Question 4:** An algorithm using SGD (33 points)\n",
    "Let us consider a different linear classifier in this problem, one based on the idea of an $\\textit{error}$ function minimization problem, similar to what we studied with linear regression, but whose error function is more tailored to the classification problem.\n",
    "\n",
    "The form of our linear classifier is $h_{\\bf{w}}(\\bf{x}) = \\mbox{sgn}(\\bf{w}^T \\bf{x})$, where $sgn$ is the sign function (so $1$ if $\\bf{w}^T \\bf{x} \\geq 0$ and $-1$ otherwise).  We are assuming here that we have padded the inputs with an extra dimension of $1$ to account for the bias term in the linear function, as we did with linear regression.\n",
    "\n",
    "In the binary classification case, each target class $y_i$ is either $1$ or $-1$.  One possible error function to use would say that the error is 0 if our linear classifier agrees with the target label (i.e. $y_i = \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), and if our linear classifier predicts the incorrect class ($y_i \\neq \\mbox{sgn}(\\bf{w}^T \\bf{x}_i$)), then the error will be given by $|\\bf{w}^T \\bf{x}_i|$---intuitively, we penalize more for predictions that are farther from being predicted correctly.\n",
    "\n",
    "**Note:** the analytical parts of this question assume column feature vectors. However, you should still assume row feature vectors in the coding parts for consistency with our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19cL4fQjsHHC"
   },
   "source": [
    "### Problem a. (5 points)\n",
    "Show that, for each training point, this error function may be concisely written as\n",
    "\n",
    "$L_i(\\bf{w}) = \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhJyt3ZrBuG2"
   },
   "source": [
    "* if sgn(wTxi ==y: \n",
    "-- Li(w) = 0\n",
    "* else : \n",
    "-- Li(w) = abs(wtxi)\n",
    "\n",
    "this means that abs(wtxi) will always be greater than 0 and that the loss function will only be 0 when it is correctly classified \n",
    "\n",
    "- the scallar multiple (-yi) will be take the oposite sign of yi {-1,1}\n",
    "\n",
    "- the scallar produced by wT@Xi will have a sign that is either positivie or negitive \n",
    "\n",
    "- Therefor if the classification matches the value of their products will be negitive bc {-(-1)*-1 = -1, -(1) *1 =-1} and if the values dont match then they will be positive {-(-1)*1 = 1, -(1) *-1 =1} \n",
    "\n",
    "- zero will always be greater than the correct matches and less than the incorrect matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlqWT0CXs5vk"
   },
   "source": [
    "### Problem b. (5 points)\n",
    "We will compute the total loss over the training data as the sum of the $L_i$ losses, namely\n",
    "\n",
    "$L(\\bf{w}) = \\sum_{i=1}^n \\max(0, -y_i \\bf{w}^T \\bf{x}_i)$.\n",
    "\n",
    "Note that one reason we chose this particular loss function is that it is continuous (unlike the 0-1 loss).  Furthermore, it has a simple sub-gradient.  The sub-gradient of the loss at 0 is 0, and everywhere else it is equal to the gradient.  Show that the sub-gradient is equal to\n",
    "$\\nabla_{\\bf{w}} L_i = \n",
    "\\begin{cases}\n",
    "0 & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ correctly}\\\\\n",
    "-y_i \\bf{x}_i & h_{\\bf{w}} \\mbox{ classifies $\\bf{x}_i$ incorrectly.}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQ26ILVB8xY"
   },
   "source": [
    "-$L(W) = \\sum_{i=1}^n(max(0,-y_iw^Tx_i)$\n",
    "\n",
    "-$L(W) = \\sum_{i=1}^n\\begin{cases}0 & -y_iw^Tx_i<0\\\\\n",
    "-y_iw^Tx_i & -y_iw^Tx_i>0\\end{cases}$\n",
    "\n",
    "-$\\nabla_w L(W) = \\nabla_w \\sum_{i=1}^n\\begin{cases}0 & -y_iw^Tx_i<0\\\\\n",
    "-y_iw^Tx_i & -y_iw^Tx_i>0\\end{cases}$\n",
    "\n",
    "\n",
    "-$\\nabla_w L(W) =  \\sum_{i=1}^n\\begin{cases}\\nabla_w(0) & -y_iw^Tx_i<0\\\\\n",
    "-\\nabla_w(y_iw^Tx_i) & -y_iw^Tx_i>0\\end{cases}$\n",
    "\n",
    "-$\\nabla_w L(W) =  \\sum_{i=1}^n\\begin{cases}0 & -y_iw^Tx_i<0\\\\\n",
    "-y_ix_i & -y_iw^Tx_i>0\\end{cases}$\n",
    "\n",
    "- -$y_iw^Tx_i<0 \\mbox{ when }h_w\\mbox{ classifies $\\bf{x}_i$ correctly}$\n",
    "\n",
    "- -$y_iw^Tx_i>0 \\mbox{ when }h_w\\mbox{ classifies $\\bf{x}_i$ incorrectly}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF24kNfQuSKB"
   },
   "source": [
    "### Problem c. (5 points)\n",
    "Using a batch size of 1 and a step size of 1, write down the update rule for stochastic (sub)-gradient descent to minimize $L(\\bf{w})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BZZldGACCr_"
   },
   "source": [
    "distatnceVectorW = L(W)\n",
    "\n",
    "step = 1 \n",
    "\n",
    "batch = 1 \n",
    "\n",
    "toll = 0.5\n",
    "\n",
    "wlast = (rand point in vector space)\n",
    "\n",
    "while ||distatnceVectorW||>toll:\n",
    "    \n",
    "  - rand = rand val in X \n",
    "  - x = X[rand]\n",
    "  - y = Y[rand]\n",
    "  - yx = -yWTx\n",
    "  - if yx < 0 : yx =0 \n",
    "  - W = wlast - step*yx  \n",
    "  - wlast = w\n",
    "  -distance = L(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpOT7Z6BrV6T"
   },
   "source": [
    "### Problem d. (12 points)\n",
    "Now it's time to implement the actual algorithm using the update rule we derived from part c. \n",
    "\n",
    "1.   Complete the one_pass function, which iterates over the entire Xext and y once, updating w in the process. (6 points)\n",
    "2.   Go over the training set twice (2 epochs) to get your final w, which you can do by calling one_pass twice in the get_wSGD function. Then report the CCR on the test set. (6 points)\n",
    "\n",
    "Note:\n",
    "* You should always shuffle the training set before each pass.\n",
    "* The first shuffle has already been performed by train_test_split, so you do not need to do a shuffle before the first pass here.\n",
    "* You can get rid of the one_pass function if you think it's better to incorporate it into get_wSGD, but you must keep get_wSGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e14KZEtjvOuO",
    "outputId": "9f779be6-9641-4b0e-ac8a-d39750e597e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7653333333333333\n",
      "The test CCR using SGD is 0.7653333333333333\n",
      "wSGD: [ 2.54070721 -1.         -1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def one_pass(Xext, y, w): # gives a single x vector \n",
    "   # n = Xext.shape[0]\n",
    "    n = Xext.size\n",
    "    step = np.ones(w.shape)\n",
    "    step = 1* step\n",
    "    yx = -y*w.T@Xext\n",
    "    if yx < 0: yx = 0\n",
    "    else: yx = -y*Xext\n",
    "\n",
    "    w = w - yx*step\n",
    "    return w \n",
    "        # WRITE CODE HERE:\n",
    "\n",
    "def get_wSGD(Xext, y):\n",
    "    # use 0 as the random state for shuffling\n",
    "    epochs = 2 \n",
    "    random_state=0\n",
    "    batchSize = 1\n",
    "    Initialw = np.zeros(Xext[0].shape)\n",
    "    w = one_pass(Xext[0], y[0], Initialw)\n",
    "    for i in range(epochs-1):\n",
    "      xT, trashx, yT, trashy = train_test_split(Xext, y, test_size=0.1, random_state=random_state)#\n",
    "      random_state=+1\n",
    "      for j in range(batchSize):\n",
    "        w = one_pass(xT[j],yT[j], w)\n",
    "        Initialw = w\n",
    "    # WRITE CODE HERE:\n",
    "    return w\n",
    "\n",
    "wSGD = get_wSGD(Xtr_ext, ytrain)\n",
    "\n",
    "CCR = compute_CCR(Xtr_ext,wSGD,ytrain)\n",
    "print(\"The test CCR using SGD is\", CCR)\n",
    "print('wSGD:', wSGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAyX-08Z1f5h"
   },
   "source": [
    "### Problem e. (6 points)\n",
    "\n",
    "The result you get from the previous part is likely not ideal. And if you change the random_state used for shuffling in the previous part (remember to change it back to 0 if you try this), you should see large swings in CCR. This is a common phenomenon when using SGD naively, and we will see this again in question 5.\n",
    "\n",
    "Recall from part c that we are using a fixed step size of 1, which is way too large for you to converge to a local minimum consistently. \n",
    "\n",
    "In practice, a technique called learning rate decay is commonly used with SGD. Essentially, the step size gets smaller after each iteration. This allows you to \"learn fast\" in the beginning and also be able to converge to a local minimum later when you are close to one.\n",
    "\n",
    "We will use an initial learning rate (step size) of 1 and the inverse square root decay, which means the step size at iteration $t$ is $\\frac1{\\sqrt t}$.\n",
    "\n",
    "Note:\n",
    "* $t$ starts at 1, not 0; otherwise, you will get a division by 0\n",
    "* You can get rid of the one_pass2 function if you think it's better to incorporate it into get_wSGD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlM2SdxOvGvi",
    "outputId": "1f6f2048-4003-462d-b73b-55d81ecaf1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7071067811865475\n",
      "0.5773502691896258\n",
      "0.5\n",
      "0.4472135954999579\n",
      "0.7653333333333333\n",
      "The test CCR using SGD with decay is 0.7653333333333333\n",
      "wSGD2: [ 2.54070721 -1.         -1.        ]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_pass2(Xext, y, w, t):\n",
    "    n = Xext.size\n",
    "    step = np.ones(w.shape)\n",
    "    fact = 1/math.sqrt(t)\n",
    "    print(fact)\n",
    "    step = fact* step\n",
    "    yx = -y*w.T@Xext\n",
    "    if yx < 0: yx = 0\n",
    "    else: yx = -y*Xext\n",
    "\n",
    "    w = w - yx*step\n",
    "    return w \n",
    "\n",
    "def get_wSGD2(Xext, y):\n",
    "    epochs = 2\n",
    "    random_state=0\n",
    "    batchSize = 1\n",
    "    Initialw = np.zeros(Xext[0].shape)\n",
    "    t = 1\n",
    "    w = one_pass2(Xext[0], y[0], Initialw,t)\n",
    "    for i in range(epochs-1):\n",
    "      xT, trashx, yT, trashy = train_test_split(Xext, y, test_size=0.1, random_state=random_state)#\n",
    "      random_state=+1\n",
    "      for j in range(batchSize):\n",
    "        t = t +1\n",
    "        w = one_pass2(xT[j],yT[j],w,t)\n",
    "        Initialw = w\n",
    "    # WRITE CODE HERE:\n",
    "    return w\n",
    "\n",
    "wSGD2 = get_wSGD2(Xtr_ext, ytrain)\n",
    "CCR = compute_CCR(Xtr_ext,wSGD2,ytrain)\n",
    "print(\"The test CCR using SGD with decay is\", CCR)\n",
    "print('wSGD2:', wSGD2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcnj3HYJHTqa"
   },
   "source": [
    "## **Question 5:** Testing the above methods on a real-world dataset (10 points)\n",
    "No code is required for this part. Getting part a to run correctly is not required for part b.\n",
    "\n",
    "The purpose of this question is to compare the three (four if you count SGD with decay as a separate one) methods on a real-world dataset. We will use the famous Iris dataset created by Sir Ronald Fisher (the same Fisher as in Fisher's Linear Discriminant). It contains feature measurements of 150 samples from three *Iris* flower species. We will combine two of the classes into a single negative class and the remaining class is the positive class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekpBYEcfB8-3"
   },
   "source": [
    "### Problem a. (6 points)\n",
    "Run the cells below. The code for this part is provided to you, but it requires several earlier functions to be properly implemented to run. They need to be able to handle data with multiple features.\n",
    "\n",
    "You will be graded on how well the behavior of this part matches our expectation. Do NOT modify the code provided here; if you have issues running it, check the code you wrote in previous parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usHZVolZXXen"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Split data into feature vectors and labels\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# change the label values for binary classification\n",
    "y[:100] = -1\n",
    "y[100:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yfPgky0OvO-"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.25)\n",
    "Xtr_ext = np.c_[X_train, np.ones(X_train.shape[0])]\n",
    "Xte_ext = np.c_[X_test, np.ones(X_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9coGwIiKQZPh",
    "outputId": "02b30d6d-4593-44c0-858f-c6bc49a28945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09984211  0.37783043 -0.0311981   1.21710629]\n",
      "The test CCR using OLS is 0.72\n",
      "The test CCR using FLD is 0.72\n",
      "The test CCR using SGD is 0.2631578947368421\n",
      "1.0\n",
      "0.7071067811865475\n",
      "0.5773502691896258\n",
      "0.5\n",
      "0.4472135954999579\n",
      "The test CCR using SGD with decay is 0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "wOLS = get_wOLS_ext(X_train, y_train)\n",
    "#CCR = compute_CCR(Xte_ext, wOLS, y_test)\n",
    "print(\"The test CCR using OLS is\", CCR)\n",
    "\n",
    "X1, X2 = seperate(X_train, y_train)\n",
    "m1, m2 = get_means(X1, X2)\n",
    "Sw = get_Sw(X1, X2, m1, m2)\n",
    "wFLD_ext = get_wFLD_ext(Sw, m1, m2)\n",
    "#CCR = compute_CCR(Xte_ext, wFLD_ext, y_test)\n",
    "print(\"The test CCR using FLD is\", CCR)\n",
    "\n",
    "wSGD = get_wSGD(Xtr_ext, y_train)\n",
    "CCR = compute_CCR(Xte_ext, wSGD, y_test)\n",
    "print(\"The test CCR using SGD is\", CCR)\n",
    "\n",
    "wSGD2 = get_wSGD2(Xtr_ext, y_train)\n",
    "CCR = compute_CCR(Xte_ext, wSGD2, y_test)\n",
    "print(\"The test CCR using SGD with decay is\", CCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxpckb9mCE_P"
   },
   "source": [
    "### Problem b. (4 points)\n",
    "A class containing sub-classes is quite common in real-world classifications. One direct effect of this is that within such a class, the feature vectors tend to be quite spread out (in distinct clusters). Comment on the effect this may have on OLS and FLD.\n",
    "\n",
    "Hint: If the previous part runs correctly, you should see OLS and FLD perform worse than the SGD algorithm from problem 4e. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e71oQW2IFCt7"
   },
   "source": [
    "My previous work doesnt work correctly. but I think that the reason that OLS and FLD perform worse than SGD is because this pulls the mean away from a tight point. This is what both ols and fld use to seporate classes. However in gradient decent you are minimizing the error of a line. this means that the means and clustering in the data will not play a role in how the classes are distingished "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EC414_HW4(2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
